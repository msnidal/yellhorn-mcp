{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yellhorn MCP Example in Notebook with LLM Manager\n",
    "\n",
    "Instruction: Swap model to get different behavior LLM Manager config (Normal, Test full chunking, Test full with retry, Test full chunking & retry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "First, let's set up our environment and import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "from pathlib import Path\n",
    "\n",
    "# Import required Yellhorn MCP components\n",
    "from yellhorn_mcp.token_counter import TokenCounter\n",
    "from yellhorn_mcp.llm_manager import LLMManager\n",
    "\n",
    "# Import API clients\n",
    "from google import genai\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "from yellhorn_mcp.server import format_metrics_section, calculate_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure API Keys\n",
    "\n",
    "Set up API keys for Gemini and/or OpenAI. You can either set them in environment variables or directly in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Set API keys directly (not recommended for production)\n",
    "GEMINI_API_KEY = \"\"\n",
    "OPENAI_API_KEY = \"\"\n",
    "\n",
    "MODEL = \"gemini-2.5-flash\"  # or any OpenAI model like \"gpt-4o\"\n",
    "REPO_PATH = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Option 2: Get API keys from environment variables (recommended)\n",
    "# GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "# OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set environment variables for server access\n",
    "# os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
    "# os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "# os.environ[\"REPO_PATH\"] = REPO_PATH\n",
    "# os.environ[\"YELLHORN_MCP_MODEL\"] = MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Working with LLMManager\n",
    "\n",
    "Now let's set up and use LLMManager, which provides unified access to different LLM APIs with automatic chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize API clients\n",
    "gemini_client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Create LLMManager with custom configuration\n",
    "config = {\n",
    "    \"safety_margin_tokens\": 200,\n",
    "    \"overlap_ratio\": 0.1,\n",
    "    \"aggregation_strategy\": \"concatenate\",\n",
    "    \"chunk_strategy\": \"paragraph\",\n",
    "    # Experimental limits to test chunking & retry behavior\n",
    "    \"model_limits\" : {\n",
    "        \"gpt-4o\": 30000,\n",
    "        \"gemini-2.0-flash-exp\": 10000, \n",
    "        \"o4-mini\": 30000,\n",
    "        \"gemini-2.5-pro\": 1_000_000\n",
    "    }\n",
    "}\n",
    "\n",
    "llm_manager = LLMManager(\n",
    "    openai_client=openai_client,\n",
    "    gemini_client=gemini_client,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "def log_callback(level, message):\n",
    "    \"\"\"Custom log callback function.\"\"\"\n",
    "    print(f\"[{level.upper()}] {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make API calls and handle async operations\n",
    "async def call_model(prompt, model, system_message=None, response_format=None):\n",
    "    \"\"\"Helper function to call a model using LLMManager.\"\"\"\n",
    "    try:\n",
    "        response_dict = await llm_manager.call_llm_with_usage(\n",
    "            prompt=prompt,\n",
    "            model=model,\n",
    "            temperature=0.0,\n",
    "            system_message=system_message,\n",
    "            response_format=response_format\n",
    "        )\n",
    "        return response_dict\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test LLM Manager Simple vs Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple vs Chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/12/25 17:34:35] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.openai.com/v1/responses</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 </span>  <a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1740</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">OK\"</span>                                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/12/25 17:34:35]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttps://api.openai.com/v1/responses\u001b[0m \u001b[32m\"HTTP/1.1 200 \u001b[0m  \u001b]8;id=313958;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=288659;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\u001b\\\u001b[2m1740\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mOK\"\u001b[0m                                                                    \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI JSON Response:\n",
      "--------------------------------------------------\n",
      "<yellhorn_mcp.llm_manager.UsageMetadata object at 0x114cf6f50>\n",
      "Sure! Here’s a list of 3 programming languages with their key features:\n",
      "\n",
      "---\n",
      "\n",
      "**1. Python**\n",
      "- Easy-to-read, clean syntax\n",
      "- Extensive standard library\n",
      "- Dynamically typed and interpreted\n",
      "- Large community support\n",
      "- Widely used for web development, data science, automation, and AI\n",
      "\n",
      "---\n",
      "\n",
      "**2. JavaScript**\n",
      "- Runs natively in web browsers\n",
      "- Event-driven, asynchronous programming support\n",
      "- Prototype-based object orientation\n",
      "- Essential for front-end web development\n",
      "- Large ecosystem with frameworks like React, Angular, and Vue\n",
      "\n",
      "---\n",
      "\n",
      "**3. Java**\n",
      "- Statically typed and compiled to bytecode (runs on JVM)\n",
      "- Strong object-oriented programming support\n",
      "- Platform-independent (“write once, run anywhere”)\n",
      "- Robust standard library and tools\n",
      "- Commonly used for enterprise applications, Android development, and backend systems\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "## Completion Metrics\n",
      "*   **Model Used**: N/A\n",
      "*   **Input Tokens**: N/A\n",
      "*   **Output Tokens**: N/A\n",
      "*   **Total Tokens**: N/A\n",
      "*   **Estimated Cost**: N/A\n"
     ]
    }
   ],
   "source": [
    "# Example 2: OpenAI call with JSON response\n",
    "json_prompt = \"Generate a list of 3 programming languages with their key features.\"\n",
    "\n",
    "model = \"gpt-4.1\"\n",
    "openai_json_response = await call_model(\n",
    "    prompt=json_prompt,\n",
    "    model=model,  # or any available OpenAI model\n",
    "    # response_format=\"json\"\n",
    ")\n",
    "\n",
    "print(\"OpenAI JSON Response:\")\n",
    "print(\"-\" * 50)\n",
    "print(openai_json_response[\"usage_metadata\"])\n",
    "print(openai_json_response[\"content\"])\n",
    "print(format_metrics_section(\"gpt-4.1\",openai_json_response[\"usage_metadata\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellhorn_mcp.llm_manager import ChunkingStrategy\n",
    "from yellhorn_mcp.token_counter import TokenCounter\n",
    "\n",
    "chunks = ChunkingStrategy.split_by_paragraphs(\n",
    "    text=json_prompt,\n",
    "    max_tokens=5000,\n",
    "    token_counter=TokenCounter(),\n",
    "    model=\"gemini-2.0-flash-exp\"\n",
    ")\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(TokenCounter().count_tokens(chunk, \"gemini-2.0-flash-exp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/13/25 19:48:48] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.openai.com/v1/responses</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 </span>  <a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1740</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">OK\"</span>                                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/13/25 19:48:48]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttps://api.openai.com/v1/responses\u001b[0m \u001b[32m\"HTTP/1.1 200 \u001b[0m  \u001b]8;id=576016;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=881369;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\u001b\\\u001b[2m1740\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mOK\"\u001b[0m                                                                    \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini Response:\n",
      "--------------------------------------------------\n",
      "**Token Chunking: An Overview**\n",
      "\n",
      "**What is Token Chunking?**\n",
      "Token chunking is a technique used in natural language processing (NLP) to break down text into smaller, manageable pieces called \"tokens.\" These tokens can be words, phrases, or even characters, depending on the context and the specific application. The process involves segmenting a continuous stream of text into discrete units that can be analyzed or processed by algorithms.\n",
      "\n",
      "**Why is Token Chunking Important for Large Language Models?**\n",
      "\n",
      "1. **Efficiency**: Large language models (LLMs) often deal with vast amounts of text data. Token chunking allows these models to process text in smaller segments, making computations more efficient and manageable.\n",
      "\n",
      "2. **Context Preservation**: By chunking text into meaningful units, models can better understand the context and relationships between words or phrases. This is crucial for tasks like sentiment analysis, translation, and summarization.\n",
      "\n",
      "3. **Memory Management**: LLMs have limitations on the amount of text they can process at once (often referred to as the \"context window\"). Token chunking helps fit larger texts into these constraints by breaking them into smaller, coherent parts.\n",
      "\n",
      "4. **Improved Performance**: Models trained on chunked data can achieve better performance in various NLP tasks. This is because chunking helps maintain the semantic integrity of the text, allowing the model to learn more effectively.\n",
      "\n",
      "5. **Flexibility**: Different applications may require different chunking strategies (e.g., word-level, sentence-level, or paragraph-level). Token chunking provides the flexibility to adapt to these varying needs.\n",
      "\n",
      "In summary, token chunking is a fundamental technique that enhances the efficiency, context understanding, and overall performance of large language models in processing and analyzing text data.\n",
      "\n",
      "\n",
      "---\n",
      "## Completion Metrics\n",
      "*   **Model Used**: `gpt-4o-mini`\n",
      "*   **Input Tokens**: 15024\n",
      "*   **Output Tokens**: 356\n",
      "*   **Total Tokens**: 15380\n",
      "*   **Estimated Cost**: $0.0025\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Simple Gemini call\n",
    "prompt = \"Explain what token chunking is and why it's important for large language models.\"*1000\n",
    "system_message = \"You are a helpful AI assistant that provides clear and concise explanations.\"\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "gemini_response = await call_model(\n",
    "    prompt=prompt,\n",
    "    model=model,\n",
    "    system_message=system_message\n",
    ")\n",
    "\n",
    "print(\"Gemini Response:\")\n",
    "print(\"-\" * 50)\n",
    "print(gemini_response[\"content\"])\n",
    "print(format_metrics_section(model,gemini_response[\"usage_metadata\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<yellhorn_mcp.llm_manager.UsageMetadata at 0x10cfdd810>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_response[\"usage_metadata\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Grounded Search\n",
    "\n",
    "Google Search Grounding is a feature available for Gemini models that allows them to search the web and include citations in their responses. This is particularly useful for getting up-to-date information and verifying facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the search grounding utilities\n",
    "from yellhorn_mcp.search_grounding import _get_gemini_search_tools, add_citations_from_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/12/25 17:22:16] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> AFC is enabled with max remote calls: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>.                               <a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">models.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py#7118\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">7118</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/12/25 17:22:16]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m AFC is enabled with max remote calls: \u001b[1;36m10\u001b[0m.                               \u001b]8;id=502880;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py\u001b\\\u001b[2mmodels.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=125787;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py#7118\u001b\\\u001b[2m7118\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/12/25 17:22:40] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span>                                                     <a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1740</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">:generateContent</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/12/25 17:22:40]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m                                                     \u001b]8;id=471735;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=61373;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\u001b\\\u001b[2m1740\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro\u001b[0m \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;94m:generateContent\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m                                     \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response WITHOUT Search Grounding:\n",
      "--------------------------------------------------\n",
      "Of course. Here is a summary of Tesla's (TSLA) stock price and its recent performance.\n",
      "\n",
      "As an AI, I cannot give you real-time, up-to-the-second stock data. Stock prices are highly volatile and change constantly during market hours.\n",
      "\n",
      "However, I can provide you with the most recent closing price and a summary of its recent performance.\n",
      "\n",
      "### **Tesla (TSLA) Stock Price**\n",
      "\n",
      "For the most current, live price, please check a reliable financial news source like:\n",
      "\n",
      "*   **Google Finance**\n",
      "*   **Yahoo Finance**\n",
      "*   **Bloomberg**\n",
      "*   **Reuters**\n",
      "\n",
      "As of the market close on **June 17, 2024**, the approximate stock price for Tesla (TSLA) was:\n",
      "\n",
      "*   **~$187.44**\n",
      "\n",
      "### **Recent Performance Summary**\n",
      "\n",
      "Tesla's stock has had a very eventful and volatile year in 2024. Here is a breakdown of its recent performance:\n",
      "\n",
      "*   **Last Trading Day (June 17, 2024):** The stock saw a significant gain of over **+5%**. This surge was largely attributed to reports that Tesla has received approval to test its advanced driver-assistance system (FSD) on some streets in Shanghai, a key step for its rollout in China.\n",
      "*   **Last Week:** The stock has been on an upward trend, driven by optimism following the shareholder meeting where investors approved Elon Musk's $56 billion pay package and the company's move of incorporation to Texas. This was seen as a vote of confidence in Musk's leadership.\n",
      "*   **Year-to-Date (YTD):** Despite recent gains, TSLA is still down significantly in 2024. The stock started the year around $250, and its price is still down approximately **-25%** YTD.\n",
      "*   **One-Year Performance:** Over the past 12 months, the stock has underperformed the broader market (like the S&P 500), showing a decline of roughly **-28%**.\n",
      "\n",
      "### **Key Factors Influencing Recent Performance**\n",
      "\n",
      "Several key factors are driving the volatility and performance of Tesla's stock:\n",
      "\n",
      "1.  **Increased Competition:** Competition in the EV market has intensified, particularly from Chinese automakers like BYD, which has put pressure on Tesla's global market share.\n",
      "2.  **Price Cuts and Margins:** Tesla has implemented several price cuts globally to spur demand, which has concerned investors about declining automotive gross margins.\n",
      "3.  **Delivery Numbers:** Tesla's Q1 2024 delivery numbers missed analyst expectations, marking the first year-over-year decline in deliveries in four years. All eyes are now on the upcoming Q2 delivery report.\n",
      "4.  **Future Growth Narrative:** The stock's valuation is heavily dependent on future growth. Positive news around the **Cybertruck production ramp**, the potential for a **lower-cost \"Model 2\"**, and advancements in **Full Self-Driving (FSD)** and the **Optimus robot** are critical for investor sentiment.\n",
      "5.  **CEO and Shareholder Votes:** The recent overwhelming approval of Elon Musk's compensation package has removed a major overhang of uncertainty about his future focus and leadership at the company.\n",
      "\n",
      "In summary, while Tesla has seen a strong rebound in the last week on positive news, the stock has faced significant headwinds throughout 2024 due to rising competition and concerns about slowing growth.\n",
      "\n",
      "***Disclaimer:** This information is for informational purposes only and does not constitute financial advice. You should consult with a qualified financial professional before making any investment decisions.*\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Compare responses with and without search grounding\n",
    "comparison_prompt = \"What is the current stock price of Tesla (TSLA) and its recent performance?\"\n",
    "\n",
    "# First, make a call without search grounding by not passing tools\n",
    "response_without_search = await llm_manager.call_llm_with_usage(\n",
    "    prompt=comparison_prompt,\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"Response WITHOUT Search Grounding:\")\n",
    "print(\"-\" * 50)\n",
    "print(response_without_search[\"content\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/12/25 17:25:01] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> AFC is enabled with max remote calls: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>.                               <a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">models.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py#7118\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">7118</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/12/25 17:25:01]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m AFC is enabled with max remote calls: \u001b[1;36m10\u001b[0m.                               \u001b]8;id=429424;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py\u001b\\\u001b[2mmodels.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=622662;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py#7118\u001b\\\u001b[2m7118\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/12/25 17:25:17] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span>                                                     <a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1740</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">:generateContent</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/12/25 17:25:17]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m                                                     \u001b]8;id=455330;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=543820;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\u001b\\\u001b[2m1740\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro\u001b[0m \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;94m:generateContent\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m                                     \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response WITH Search Grounding:\n",
      "--------------------------------------------------\n",
      "## Tesla Stock Shows Volatility with Recent Dip But Long-Term Gains\n",
      "\n",
      "**As of Friday, July 11, 2025, Tesla (TSLA) closed at approximately $313.51, reflecting a slight increase of 1.17% in the last 24 hours of trading.** This comes amid a period of mixed performance for the electric vehicle giant.\n",
      "\n",
      "While the daily performance shows a modest gain, a broader look reveals a recent downturn. The stock has seen a decline of 1.41% over the past week and a more significant drop of 6.25% over the last month.\n",
      "\n",
      "However, looking at the longer-term picture, Tesla's stock has demonstrated substantial growth. Over the last year, it has surged by 19.07%, and in the last 12 months, the price has risen by 26.40%. This indicates underlying strength and investor confidence in the company's future prospects. The 52-week trading range for the stock has been between a low of $182.00 and a high of $488.54.\n",
      "\n",
      "Tesla's market capitalization currently stands at a robust $1.01 trillion. The company is a key player in the consumer cyclical sector, specializing in auto manufacturing. Beyond its well-known electric vehicles like the Model S, Model 3, Model X, and Model Y, Tesla is also a significant force in the energy generation and storage sector with products like Powerwall, Powerpack, and Megapack.\n",
      "\n",
      "✓ Found 4 citation sources\n"
     ]
    }
   ],
   "source": [
    "# Now make the same call with search grounding\n",
    "response_with_search = await llm_manager.call_llm_with_citations(\n",
    "    prompt=comparison_prompt,\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    temperature=0.0,\n",
    "    tools=_get_gemini_search_tools(\"gemini-2.5-pro\")\n",
    ")\n",
    "\n",
    "print(\"Response WITH Search Grounding:\")\n",
    "print(\"-\" * 50)\n",
    "print(response_with_search[\"content\"])\n",
    "\n",
    "# Show if citations were found\n",
    "if \"grounding_metadata\" in response_with_search:\n",
    "    grounding_meta = response_with_search[\"grounding_metadata\"]\n",
    "    if hasattr(grounding_meta, 'grounding_chunks') and grounding_meta.grounding_chunks:\n",
    "        print(f\"\\n✓ Found {len(grounding_meta.grounding_chunks)} citation sources\")\n",
    "else:\n",
    "    print(\"\\n✗ No grounding metadata found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test OpenAI Deep Research\n",
    "\n",
    "OpenAI Deep Research models (`o3-deep-research` and `o4-mini-deep-research`) are specialized models that can perform in-depth research and analysis. According to the CHANGELOG, these models automatically have access to `web_search_preview` and `code_interpreter` tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Regular vs Deep Research Models\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/12/25 17:47:38] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.openai.com/v1/responses</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 </span>  <a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1740</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">OK\"</span>                                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/12/25 17:47:38]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttps://api.openai.com/v1/responses\u001b[0m \u001b[32m\"HTTP/1.1 200 \u001b[0m  \u001b]8;id=242221;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=662087;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\u001b\\\u001b[2m1740\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mOK\"\u001b[0m                                                                    \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Model response:  {'### Implementation Plan for Real-Time Collaboration in a Code Editor\\n\\n#### 1. Choose a Synchronization Model\\n- **Operational Transformation (OT)**: Suitable for text-based collaboration, widely used in Google Docs.\\n- **CRDTs (Conflict-free Replicated Data Types)**: Better for decentralized systems, handles conflicts naturally.\\n\\n**Decision**: Use OT for simplicity and existing library support.\\n\\n#### 2. Communication Protocol\\n- **WebSocket**: Efficient for real-time, bidirectional communication.\\n- **WebRTC**: More complex, used for peer-to-peer connections.\\n\\n**Decision**: Use WebSocket for server-client architecture.\\n\\n#### 3. Conflict Resolution Strategy\\n- Implement server-side logic to handle conflicts using OT.\\n- Use a central server to maintain the document state and broadcast changes.\\n\\n#### 4. Performance at Scale\\n- Use horizontal scaling with load balancers.\\n- Implement sharding for document storage.\\n- Optimize WebSocket connections with libraries like `gevent` or `asyncio`.\\n\\n#### 5. Example Code\\n\\n```python\\n# Install necessary libraries\\n# pip install autobahn twisted\\n\\nfrom autobahn.twisted.websocket import WebSocketServerProtocol, WebSocketServerFactory\\nfrom twisted.internet import reactor\\nfrom twisted.python import log\\nimport sys\\n\\nlog.startLogging(sys.stdout)\\n\\nclass CodeEditorServerProtocol(WebSocketServerProtocol):\\n    def onConnect(self, request):\\n        print(f\"Client connecting: {request.peer}\")\\n\\n    def onOpen(self):\\n        print(\"WebSocket connection open.\")\\n\\n    def onMessage(self, payload, isBinary):\\n        if not isBinary:\\n            message = payload.decode(\\'utf8\\')\\n            print(f\"Text message received: {message}\")\\n            # Here, apply OT logic to update document state\\n            self.broadcastUpdate(message)\\n\\n    def onClose(self, wasClean, code, reason):\\n        print(f\"WebSocket connection closed: {reason}\")\\n\\n    def broadcastUpdate(self, message):\\n        # Broadcast the updated document state to all connected clients\\n        for client in self.factory.clients:\\n            client.sendMessage(message.encode(\\'utf8\\'))\\n\\nclass CodeEditorServerFactory(WebSocketServerFactory):\\n    def __init__(self, url):\\n        super().__init__(url)\\n        self.clients = []\\n\\n    def buildProtocol(self, addr):\\n        protocol = CodeEditorServerProtocol()\\n        protocol.factory = self\\n        self.clients.append(protocol)\\n        return protocol\\n\\nif __name__ == \\'__main__\\':\\n    factory = CodeEditorServerFactory(\"ws://localhost:9000\")\\n    reactor.listenTCP(9000, factory)\\n    reactor.run()\\n```\\n\\n#### 6. Testing and Deployment\\n- Test with multiple clients to ensure real-time updates.\\n- Deploy using a cloud provider with auto-scaling capabilities.\\n\\n#### 7. Future Enhancements\\n- Consider CRDTs for decentralized collaboration.\\n- Explore WebRTC for peer-to-peer connections if needed.\\n\\nThis plan provides a basic framework for implementing real-time collaboration in a code editor using Python.'}\n",
      "Regular Model (gpt-4o) Response Length: 2947 chars\n",
      "Token Usage: 0\n",
      "Estimated Cost: $0.0000\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Compare regular model vs Deep Research model on the same task\n",
    "comparison_task = \"\"\"\n",
    "Create a very short and concise implementation plan for adding real-time collaboration features \n",
    "to a code editor, similar to Google Docs but for code. Consider the most relevant python libraries and write example code and run it:\n",
    "- Operational Transformation vs CRDTs\n",
    "- WebSocket vs WebRTC\n",
    "- Conflict resolution strategies\n",
    "- Performance at scale\n",
    "\"\"\"\n",
    "\n",
    "print(\"Comparing Regular vs Deep Research Models\\n\")\n",
    "\n",
    "# First try with regular gpt-4o\n",
    "regular_model = \"gpt-4o\"\n",
    "try:\n",
    "    regular_response = await llm_manager.call_llm_with_usage(\n",
    "        prompt=comparison_task,\n",
    "        model=regular_model,\n",
    "        temperature=0.0\n",
    "    )\n",
    "\n",
    "    print(f\"Regular Model response: \", {regular_response[\"content\"]})\n",
    "    \n",
    "    print(f\"Regular Model ({regular_model}) Response Length: {len(regular_response['content'])} chars\")\n",
    "    print(f\"Token Usage: {regular_response['usage_metadata'].total_tokens}\")\n",
    "    estimated_cost = calculate_cost(\n",
    "        regular_model,\n",
    "        regular_response['usage_metadata'].prompt_tokens,\n",
    "        regular_response['usage_metadata'].completion_tokens\n",
    "    )\n",
    "    print(f\"Estimated Cost: ${estimated_cost:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error with {regular_model}: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/12/25 17:47:53] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Enabling Deep Research tools for model o4-mini-deep-research        <a href=\"file:///Users/sravanj/project_work/yellhorn-mcp/yellhorn_mcp/llm_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">llm_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/sravanj/project_work/yellhorn-mcp/yellhorn_mcp/llm_manager.py#508\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">508</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/12/25 17:47:53]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Enabling Deep Research tools for model o4-mini-deep-research        \u001b]8;id=964967;file:///Users/sravanj/project_work/yellhorn-mcp/yellhorn_mcp/llm_manager.py\u001b\\\u001b[2mllm_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=52942;file:///Users/sravanj/project_work/yellhorn-mcp/yellhorn_mcp/llm_manager.py#508\u001b\\\u001b[2m508\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/12/25 17:50:43] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.openai.com/v1/responses</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 </span>  <a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1740</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">OK\"</span>                                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/12/25 17:50:43]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttps://api.openai.com/v1/responses\u001b[0m \u001b[32m\"HTTP/1.1 200 \u001b[0m  \u001b]8;id=449305;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=618910;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\u001b\\\u001b[2m1740\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mOK\"\u001b[0m                                                                    \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Research Model (o4-mini-deep-research) Response Length: 3340 chars\n",
      "Token Usage: 0\n",
      "Estimated Cost: $0.0000\n",
      "\n",
      "First 500 chars of Deep Research response:\n",
      "- **OT vs CRDT:** Modern editors often favor CRDTs (like Ypy/Y-CRDT) for simpler merge semantics and offline edits.  CRDTs ensure *eventual consistency* without explicit coordination ([www.codingeasypeasy.com](https://www.codingeasypeasy.com/blog/build-a-real-time-collaborative-editor-with-fastapi-and-crdts-a-comprehensive-guide#:~:text=scenario,are%20applied%20in%20different%20orders)), whereas OT requires complex transform functions and a central server.  For example, a toy CRDT string merge (...\n"
     ]
    }
   ],
   "source": [
    "# Now try with deep research model\n",
    "deep_model = \"o4-mini-deep-research\"\n",
    "try:\n",
    "    deep_response = await llm_manager.call_llm_with_usage(\n",
    "        prompt=comparison_task,\n",
    "        model=deep_model,\n",
    "        temperature=0.0\n",
    "    )\n",
    "\n",
    "    print(f\"Deep Model response: \", {deep_response[\"content\"]})\n",
    "    \n",
    "    print(f\"Deep Research Model ({deep_model}) Response Length: {len(deep_response['content'])} chars\")\n",
    "    print(f\"Token Usage: {deep_response['usage_metadata'].total_tokens}\")\n",
    "    estimated_cost = calculate_cost(\n",
    "        deep_model,\n",
    "        deep_response['usage_metadata'].prompt_tokens,\n",
    "        deep_response['usage_metadata'].completion_tokens\n",
    "    )\n",
    "    print(f\"Estimated Cost: ${estimated_cost:.4f}\")\n",
    "    \n",
    "    # Show a snippet of the response to see the difference\n",
    "    print(\"\\nFirst 500 chars of Deep Research response:\")\n",
    "    print(deep_response['content'][:500] + \"...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error with {deep_model}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **OT vs CRDT:** Modern editors often favor CRDTs (like Ypy/Y-CRDT) for simpler merge semantics and offline edits.  CRDTs ensure *eventual consistency* without explicit coordination ([www.codingeasypeasy.com](https://www.codingeasypeasy.com/blog/build-a-real-time-collaborative-editor-with-fastapi-and-crdts-a-comprehensive-guide#:~:text=scenario,are%20applied%20in%20different%20orders)), whereas OT requires complex transform functions and a central server.  For example, a toy CRDT string merge (using unique IDs) might look like:\n",
      "  \n",
      "  ```python\n",
      "  doc1 = []\n",
      "  doc2 = []\n",
      "  doc1.append(('1_A', 'A'))   # user1 inserts 'A'\n",
      "  doc2.append(('1_B', 'B'))   # user2 inserts 'B'\n",
      "  merged = sorted(doc1 + doc2, key=lambda x: x[0])\n",
      "  print(''.join(char for _,char in merged))  # AB\n",
      "  ```\n",
      "\n",
      "- **WebSocket vs WebRTC:** Use WebSockets for a server-based broadcast model (e.g. with Python’s `websockets` or FastAPI), as in many CRDT stacks.  (WebRTC/P2P is possible but adds browser–signaling overhead.)  For instance, a simple WebSocket echo server/client pair with `websockets`:\n",
      "  \n",
      "  ```python\n",
      "  import asyncio, websockets\n",
      "\n",
      "  async def handler(ws, path):\n",
      "      async for msg in ws:\n",
      "          await ws.send(f\"Echo: {msg}\")\n",
      "\n",
      "  async def main():\n",
      "      server = await websockets.serve(handler, \"localhost\", 8765)\n",
      "      async with websockets.connect(\"ws://localhost:8765\") as ws:\n",
      "          await ws.send(\"Hello\")\n",
      "          print(await ws.recv())  # Echo: Hello\n",
      "      server.close()\n",
      "      await server.wait_closed()\n",
      "\n",
      "  asyncio.run(main())\n",
      "  ```\n",
      "\n",
      "- **Conflict resolution strategies:** With CRDTs, conflicts are resolved automatically by merge rules (each character or operation has a globally unique ID), avoiding manual conflict logic ([www.codingeasypeasy.com](https://www.codingeasypeasy.com/blog/build-a-real-time-collaborative-editor-with-fastapi-and-crdts-a-comprehensive-guide#:~:text=scenario,are%20applied%20in%20different%20orders)).  With OT, you must implement transform functions (like inclusion/exclusion transforms) to reorder edits.  For simple data, one might fall back to *last-writer-wins* or version vectors.  In practice, libraries like Ypy handle the merge under the hood, while OT libraries (e.g. `python-ottype`) provide transform APIs.\n",
      "\n",
      "- **Performance at scale:** Use incremental updates and broadcasts rather than full-text snaps. Batch operations or diffs (state-vectors or deltas) for network efficiency.  Scale WebSocket servers with async frameworks (Uvicorn/Gunicorn or `pycrdt-websocket` like Jupyter’s) and use horizontal sharding or pub/sub (Redis) for multi-server sync.  Persist history (e.g. YDoc checkpoints) so new clients can catch up without replaying all ops.  In short, optimize by compressing CRDT updates and distributing load across nodes.  \n",
      "\n",
      "**Sources:** CRDTs guarantee eventual consistency without coordination ([www.codingeasypeasy.com](https://www.codingeasypeasy.com/blog/build-a-real-time-collaborative-editor-with-fastapi-and-crdts-a-comprehensive-guide#:~:text=scenario,are%20applied%20in%20different%20orders)); JupyterLab’s collaborative editing uses a shared Y-CRDT (YDoc) over WebSocket endpoints ([jupyterlab-realtime-collaboration.readthedocs.io](https://jupyterlab-realtime-collaboration.readthedocs.io/en/latest/developer/architecture.html#:~:text=,file%20management%20and%20kernel%20system)).\n"
     ]
    }
   ],
   "source": [
    "print(deep_response['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Differences Between Search Grounding and Deep Research\n",
    "\n",
    "1. **Search Grounding (Gemini)**:\n",
    "   - Automatically searches the web for relevant information\n",
    "   - Adds inline citations to responses\n",
    "   - Best for factual queries requiring current information\n",
    "   - No additional cost beyond regular API usage\n",
    "\n",
    "2. **Deep Research Models (OpenAI)**:\n",
    "   - Specialized models with web search and code interpreter tools\n",
    "   - Designed for complex, multi-step research tasks\n",
    "   - Can execute code and analyze results\n",
    "   - Higher cost but more comprehensive analysis\n",
    "   - May require special API access\n",
    "\n",
    "Both features enhance the LLM's ability to provide accurate, up-to-date information, but they serve different use cases and have different cost/performance tradeoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Replace existing model with unified Model Calling Service (OpenRouter or Litellm)\"\n",
    "description = \"\"\"\n",
    "Describe how to replace and consolidate LLM model calls in yellhorn with Gemini, Open AI, etc. with OpenRouter or LiteLLM\n",
    "\"\"\"\n",
    "\n",
    "user_task = f\"Title: {title}, Description: {description}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Curate Context at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.mock_context import run_curate_context, mock_github_command\n",
    "from yellhorn_mcp.server import curate_context\n",
    "from yellhorn_mcp.utils.git_utils import run_git_command_with_set_cwd, run_github_command_with_set_cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 16:02:06,394 INFO AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting context curation process\n",
      "[INFO] Deleted existing .yellhorncontext file before analysis\n",
      "[INFO] Getting codebase context using full mode\n",
      "[INFO] Getting codebase snapshot in mode: full\n",
      "[INFO] Found .gitignore with 86 patterns\n",
      "[INFO] Codebase context metrics: 12009 files, 110173 tokens based on (gemini-2.5-flash)\n",
      "[INFO] Extracted 11 directories from 38 filtered files\n",
      "[INFO] Directory context:\n",
      "<codebase_tree>\n",
      ".\n",
      "├── .mcp.json\n",
      "├── .python-version\n",
      "├── CHANGELOG.md\n",
      "├── CLAUDE.md\n",
      "├── LLMManagerREADME.md\n",
      "├── README.md\n",
      "├── coverage_stats.txt\n",
      "├── pyproject.toml\n",
      "├── pyrightconfig.json\n",
      "│   ├── workflows/\n",
      "│   │   ├── publish.yml\n",
      "│   │   └── tests.yml\n",
      "├── docs/\n",
      "│   ├── USAGE.md\n",
      "│   └── coverage_baseline.md\n",
      "├── notebooks/\n",
      "│   ├── file_structure.ipynb\n",
      "│   ├── llm_manager.ipynb\n",
      "│   └── test_curate_context_v2.ipynb\n",
      "├── yellhorn_mcp/\n",
      "│   ├── __init__.py\n",
      "│   ├── cli.py\n",
      "│   ├── llm_manager.py\n",
      "│   ├── me...\n",
      "[INFO] Analyzing directory structure with gemini-2.5-flash\n",
      "[INFO] [DEBUG] System message: You are an expert software developer tasked with analyzing a codebase structure to identify important directories for building and executing a workplan.\n",
      "\n",
      "Your goal is to identify the most important directories that should be included for the user's task.\n",
      "\n",
      "Analyze the directories and identify the ones that:\n",
      "1. Contain core application code relevant to the user's task\n",
      "2. Likely contain important business logic\n",
      "3. Would be essential for understanding the codebase architecture\n",
      "4. Are needed to implement the requested task\n",
      "5. Contain SDKs or libraries relevant to the user's task\n",
      "\n",
      "Ignore directories that:\n",
      "1. Contain only build artifacts or generated code\n",
      "2. Store dependencies or vendor code\n",
      "3. Contain temporary or cache files\n",
      "4. Probably aren't relevant to the user's specific task\n",
      "\n",
      "User Task: Title: Replace existing model with unified Model Calling Service (OpenRouter or Litellm), Description: \n",
      "Describe how to replace and consolidate LLM model calls in yellhorn with Gemini, Open AI, etc. with OpenRouter or LiteLLM\n",
      "\n",
      "\n",
      "Return your analysis as a list of important directories, one per line, without any additional text or formatting as below:\n",
      "\n",
      "```context\n",
      "dir1/subdir1/\n",
      "dir2/\n",
      "dir3/subdir3/file3.filetype\n",
      "```\n",
      "\n",
      "Prefer to include directories, and not just file paths but include just file paths when appropriate.\n",
      "Don't include explanations for your choices, just return the list in the specified format.\n",
      "[INFO] [DEBUG] User prompt (461525 chars): <codebase_tree>\n",
      ".\n",
      "├── .mcp.json\n",
      "├── .python-version\n",
      "├── CHANGELOG.md\n",
      "├── CLAUDE.md\n",
      "├── LLMManagerREADME.md\n",
      "├── README.md\n",
      "├── coverage_stats.txt\n",
      "├── pyproject.toml\n",
      "├── pyrightconfig.json\n",
      "│   ├── workflows/\n",
      "│   │   ├── publish.yml\n",
      "│   │   └── tests.yml\n",
      "├── docs/\n",
      "│   ├── USAGE.md\n",
      "│   └── coverage_baseline.md\n",
      "├── notebooks/\n",
      "│   ├── file_structure.ipynb\n",
      "│   ├── llm_manager.ipynb\n",
      "│   └── test_curate_context_v2.ipynb\n",
      "├── yellhorn_mcp/\n",
      "│   ├── __init__.py\n",
      "│   ├── cli.py\n",
      "│   ├── llm_manager.py\n",
      "│   ├── metadata_models.py\n",
      "│   ├── server.py\n",
      "│   └── token_counter.py\n",
      "│   ├── formatters/\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── codebase_snapshot.py\n",
      "│   │   ├── context_fetcher.py\n",
      "│   │   └── prompt_formatter.py\n",
      "│   ├── integrations/\n",
      "│   │   ├── gemini_integration.py\n",
      "│   │   └── github_integration.py\n",
      "│   ├── models/\n",
      "│   │   └── metadata_models.py\n",
      "│   ├── processors/\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── context_processor.py\n",
      "│   │   ├── judgement_processor.py\n",
      "│   │   └── workplan_processor.py\n",
      "│   ├── utils/\n",
      "│   │   ├── comment_utils.py\n",
      "│   │   ├── cost_tracker_utils.py\n",
      "│   │   ├── git_utils.py\n",
      "│   │   ├── lsp_utils.py\n",
      "│   │   └── search_grounding_utils.py\n",
      "</codebase_tree>\n",
      "\n",
      "<file_contents>\n",
      "\n",
      "--- File: .github/workflows/publish.yml ---\n",
      "name: Publish to PyPI\n",
      "\n",
      "on:\n",
      "  push:\n",
      "    tags:\n",
      "      - 'v*'\n",
      "\n",
      "jobs:\n",
      "  deploy:\n",
      "    runs-on: ubuntu-latest\n",
      "    steps:\n",
      "    - uses: actions/checkout@v3\n",
      "    - name: Set up Python\n",
      "      uses: actions/setup-python@v4\n",
      "      with:\n",
      "        python-version: '3.10'\n",
      "    - name: Install dependencies\n",
      "      run: |\n",
      "        python -m pip install --upgrade pip\n",
      "        pip install build twine\n",
      "    - name: Verify version matches tag\n",
      "      run: |\n",
      "        # Extract version from the tag (remove 'v' prefix)\n",
      "        TAG_VERSION=${GITHUB_REF#refs/tags/v}\n",
      "        # Extract version from pyproject.toml\n",
      "        PROJECT_VERSION=$(grep -m 1 'version = ' pyproject.toml | cut -d '\"' -f 2)\n",
      "        echo \"Tag version: $TAG_VERSION\"\n",
      "        echo \"Project version: $PROJECT_VERSION\"\n",
      "        if [ \"$TAG_VERSION\" != \"$PROJECT_VERSION\" ]; then\n",
      "          echo \"::error::Version mismatch! Tag ($TAG_VERSION) does not match pyproject.toml ($PROJECT_VERSION)\"\n",
      "          exit 1\n",
      "        fi\n",
      "    - name: Build package\n",
      "      run: python -m build\n",
      "    - name: Publish package\n",
      "      uses: pypa/gh-action-pypi-publish@release/v1\n",
      "      with:\n",
      "        password: ${{ secrets.PYPI_API_TOKEN }}\n",
      "\n",
      "--- File: .github/workflows/tests.yml ---\n",
      "name: Tests\n",
      "\n",
      "on:\n",
      "  push:\n",
      "    branches: [ main ]\n",
      "  pull_request:\n",
      "    branches: [ main ]\n",
      "\n",
      "jobs:\n",
      "  test:\n",
      "    runs-on: ubuntu-latest\n",
      "    strategy:\n",
      "      matrix:\n",
      "        python-version: [\"3.10\", \"3.11\"]\n",
      "\n",
      "    steps:\n",
      "    - uses: actions/checkout@v3\n",
      "    - name: Set up Python ${{ matrix.python-version }}\n",
      "      uses: actions/setup-python@v4\n",
      "      with:\n",
      "        python-version: ${{ matrix.python-version }}\n",
      "        cache: 'pip'\n",
      "    - name: Install dependencies\n",
      "      run: |\n",
      "        python -m pip install --upgrade pip\n",
      "        pip install .[dev]\n",
      "    - name: Format check with black\n",
      "      run: |\n",
      "        black --check yellhorn_mcp tests\n",
      "    - name: Import sort check with isort\n",
      "      run: |\n",
      "        isort --check yellhorn_mcp tests\n",
      "    - name: Test with pytest (coverage)\n",
      "      run: |\n",
      "        pytest --cov=yellhorn_mcp --cov=examples --cov-report=xml --cov-report=term\n",
      "    - name: Fail if coverage below threshold\n",
      "      run: |\n",
      "        python - <<'PY'\n",
      "        import xml.etree.ElementTree as ET, sys\n",
      "        tree = ET.parse('coverage.xml')\n",
      "        root = tree.getroot()\n",
      "        total = root.attrib.get('line-rate')\n",
      "        if float(total) < 0.70:\n",
      "            print(f\"::error::Coverage too low: {float(total)*100:.2f}% (required 70%)\")\n",
      "            sys.exit(1)\n",
      "        else:\n",
      "            print(f\"Coverage passed: {float(total)*100:.2f}% (required ≥70%)\")\n",
      "        PY\n",
      "\n",
      "--- File: .mcp.json ---\n",
      "{\n",
      "  \"mcpServers\": {\n",
      "    \"yellhorn-mcp\": {\n",
      "      \"type\": \"stdio\",\n",
      "      \"command\": \"yellhorn-mcp\",\n",
      "      \"args\": [],\n",
      "      \"env\": {}\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "--- File: .python-version ---\n",
      "yellhorn-mcp-3.12.2\n",
      "\n",
      "--- File: CHANGELOG.md ---\n",
      "# Changelog\n",
      "\n",
      "All notable changes to this project will be documented in this file.\n",
      "\n",
      "The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\n",
      "and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n",
      "\n",
      "## [Unreleased]\n",
      "\n",
      "## [0.7.0] - 2025-07-18\n",
      "\n",
      "### Added\n",
      "\n",
      "- **Unified LLM Manager**: New centralized LLM management system (`LLMManager` class) that provides:\n",
      "  - Unified interface for both OpenAI and Gemini models\n",
      "  - Automatic prompt chunking when content exceeds model context limits\n",
      "  - Intelligent chunking strategies (paragraph-based and sentence-based)\n",
      "  - Response aggregation for chunked calls\n",
      "  - Configurable overlap between chunks for better context preservation\n",
      "\n",
      "- **Enhanced Retry Logic**: Robust retry mechanism with exponential backoff:\n",
      "  - Automatic retry on rate limits and transient failures\n",
      "  - Configurable retry attempts (default: 5) with exponential backoff\n",
      "  - Support for both OpenAI `RateLimitError` and Gemini `Resour...\n",
      "[INFO] Found .yellhornignore with 1 patterns\n",
      "[INFO] File categorization results out of 72 files:\n",
      "[INFO]   - 5 always ignored (images, binaries, configs, etc.)\n",
      "[INFO]   - 0 in yellhorncontext whitelist (included)\n",
      "[INFO]   - 0 in yellhorncontext blacklist (excluded)\n",
      "[INFO]   - 0 in yellhornignore whitelist (included)\n",
      "[INFO]   - 29 in yellhornignore blacklist (excluded)\n",
      "[INFO]   - 38 other files (included - no .yellhorncontext)\n",
      "[INFO] Total included: 38 files (excluded 5 always-ignored files)\n",
      "[INFO] Read contents of 38 files\n",
      "[INFO] Analysis complete, found 17 important directories: CLAUDE.md, LLMManagerREADME.md, docs/USAGE.md, notebooks/llm_manager.ipynb, notebooks/test_curate_context_v2.ipynb, ... (12 more)\n",
      "[INFO] Processing complete, identified 17 important directories\n",
      "[INFO] Successfully wrote .yellhorncontext file to /Users/sravanj/project_work/yellhorn-mcp/.yellhorncontext\n",
      "[INFO] Waiting for 0 background tasks...\n",
      "[INFO] All background tasks completed\n",
      "Path to yellhorn context\n",
      "{\"status\": \"\\u2705 Context curation completed successfully\", \"message\": \"Successfully created .yellhorncontext file at /Users/sravanj/project_work/yellhorn-mcp/.yellhorncontext with 15 files and 2 directories.\"}\n"
     ]
    }
   ],
   "source": [
    "# Call create_workplan with our mock context\n",
    "context_result = await (run_curate_context(\n",
    "        user_task=user_task,\n",
    "        repo_path=REPO_PATH,\n",
    "        gemini_client=gemini_client,\n",
    "        openai_client=openai_client,\n",
    "        llm_manager=llm_manager,\n",
    "        model=MODEL,\n",
    "        codebase_reasoning=\"full\",  # Use \"none\" for faster processing\n",
    "        log_callback=log_callback,\n",
    "        git_command_func=run_git_command_with_set_cwd(REPO_PATH),\n",
    "        debug=True\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Path to yellhorn context\")\n",
    "print(context_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load .yellhorncontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted path: /Users/sravanj/project_work/yellhorn-mcp/.yellhorncontext\n",
      "Yellhorn context:\n",
      "# Yellhorn Context File - AI context optimization\n",
      "# Generated by yellhorn-mcp curate_context tool\n",
      "# Based on task: Title: Replace existing model with unified Model Calling Service (OpenRouter or \n",
      "\n",
      "# Important directories to specifically include\n",
      "CLAUDE.md\n",
      "LLMManagerREADME.md\n",
      "docs/USAGE.md\n",
      "notebooks/llm_manager.ipynb\n",
      "notebooks/test_curate_context_v2.ipynb\n",
      "pyproject.toml\n",
      "yellhorn_mcp/cli.py\n",
      "yellhorn_mcp/integrations/gemini_integration.py\n",
      "yellhorn_mcp/llm_manager.py\n",
      "yellhorn_mcp/models/metadata_models.py\n",
      "yellhorn_mcp/server.py\n",
      "yellhorn_mcp/token_counter.py\n",
      "yellhorn_mcp/utils/cost_tracker_utils.py\n",
      "yellhorn_mcp/utils/lsp_utils.py\n",
      "yellhorn_mcp/utils/search_grounding_utils.py\n",
      "yellhorn_mcp/formatters/\n",
      "yellhorn_mcp/processors/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse the path from the context_result string\n",
    "import re\n",
    "\n",
    "# Extract the file path using regex\n",
    "match = re.search(r'at\\s+(.+?)\\s+with', context_result)\n",
    "if match:\n",
    "    context_file_path = match.group(1)\n",
    "    print(f\"Extracted path: {context_file_path}\")\n",
    "else:\n",
    "    # Fallback: try to find any path-like string\n",
    "    match = re.search(r'(/[^\\s]+\\.yellhorncontext)', context_result)\n",
    "    if match:\n",
    "        context_file_path = match.group(1)\n",
    "        print(f\"Extracted path: {context_file_path}\")\n",
    "    else:\n",
    "        print(\"Could not extract path from result string\")\n",
    "        context_file_path = None\n",
    "\n",
    "# Now you can use context_file_path to read the file\n",
    "if context_file_path:\n",
    "    with open(context_file_path, 'r') as f:\n",
    "        context = f.read()\n",
    "    print(\"Yellhorn context:\")\n",
    "    print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using Create Workplan\n",
    "\n",
    "Now let's demonstrate how to use the create_workplan MCP tool to generate implementation plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.mock_context import run_create_workplan, mock_github_command\n",
    "from yellhorn_mcp.server import process_workplan_async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 16:08:47,515 INFO AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Getting codebase snapshot in mode: full\n",
      "[INFO] Found .gitignore with 86 patterns\n",
      "[INFO] Attempting to enable search grounding for model gemini-2.5-flash\n",
      "[INFO] Search grounding enabled for model gemini-2.5-flash\n",
      "[INFO] Found .yellhornignore with 1 patterns\n",
      "[INFO] Found .yellhorncontext with 17 whitelist, 0 blacklist, and 0 negation patterns\n",
      "[INFO] File categorization results out of 73 files:\n",
      "[INFO]   - 6 always ignored (images, binaries, configs, etc.)\n",
      "[INFO]   - 23 in yellhorncontext whitelist (included)\n",
      "[INFO]   - 0 in yellhorncontext blacklist (excluded)\n",
      "[INFO]   - 0 in yellhornignore whitelist (included)\n",
      "[INFO]   - 0 in yellhornignore blacklist (excluded)\n",
      "[INFO]   - 0 other files (excluded - .yellhorncontext exists)\n",
      "[INFO] Total included: 23 files (excluded 6 always-ignored files)\n",
      "[INFO] Read contents of 23 files\n",
      "[MOCK GitHub CLI] Running: gh issue edit 2760 --body # Replace existing model with unified Model Calling Service (OpenRouter or Litellm)\n",
      "\n",
      "## Refactor LLM Management with Unified Model Calling Service (LiteLLM)\n",
      "\n",
      "### Summary\n",
      "The current `yellhorn-mcp` codebase utilizes direct API calls to OpenAI and Google Gemini models, leading to fragmented LLM interaction logic and increased maintenance overhead for supporting new providers. This work plan proposes to replace and consolidate all LLM model calls through a unified model calling service, LiteLLM. This integration will simplify LLM API interactions, streamline the addition of new models and providers, and centralize advanced features like retry logic and cost tracking, thereby enhancing the system's flexibility, maintainability, and scalability. The primary subsystems affected will be `yellhorn_mcp/llm_manager.py`, `yellhorn_mcp/server.py`, and related utility and configuration files.\n",
      "\n",
      "### Technical Details\n",
      "*   **Languages & Frameworks**: Python 3.10+\n",
      "*   **External Dependencies**:\n",
      "    *   `litellm~=1.33.0`: This new dependency will be added to `pyproject.toml` under `[project] dependencies`. LiteLLM provides a unified interface to over 100 LLM APIs, including OpenAI and Gemini, simplifying API calls and offering built-in features like retry logic and cost tracking.[1](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG4Ty1kIv8-x706DKH3z0MyTlkEO8W57vOoDQOfso2nMsh9xh2BNH93Zg-7QbiF8-0CBxgxiyt3VuInjEF2v0egmfYTmiNcdHEDT6Id4ChPxXEX3wuBraaQMTjxKfKfsdgM_OdlbKP6n4hHZpfv8Z0Dq_1WI8VknpKOaGbluD5pvvgochg=), [2](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNO9XSCuQVloTCtVZqw58jsi_tTJFmx2aAJdBRQlPbj69lF2ZDEeGXdC4DOfLIXQ9wnw8vmYjsUreGa5rXUZogzcTaVIPrrHdIc-MoZp93nZIdnsLsynw=), [3](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHlyoBNK5-LKhYp37fYMMonXDdEhq0L2HOStpzDqK6Q8k29EGQBFuCvilzsLfcHyYt9xJdTW8jE2Wz8PAE91IOy5TT3WTDN4NOE9z84iS97Ix0-gVv17hI4sYRhfiK3Kh5Ak2KBhLJUVGxRzPqdJMp3sCjRj1nPP8ZAIOJasfsJrkzXU8Rh5ztGaAkie4Iz)\n",
      "*   **Dependency Management & Pinning Strategy**: `poetry` (as indicated by `pyproject.toml`). The new `litellm` dependency will be pinned using the `~=` operator for compatible releases.\n",
      "*   **Build, Lint, Formatting, Type-checking Commands**:\n",
      "    *   `python -m black yellhorn_mcp tests`\n",
      "    *   `python -m isort yellhorn_mcp tests`\n",
      "    *   `python -m pytest`\n",
      "    *   Type-checking will be performed via `pyright` (configured in `pyrightconfig.json`).\n",
      "*   **Logging & Observability**: Existing `logging` module will be leveraged. Ensure LiteLLM's internal logging integrates seamlessly or is configured to use the existing logging setup.\n",
      "*   **Analytics/KPIs**: Existing cost tracking and usage metadata (`yellhorn_mcp/utils/cost_tracker_utils.py`, `yellhorn_mcp/models/metadata_models.py`) will be adapted to consume LiteLLM's unified usage reports.\n",
      "*   **Testing Frameworks & Helpers**: `pytest`, `pytest-asyncio`, `httpx`, `pytest-cov` will continue to be used. Existing mock testing infrastructure (`examples/mock_context.py`) will be adapted to mock LiteLLM calls.\n",
      "\n",
      "### Architecture\n",
      "*   **Existing Components Leveraged**:\n",
      "    *   `yellhorn_mcp/llm_manager.py`: The core `LLMManager` class will be refactored to use LiteLLM as its underlying LLM interaction layer. Its existing logic for chunking (`ChunkingStrategy`), token counting (`TokenCounter`), and usage metadata aggregation (`UsageMetadata`) will be adapted to work with LiteLLM's outputs.\n",
      "    *   `yellhorn_mcp/token_counter.py`: Will continue to provide model-specific token limits and counting, potentially integrating with LiteLLM's internal token counting utilities if beneficial.\n",
      "    *   `yellhorn_mcp/models/metadata_models.py`: `UsageMetrics` and `CompletionMetadata` will continue to track LLM usage, adapted for LiteLLM's reporting format.\n",
      "    *   `yellhorn_mcp/utils/cost_tracker_utils.py`: `calculate_cost` and `format_metrics_section` will be updated to map LiteLLM's model identifiers to existing pricing.\n",
      "    *   `yellhorn_mcp/utils/search_grounding_utils.py`: The logic for configuring Google Search tools and adding citations will be adapted to pass through LiteLLM's API.\n",
      "    *   `yellhorn_mcp/processors/`: All processors (`context_processor.py`, `judgement_processor.py`, `workplan_processor.py`) will continue to use `LLMManager` as their LLM interface.\n",
      "*   **New Components Introduced**: No new top-level components will be introduced. The change is primarily an internal refactoring of the `LLMManager` to use LiteLLM.\n",
      "*   **Control-flow & Data-flow Diagram (ASCII)**:\n",
      "    ```\n",
      "    +-----------------+       +-------------------+       +-------------------+\n",
      "    | MCP Tools (CLI, |       | yellhorn_mcp/     |       | yellhorn_mcp/     |\n",
      "    | VSCode, Claude) |------>| server.py         |------>| llm_manager.py    |\n",
      "    +-----------------+       | (Tool Dispatcher) |       | (Unified LLM API) |\n",
      "                               +-------------------+       +-------------------+\n",
      "                                         ^                         |\n",
      "                                         |                         |\n",
      "                                         |                         |\n",
      "                                         |                         |\n",
      "                                         |                         v\n",
      "                                         |                   +-----------------+\n",
      "                                         |                   | LiteLLM Library |\n",
      "                                         |                   | (litellm.py)    |\n",
      "                                         |                   +-----------------+\n",
      "                                         |                         |\n",
      "                                         |                         |\n",
      "                                         |                         |\n",
      "                                         |                         v\n",
      "                                         |                   +-----------------+\n",
      "                                         |                   | OpenAI API      |\n",
      "                                         |                   | Gemini API      |\n",
      "                                         |                   | Other LLM APIs  |\n",
      "                                         |                   +-----------------+\n",
      "                                         |\n",
      "                                         +-------------------------------------+\n",
      "                                         (Usage Metadata, Responses, Errors)\n",
      "    ```\n",
      "*   **State-management, Retry/Fallback, and Error-handling Patterns**: LiteLLM offers robust built-in retry and fallback mechanisms.[2](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNO9XSCuQVloTCtVZqw58jsi_tTJFmx2aAJdBRQlPbj69lF2ZDEeGXdC4DOfLIXQ9wnw8vmYjsUreGa5rXUZogzcTaVIPrrHdIc-MoZp93nZIdnsLsynw=) The existing `api_retry` decorator in `llm_manager.py` will be removed, and `LLMManager` will rely on LiteLLM's internal retry logic. Error handling will be adapted to catch LiteLLM-specific exceptions and translate them into `YellhornMCPError` where appropriate.\n",
      "\n",
      "### Completion Criteria & Metrics\n",
      "*   **Engineering Metrics**:\n",
      "    *   All `pytest` unit and integration tests pass with 100% success rate.\n",
      "    *   Test coverage for `yellhorn_mcp` remains at or above 70% line coverage.\n",
      "    *   `black` and `isort` formatting checks pass.\n",
      "    *   `pyright` type-checking runs clean with no errors.\n",
      "    *   `LLMManager` successfully makes calls to both OpenAI and Gemini models via LiteLLM.\n",
      "    *   Cost tracking and usage metadata reported in GitHub comments remain accurate and consistent with previous versions.\n",
      "*   **Business Metrics**:\n",
      "    *   Reduced time-to-integrate new LLM providers by leveraging LiteLLM's unified interface.\n",
      "    *   Improved system reliability due to LiteLLM's robust retry and fallback mechanisms.\n",
      "*   **Code-state Definition of Done**:\n",
      "    *   All CI jobs (linting, formatting, testing) are green.\n",
      "    *   `pyproject.toml` is updated with the new `litellm` dependency.\n",
      "    *   `LLMManagerREADME.md` and `docs/USAGE.md` are updated to reflect the LiteLLM integration.\n",
      "\n",
      "### References\n",
      "*   `yellhorn_mcp/llm_manager.py`\n",
      "*   `yellhorn_mcp/server.py`\n",
      "*   `yellhorn_mcp/cli.py`\n",
      "*   `yellhorn_mcp/token_counter.py`\n",
      "*   `yellhorn_mcp/integrations/gemini_integration.py`\n",
      "*   `yellhorn_mcp/utils/cost_tracker_utils.py`\n",
      "*   `yellhorn_mcp/utils/search_grounding_utils.py`\n",
      "*   `pyproject.toml`\n",
      "*   LiteLLM GitHub Repository: `https://github.com/BerriAI/litellm`[4](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHJqrTQMqVpo30aziDctGWaQBTciLRLQ7cxR_-2L18AA-WDtAeW9Tc15Iulaz3EocAd6-xx6fScsj_J_o8JGGnbw4Umaj6-fP6WKin0aBn91jDU9DRzRDn6aHscPA==)\n",
      "*   LiteLLM Documentation: `https://docs.litellm.ai/`[5](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE7TAfZPbbVvO9V1PNxONobzzviteU9UhFpNZDcEBmmds4AFNHD3ve1SKZzeXa0X9_gmmsP5LkeBeFliaiF_qbJbV6oETAKOuLHrSY5jSvChIQC6H3FfQG08-jmKCzRQv0=), [2](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNO9XSCuQVloTCtVZqw58jsi_tTJFmx2aAJdBRQlPbj69lF2ZDEeGXdC4DOfLIXQ9wnw8vmYjsUreGa5rXUZogzcTaVIPrrHdIc-MoZp93nZIdnsLsynw=), [6](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG7LN0z9TdU0oWVk8IIjcNM2V5AwitnYaZ-qI9rf39ZkfkJY2RKVT_n-_7ZGXVmSnpQUZhy7xYafWgPWoH-uLFo1hTUD_FR4gh70OMHcD0r3Xnyj2xp1C1MS7iM3jgMnBsDCjkduQDfz5uk_xOGbD0ptegACqLFiQsWs1bcvY6MW1DJ6yc52PuDzj2LAvvO8tin), [7](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGqLP_Q1VgUi5mp07zBgowNwY4Jc-LmtXBEAwG-NmrcwdEvaEjV4dMKVwWxOPCN4gyhmyl-Ccj5RiSuhOzyq-f-Ym9flHxUOiNu344Rxvqvk8wJ)\n",
      "*   LiteLLM PyPI: `https://pypi.org/project/litellm/`\n",
      "\n",
      "### Implementation Steps\n",
      "### - [ ] Step 1: Add LiteLLM Dependency and Remove Direct API Clients\n",
      "**Description**: Introduce `litellm` as a core dependency in `pyproject.toml`. Remove direct `google-genai` and `openai` client dependencies, as LiteLLM will abstract these. Remove `tenacity` as LiteLLM provides its own retry logic.\n",
      "**Files**:\n",
      "*   `pyproject.toml`\n",
      "*   `yellhorn_mcp/server.py`\n",
      "**Reference(s)**:\n",
      "*   `pyproject.toml`\n",
      "*   `yellhorn_mcp/server.py`\n",
      "*   LiteLLM PyPI: `https://pypi.org/project/litellm/`\n",
      "**Test(s)**:\n",
      "*   Run `poetry install` to verify dependency resolution.\n",
      "*   Ensure `yellhorn-mcp` CLI still starts without errors: `python -m yellhorn_mcp.cli --help`.\n",
      "\n",
      "### - [ ] Step 2: Refactor `LLMManager` Initialization and Core Call Logic\n",
      "**Description**: Modify `LLMManager.__init__` to accept a `litellm` client (or initialize it internally) instead of separate OpenAI and Gemini clients. Update `_single_call` to use `litellm.acompletion()` for all LLM interactions. Remove the `api_retry` decorator from `_call_openai` and `_call_gemini` (or remove these methods entirely if `_single_call` becomes the sole entry point).\n",
      "**Files**:\n",
      "*   `yellhorn_mcp/llm_manager.py`\n",
      "*   `yellhorn_mcp/server.py`\n",
      "**Reference(s)**:\n",
      "*   `yellhorn_mcp/llm_manager.py`\n",
      "*   `yellhorn_mcp/server.py`\n",
      "*   LiteLLM Documentation: `https://docs.litellm.ai/docs/completion`\n",
      "**Test(s)**:\n",
      "*   Unit tests for `LLMManager`'s `_single_call` method, ensuring it correctly routes to LiteLLM.\n",
      "*   Mock LiteLLM's `acompletion` to verify call parameters.\n",
      "\n",
      "### - [ ] Step 3: Adapt `UsageMetadata` and `TokenCounter` for LiteLLM\n",
      "**Description**: Update `UsageMetadata` to correctly parse LiteLLM's unified usage format. Investigate if `TokenCounter` can leverage LiteLLM's `token_counter` utility for more accurate or consistent token estimation across models.\n",
      "**Files**:\n",
      "*   `yellhorn_mcp/llm_manager.py`\n",
      "*   `yellhorn_mcp/token_counter.py`\n",
      "*   `yellhorn_mcp/models/metadata_models.py`\n",
      "**Reference(s)**:\n",
      "*   `yellhorn_mcp/llm_manager.py`\n",
      "*   `yellhorn_mcp/token_counter.py`\n",
      "*   `yellhorn_mcp/models/metadata_models.py`\n",
      "*   LiteLLM API: `https://docs.litellm.ai/docs/token_counter`[8](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGO7UZuLkO5CgKE-QOmpelvPzFXNHEcEyR-rll1KfKNLtvPxjmI1sxV0MFp3BiMU0CQxaoQb1h7uJBwtC2MOBdhw2fNZtrmaBRIrDy3pp3Xrb1448DCO1X_TBcT7TY=)\n",
      "**Test(s)**:\n",
      "*   Unit tests for `UsageMetadata` to ensure correct parsing of LiteLLM's usage data.\n",
      "*   Unit tests for `TokenCounter` to verify token counts using LiteLLM's method (if adopted) or ensuring compatibility.\n",
      "\n",
      "### - [ ] Step 4: Integrate Search Grounding and Deep Research Model Logic with LiteLLM\n",
      "**Description**: Adapt the logic for Google Search Grounding (`_get_gemini_search_tools`, `add_citations_from_metadata`) and Deep Research model tool activation (`_is_deep_research_model` in `LLMManager`) to work seamlessly with LiteLLM's unified API. LiteLLM supports passing `tools` parameters.\n",
      "**Files**:\n",
      "*   `yellhorn_mcp/llm_manager.py`\n",
      "*   `yellhorn_mcp/utils/search_grounding_utils.py`\n",
      "*   `yellhorn_mcp/integrations/gemini_integration.py` (will be refactored or removed)\n",
      "**Reference(s)**:\n",
      "*   `yellhorn_mcp/llm_manager.py`\n",
      "*   `yellhorn_mcp/utils/search_grounding_utils.py`\n",
      "*   LiteLLM API: `https://docs.litellm.ai/docs/completion` (for `tools` parameter)\n",
      "*   LiteLLM API: `https://docs.litellm.ai/docs/providers` (for model support)[5](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE7TAfZPbbVvO9V1PNxONobzzviteU9UhFpNZDcEBmmds4AFNHD3ve1SKZzeXa0X9_gmmsP5LkeBeFliaiF_qbJbV6oETAKOuLHrSY5jSvChIQC6H3FfQG08-jmKCzRQv0=)\n",
      "**Test(s)**:\n",
      "*   Integration tests for `call_llm_with_citations` to ensure search grounding works via LiteLLM.\n",
      "*   Unit tests for `LLMManager` to verify deep research model tool activation.\n",
      "\n",
      "### - [ ] Step 5: Update `server.py` and `cli.py` for LiteLLM Integration\n",
      "**Description**: Modify `app_lifespan` in `server.py` to initialize LiteLLM instead of separate OpenAI and Gemini clients. Adjust `cli.py` to handle API keys and model selection in a LiteLLM-compatible manner, potentially simplifying environment variable requirements. Remove `yellhorn_mcp/integrations/gemini_integration.py` as its functionality will be absorbed by `llm_manager.py` and LiteLLM.\n",
      "**Files**:\n",
      "*   `yellhorn_mcp/server.py`\n",
      "*   `yellhorn_mcp/cli.py`\n",
      "*   `yellhorn_mcp/integrations/gemini_integration.py` (delete)\n",
      "**Reference(s)**:\n",
      "*   `yellhorn_mcp/server.py`\n",
      "*   `yellhorn_mcp/cli.py`\n",
      "*   LiteLLM Getting Started: `https://docs.litellm.ai/docs/getting_started`[2](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNO9XSCuQVloTCtVZqw58jsi_tTJFmx2aAJdBRQlPbj69lF2ZDEeGXdC4DOfLIXQ9wnw8vmYjsUreGa5rXUZogzcTaVIPrrHdIc-MoZp93nZIdnsLsynw=)\n",
      "**Test(s)**:\n",
      "*   End-to-end integration tests for `create_workplan`, `judge_workplan`, and `curate_context` via the MCP server to ensure full functionality.\n",
      "*   Verify CLI commands still function correctly.\n",
      "\n",
      "### - [ ] Step 6: Update Cost Tracking and Documentation\n",
      "**Description**: Review and update `yellhorn_mcp/utils/cost_tracker_utils.py` to ensure accurate cost calculation with LiteLLM's model naming conventions and potentially leverage LiteLLM's cost tracking features if they offer more granularity. Update `LLMManagerREADME.md` and `docs/USAGE.md` to reflect the new LiteLLM-based architecture and usage.\n",
      "**Files**:\n",
      "*   `yellhorn_mcp/utils/cost_tracker_utils.py`\n",
      "*   `LLMManagerREADME.md`\n",
      "*   `docs/USAGE.md`\n",
      "**Reference(s)**:\n",
      "*   `yellhorn_mcp/utils/cost_tracker_utils.py`\n",
      "*   `LLMManagerREADME.md`\n",
      "*   `docs/USAGE.md`\n",
      "*   LiteLLM Model Cost Map: `https://docs.litellm.ai/docs/proxy/model_cost_map`[8](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGO7UZuLkO5CgKE-QOmpelvPzFXNHEcEyR-rll1KfKNLtvPxjmI1sxV0MFp3BiMU0CQxaoQb1h7uJBwtC2MOBdhw2fNZtrmaBRIrDy3pp3Xrb1448DCO1X_TBcT7TY=)\n",
      "**Test(s)**:\n",
      "*   Verify cost calculations in generated GitHub comments are correct.\n",
      "*   Review documentation for clarity and accuracy.\n",
      "\n",
      "### Global Test Strategy\n",
      "*   **Unit Tests**: All modified functions and classes will have comprehensive unit tests using `pytest` and `pytest-asyncio`. Mocking will be used for external API calls (LiteLLM).\n",
      "*   **Integration Tests**: The `examples/mock_context.py` will be updated and used to run integration tests for `create_workplan`, `judge_workplan`, and `curate_context` tools, simulating GitHub interactions and verifying LLM calls through the LiteLLM integration.\n",
      "*   **End-to-End Tests**: Manual verification of the `yellhorn-mcp` CLI and its interaction with GitHub issues.\n",
      "*   **Test Coverage**: Maintain minimum 70% test coverage for all new and modified code, enforced by `pytest-cov`.\n",
      "*   **Local Execution**: Tests can be run locally using `poetry run pytest`.\n",
      "*   **Environment Variables/Secrets**: API keys for LiteLLM (and underlying providers if needed) will be managed via environment variables (`LITELLM_API_KEY` or provider-specific keys like `OPENAI_API_KEY`, `GEMINI_API_KEY` as LiteLLM supports them).\n",
      "*   **Async Helpers/Fixtures**: Existing `pytest-asyncio` fixtures will be leveraged.\n",
      "\n",
      "### Files to Modify / New Files to Create\n",
      "*   **Files to Modify**:\n",
      "    *   `pyproject.toml`: Add `litellm` dependency, remove `google-genai`, `openai`, `tenacity`, `google-api-core`.\n",
      "    *   `yellhorn_mcp/llm_manager.py`: Refactor `LLMManager` to use LiteLLM, update `UsageMetadata`, remove `api_retry` decorator.\n",
      "    *   `yellhorn_mcp/server.py`: Update `app_lifespan` to initialize LiteLLM, remove direct client initializations.\n",
      "    *   `yellhorn_mcp/cli.py`: Adjust API key validation and model selection logic.\n",
      "    *   `yellhorn_mcp/token_counter.py`: Potentially integrate LiteLLM's token counting.\n",
      "    *   `yellhorn_mcp/utils/cost_tracker_utils.py`: Update `MODEL_PRICING` and `calculate_cost` for LiteLLM models.\n",
      "    *   `yellhorn_mcp/utils/search_grounding_utils.py`: Ensure compatibility with LiteLLM's `tools` parameter.\n",
      "    *   `LLMManagerREADME.md`: Update LLM Manager usage and architecture.\n",
      "    *   `docs/USAGE.md`: Update installation, configuration, and tool usage sections.\n",
      "*   **New Files to Create**:\n",
      "    *   None, aiming for refactoring existing files.\n",
      "[MOCK GitHub CLI] In directory: /Users/sravanj/project_work/yellhorn-mcp\n",
      "[MOCK GitHub CLI] Updated issue 2760\n",
      "[INFO] Successfully updated GitHub issue #2760 with generated workplan and metrics\n",
      "[MOCK GitHub CLI] Running: gh issue comment 2760 --body ## ✅ Workplan generated successfully\n",
      "\n",
      "### Generation Details\n",
      "**Time**: 46.1 seconds  \n",
      "**Completed**: 2025-08-10 23:09:33 UTC  \n",
      "**Model Used**: `gemini-2.5-flash`  \n",
      "\n",
      "### Token Usage\n",
      "**Input Tokens**: 113,934  \n",
      "**Output Tokens**: 4,109  \n",
      "**Total Tokens**: 124,466  \n",
      "**Estimated Cost**: $0.0445  \n",
      "\n",
      "**Search Results Used**: 1  \n",
      "**Context Size**: 398,011 characters  \n",
      "[MOCK GitHub CLI] In directory: /Users/sravanj/project_work/yellhorn-mcp\n",
      "[MOCK GitHub CLI] Added comment to issue 2760\n",
      "[INFO] Waiting for 0 background tasks...\n",
      "[INFO] All background tasks completed\n",
      "Workplan Created:\n",
      "--------------------------------------------------\n",
      "Issue URL: https://github.com/mock/repo/issues/2760\n",
      "Issue Number: 2760\n"
     ]
    }
   ],
   "source": [
    "# Call process_workplan_async with our mock context\n",
    "workplan_result = await (run_create_workplan(\n",
    "        title=title,\n",
    "        detailed_description=description,\n",
    "        repo_path=REPO_PATH,\n",
    "        gemini_client=gemini_client,\n",
    "        openai_client=openai_client,\n",
    "        llm_manager=llm_manager,\n",
    "        model=MODEL,\n",
    "        codebase_reasoning=\"full\",\n",
    "        log_callback=log_callback,\n",
    "        github_command_func=mock_github_command,\n",
    "        git_command_func=run_git_command_with_set_cwd(REPO_PATH),\n",
    "        background_task_timeout=180\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Workplan Created:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Issue URL: {workplan_result['issue_url']}\")\n",
    "print(f\"Issue Number: {workplan_result['issue_number']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using Review Workplan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.mock_context import run_revise_workplan, mock_github_command\n",
    "from yellhorn_mcp.server import process_revision_async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_workplan = \"\"\"## Refactor LLM Management with Unified Model Calling Service (LiteLLM)\n",
    "\n",
    "### Summary\n",
    "The current `yellhorn-mcp` codebase utilizes direct API calls to OpenAI and Google Gemini models, leading to fragmented LLM interaction logic and increased maintenance overhead for supporting new providers. This work plan proposes to replace and consolidate all LLM model calls through a unified model calling service, LiteLLM. This integration will simplify LLM API interactions, streamline the addition of new models and providers, and centralize advanced features like retry logic and cost tracking, thereby enhancing the system's flexibility, maintainability, and scalability. The primary subsystems affected will be `yellhorn_mcp/llm_manager.py`, `yellhorn_mcp/server.py`, and related utility and configuration files.\n",
    "\n",
    "### Technical Details\n",
    "*   **Languages & Frameworks**: Python 3.10+\n",
    "*   **External Dependencies**:\n",
    "    *   `litellm~=1.33.0`: This new dependency will be added to `pyproject.toml` under `[project] dependencies`. LiteLLM provides a unified interface to over 100 LLM APIs, including OpenAI and Gemini, simplifying API calls and offering built-in features like retry logic and cost tracking.[1](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG4Ty1kIv8-x706DKH3z0MyTlkEO8W57vOoDQOfso2nMsh9xh2BNH93Zg-7QbiF8-0CBxgxiyt3VuInjEF2v0egmfYTmiNcdHEDT6Id4ChPxXEX3wuBraaQMTjxKfKfsdgM_OdlbKP6n4hHZpfv8Z0Dq_1WI8VknpKOaGbluD5pvvgochg=), [2](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNO9XSCuQVloTCtVZqw58jsi_tTJFmx2aAJdBRQlPbj69lF2ZDEeGXdC4DOfLIXQ9wnw8vmYjsUreGa5rXUZogzcTaVIPrrHdIc-MoZp93nZIdnsLsynw=), [3](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHlyoBNK5-LKhYp37fYMMonXDdEhq0L2HOStpzDqK6Q8k29EGQBFuCvilzsLfcHyYt9xJdTW8jE2Wz8PAE91IOy5TT3WTDN4NOE9z84iS97Ix0-gVv17hI4sYRhfiK3Kh5Ak2KBhLJUVGxRzPqdJMp3sCjRj1nPP8ZAIOJasfsJrkzXU8Rh5ztGaAkie4Iz)\n",
    "*   **Dependency Management & Pinning Strategy**: `poetry` (as indicated by `pyproject.toml`). The new `litellm` dependency will be pinned using the `~=` operator for compatible releases.\n",
    "*   **Build, Lint, Formatting, Type-checking Commands**:\n",
    "    *   `python -m black yellhorn_mcp tests`\n",
    "    *   `python -m isort yellhorn_mcp tests`\n",
    "    *   `python -m pytest`\n",
    "    *   Type-checking will be performed via `pyright` (configured in `pyrightconfig.json`).\n",
    "*   **Logging & Observability**: Existing `logging` module will be leveraged. Ensure LiteLLM's internal logging integrates seamlessly or is configured to use the existing logging setup.\n",
    "*   **Analytics/KPIs**: Existing cost tracking and usage metadata (`yellhorn_mcp/utils/cost_tracker_utils.py`, `yellhorn_mcp/models/metadata_models.py`) will be adapted to consume LiteLLM's unified usage reports.\n",
    "*   **Testing Frameworks & Helpers**: `pytest`, `pytest-asyncio`, `httpx`, `pytest-cov` will continue to be used. Existing mock testing infrastructure (`examples/mock_context.py`) will be adapted to mock LiteLLM calls.\n",
    "\n",
    "### Architecture\n",
    "*   **Existing Components Leveraged**:\n",
    "    *   `yellhorn_mcp/llm_manager.py`: The core `LLMManager` class will be refactored to use LiteLLM as its underlying LLM interaction layer. Its existing logic for chunking (`ChunkingStrategy`), token counting (`TokenCounter`), and usage metadata aggregation (`UsageMetadata`) will be adapted to work with LiteLLM's outputs.\n",
    "    *   `yellhorn_mcp/token_counter.py`: Will continue to provide model-specific token limits and counting, potentially integrating with LiteLLM's internal token counting utilities if beneficial.\n",
    "    *   `yellhorn_mcp/models/metadata_models.py`: `UsageMetrics` and `CompletionMetadata` will continue to track LLM usage, adapted for LiteLLM's reporting format.\n",
    "    *   `yellhorn_mcp/utils/cost_tracker_utils.py`: `calculate_cost` and `format_metrics_section` will be updated to map LiteLLM's model identifiers to existing pricing.\n",
    "    *   `yellhorn_mcp/utils/search_grounding_utils.py`: The logic for configuring Google Search tools and adding citations will be adapted to pass through LiteLLM's API.\n",
    "    *   `yellhorn_mcp/processors/`: All processors (`context_processor.py`, `judgement_processor.py`, `workplan_processor.py`) will continue to use `LLMManager` as their LLM interface.\n",
    "*   **New Components Introduced**: No new top-level components will be introduced. The change is primarily an internal refactoring of the `LLMManager` to use LiteLLM.\n",
    "*   **Control-flow & Data-flow Diagram (ASCII)**:\n",
    "    ```\n",
    "    +-----------------+       +-------------------+       +-------------------+\n",
    "    | MCP Tools (CLI, |       | yellhorn_mcp/     |       | yellhorn_mcp/     |\n",
    "    | VSCode, Claude) |------>| server.py         |------>| llm_manager.py    |\n",
    "    +-----------------+       | (Tool Dispatcher) |       | (Unified LLM API) |\n",
    "                               +-------------------+       +-------------------+\n",
    "                                         ^                         |\n",
    "                                         |                         |\n",
    "                                         |                         |\n",
    "                                         |                         |\n",
    "                                         |                         v\n",
    "                                         |                   +-----------------+\n",
    "                                         |                   | LiteLLM Library |\n",
    "                                         |                   | (litellm.py)    |\n",
    "                                         |                   +-----------------+\n",
    "                                         |                         |\n",
    "                                         |                         |\n",
    "                                         |                         |\n",
    "                                         |                         v\n",
    "                                         |                   +-----------------+\n",
    "                                         |                   | OpenAI API      |\n",
    "                                         |                   | Gemini API      |\n",
    "                                         |                   | Other LLM APIs  |\n",
    "                                         |                   +-----------------+\n",
    "                                         |\n",
    "                                         +-------------------------------------+\n",
    "                                         (Usage Metadata, Responses, Errors)\n",
    "    ```\n",
    "*   **State-management, Retry/Fallback, and Error-handling Patterns**: LiteLLM offers robust built-in retry and fallback mechanisms.[2](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNO9XSCuQVloTCtVZqw58jsi_tTJFmx2aAJdBRQlPbj69lF2ZDEeGXdC4DOfLIXQ9wnw8vmYjsUreGa5rXUZogzcTaVIPrrHdIc-MoZp93nZIdnsLsynw=) The existing `api_retry` decorator in `llm_manager.py` will be removed, and `LLMManager` will rely on LiteLLM's internal retry logic. Error handling will be adapted to catch LiteLLM-specific exceptions and translate them into `YellhornMCPError` where appropriate.\n",
    "\n",
    "### Completion Criteria & Metrics\n",
    "*   **Engineering Metrics**:\n",
    "    *   All `pytest` unit and integration tests pass with 100% success rate.\n",
    "    *   Test coverage for `yellhorn_mcp` remains at or above 70% line coverage.\n",
    "    *   `black` and `isort` formatting checks pass.\n",
    "    *   `pyright` type-checking runs clean with no errors.\n",
    "    *   `LLMManager` successfully makes calls to both OpenAI and Gemini models via LiteLLM.\n",
    "    *   Cost tracking and usage metadata reported in GitHub comments remain accurate and consistent with previous versions.\n",
    "*   **Business Metrics**:\n",
    "    *   Reduced time-to-integrate new LLM providers by leveraging LiteLLM's unified interface.\n",
    "    *   Improved system reliability due to LiteLLM's robust retry and fallback mechanisms.\n",
    "*   **Code-state Definition of Done**:\n",
    "    *   All CI jobs (linting, formatting, testing) are green.\n",
    "    *   `pyproject.toml` is updated with the new `litellm` dependency.\n",
    "    *   `LLMManagerREADME.md` and `docs/USAGE.md` are updated to reflect the LiteLLM integration.\n",
    "\n",
    "### References\n",
    "*   `yellhorn_mcp/llm_manager.py`\n",
    "*   `yellhorn_mcp/server.py`\n",
    "*   `yellhorn_mcp/cli.py`\n",
    "*   `yellhorn_mcp/token_counter.py`\n",
    "*   `yellhorn_mcp/integrations/gemini_integration.py`\n",
    "*   `yellhorn_mcp/utils/cost_tracker_utils.py`\n",
    "*   `yellhorn_mcp/utils/search_grounding_utils.py`\n",
    "*   `pyproject.toml`\n",
    "*   LiteLLM GitHub Repository: `https://github.com/BerriAI/litellm`[4](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHJqrTQMqVpo30aziDctGWaQBTciLRLQ7cxR_-2L18AA-WDtAeW9Tc15Iulaz3EocAd6-xx6fScsj_J_o8JGGnbw4Umaj6-fP6WKin0aBn91jDU9DRzRDn6aHscPA==)\n",
    "*   LiteLLM Documentation: `https://docs.litellm.ai/`[5](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE7TAfZPbbVvO9V1PNxONobzzviteU9UhFpNZDcEBmmds4AFNHD3ve1SKZzeXa0X9_gmmsP5LkeBeFliaiF_qbJbV6oETAKOuLHrSY5jSvChIQC6H3FfQG08-jmKCzRQv0=), [2](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNO9XSCuQVloTCtVZqw58jsi_tTJFmx2aAJdBRQlPbj69lF2ZDEeGXdC4DOfLIXQ9wnw8vmYjsUreGa5rXUZogzcTaVIPrrHdIc-MoZp93nZIdnsLsynw=), [6](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG7LN0z9TdU0oWVk8IIjcNM2V5AwitnYaZ-qI9rf39ZkfkJY2RKVT_n-_7ZGXVmSnpQUZhy7xYafWgPWoH-uLFo1hTUD_FR4gh70OMHcD0r3Xnyj2xp1C1MS7iM3jgMnBsDCjkduQDfz5uk_xOGbD0ptegACqLFiQsWs1bcvY6MW1DJ6yc52PuDzj2LAvvO8tin), [7](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGqLP_Q1VgUi5mp07zBgowNwY4Jc-LmtXBEAwG-NmrcwdEvaEjV4dMKVwWxOPCN4gyhmyl-Ccj5RiSuhOzyq-f-Ym9flHxUOiNu344Rxvqvk8wJ)\n",
    "*   LiteLLM PyPI: `https://pypi.org/project/litellm/`\n",
    "\n",
    "### Implementation Steps\n",
    "### - [ ] Step 1: Add LiteLLM Dependency and Remove Direct API Clients\n",
    "**Description**: Introduce `litellm` as a core dependency in `pyproject.toml`. Remove direct `google-genai` and `openai` client dependencies, as LiteLLM will abstract these. Remove `tenacity` as LiteLLM provides its own retry logic.\n",
    "**Files**:\n",
    "*   `pyproject.toml`\n",
    "*   `yellhorn_mcp/server.py`\n",
    "**Reference(s)**:\n",
    "*   `pyproject.toml`\n",
    "*   `yellhorn_mcp/server.py`\n",
    "*   LiteLLM PyPI: `https://pypi.org/project/litellm/`\n",
    "**Test(s)**:\n",
    "*   Run `poetry install` to verify dependency resolution.\n",
    "*   Ensure `yellhorn-mcp` CLI still starts without errors: `python -m yellhorn_mcp.cli --help`.\n",
    "\n",
    "### - [ ] Step 2: Refactor `LLMManager` Initialization and Core Call Logic\n",
    "**Description**: Modify `LLMManager.__init__` to accept a `litellm` client (or initialize it internally) instead of separate OpenAI and Gemini clients. Update `_single_call` to use `litellm.acompletion()` for all LLM interactions. Remove the `api_retry` decorator from `_call_openai` and `_call_gemini` (or remove these methods entirely if `_single_call` becomes the sole entry point).\n",
    "**Files**:\n",
    "*   `yellhorn_mcp/llm_manager.py`\n",
    "*   `yellhorn_mcp/server.py`\n",
    "**Reference(s)**:\n",
    "*   `yellhorn_mcp/llm_manager.py`\n",
    "*   `yellhorn_mcp/server.py`\n",
    "*   LiteLLM Documentation: `https://docs.litellm.ai/docs/completion`\n",
    "**Test(s)**:\n",
    "*   Unit tests for `LLMManager`'s `_single_call` method, ensuring it correctly routes to LiteLLM.\n",
    "*   Mock LiteLLM's `acompletion` to verify call parameters.\n",
    "\n",
    "### - [ ] Step 3: Adapt `UsageMetadata` and `TokenCounter` for LiteLLM\n",
    "**Description**: Update `UsageMetadata` to correctly parse LiteLLM's unified usage format. Investigate if `TokenCounter` can leverage LiteLLM's `token_counter` utility for more accurate or consistent token estimation across models.\n",
    "**Files**:\n",
    "*   `yellhorn_mcp/llm_manager.py`\n",
    "*   `yellhorn_mcp/token_counter.py`\n",
    "*   `yellhorn_mcp/models/metadata_models.py`\n",
    "**Reference(s)**:\n",
    "*   `yellhorn_mcp/llm_manager.py`\n",
    "*   `yellhorn_mcp/token_counter.py`\n",
    "*   `yellhorn_mcp/models/metadata_models.py`\n",
    "*   LiteLLM API: `https://docs.litellm.ai/docs/token_counter`[8](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGO7UZuLkO5CgKE-QOmpelvPzFXNHEcEyR-rll1KfKNLtvPxjmI1sxV0MFp3BiMU0CQxaoQb1h7uJBwtC2MOBdhw2fNZtrmaBRIrDy3pp3Xrb1448DCO1X_TBcT7TY=)\n",
    "**Test(s)**:\n",
    "*   Unit tests for `UsageMetadata` to ensure correct parsing of LiteLLM's usage data.\n",
    "*   Unit tests for `TokenCounter` to verify token counts using LiteLLM's method (if adopted) or ensuring compatibility.\n",
    "\n",
    "### - [ ] Step 4: Integrate Search Grounding and Deep Research Model Logic with LiteLLM\n",
    "**Description**: Adapt the logic for Google Search Grounding (`_get_gemini_search_tools`, `add_citations_from_metadata`) and Deep Research model tool activation (`_is_deep_research_model` in `LLMManager`) to work seamlessly with LiteLLM's unified API. LiteLLM supports passing `tools` parameters.\n",
    "**Files**:\n",
    "*   `yellhorn_mcp/llm_manager.py`\n",
    "*   `yellhorn_mcp/utils/search_grounding_utils.py`\n",
    "*   `yellhorn_mcp/integrations/gemini_integration.py` (will be refactored or removed)\n",
    "**Reference(s)**:\n",
    "*   `yellhorn_mcp/llm_manager.py`\n",
    "*   `yellhorn_mcp/utils/search_grounding_utils.py`\n",
    "*   LiteLLM API: `https://docs.litellm.ai/docs/completion` (for `tools` parameter)\n",
    "*   LiteLLM API: `https://docs.litellm.ai/docs/providers` (for model support)[5](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE7TAfZPbbVvO9V1PNxONobzzviteU9UhFpNZDcEBmmds4AFNHD3ve1SKZzeXa0X9_gmmsP5LkeBeFliaiF_qbJbV6oETAKOuLHrSY5jSvChIQC6H3FfQG08-jmKCzRQv0=)\n",
    "**Test(s)**:\n",
    "*   Integration tests for `call_llm_with_citations` to ensure search grounding works via LiteLLM.\n",
    "*   Unit tests for `LLMManager` to verify deep research model tool activation.\n",
    "\n",
    "### - [ ] Step 5: Update `server.py` and `cli.py` for LiteLLM Integration\n",
    "**Description**: Modify `app_lifespan` in `server.py` to initialize LiteLLM instead of separate OpenAI and Gemini clients. Adjust `cli.py` to handle API keys and model selection in a LiteLLM-compatible manner, potentially simplifying environment variable requirements. Remove `yellhorn_mcp/integrations/gemini_integration.py` as its functionality will be absorbed by `llm_manager.py` and LiteLLM.\n",
    "**Files**:\n",
    "*   `yellhorn_mcp/server.py`\n",
    "*   `yellhorn_mcp/cli.py`\n",
    "*   `yellhorn_mcp/integrations/gemini_integration.py` (delete)\n",
    "**Reference(s)**:\n",
    "*   `yellhorn_mcp/server.py`\n",
    "*   `yellhorn_mcp/cli.py`\n",
    "*   LiteLLM Getting Started: `https://docs.litellm.ai/docs/getting_started`[2](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNO9XSCuQVloTCtVZqw58jsi_tTJFmx2aAJdBRQlPbj69lF2ZDEeGXdC4DOfLIXQ9wnw8vmYjsUreGa5rXUZogzcTaVIPrrHdIc-MoZp93nZIdnsLsynw=)\n",
    "**Test(s)**:\n",
    "*   End-to-end integration tests for `create_workplan`, `judge_workplan`, and `curate_context` via the MCP server to ensure full functionality.\n",
    "*   Verify CLI commands still function correctly.\n",
    "\n",
    "### - [ ] Step 6: Update Cost Tracking and Documentation\n",
    "**Description**: Review and update `yellhorn_mcp/utils/cost_tracker_utils.py` to ensure accurate cost calculation with LiteLLM's model naming conventions and potentially leverage LiteLLM's cost tracking features if they offer more granularity. Update `LLMManagerREADME.md` and `docs/USAGE.md` to reflect the new LiteLLM-based architecture and usage.\n",
    "**Files**:\n",
    "*   `yellhorn_mcp/utils/cost_tracker_utils.py`\n",
    "*   `LLMManagerREADME.md`\n",
    "*   `docs/USAGE.md`\n",
    "**Reference(s)**:\n",
    "*   `yellhorn_mcp/utils/cost_tracker_utils.py`\n",
    "*   `LLMManagerREADME.md`\n",
    "*   `docs/USAGE.md`\n",
    "*   LiteLLM Model Cost Map: `https://docs.litellm.ai/docs/proxy/model_cost_map`[8](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGO7UZuLkO5CgKE-QOmpelvPzFXNHEcEyR-rll1KfKNLtvPxjmI1sxV0MFp3BiMU0CQxaoQb1h7uJBwtC2MOBdhw2fNZtrmaBRIrDy3pp3Xrb1448DCO1X_TBcT7TY=)\n",
    "**Test(s)**:\n",
    "*   Verify cost calculations in generated GitHub comments are correct.\n",
    "*   Review documentation for clarity and accuracy.\n",
    "\n",
    "### Global Test Strategy\n",
    "*   **Unit Tests**: All modified functions and classes will have comprehensive unit tests using `pytest` and `pytest-asyncio`. Mocking will be used for external API calls (LiteLLM).\n",
    "*   **Integration Tests**: The `examples/mock_context.py` will be updated and used to run integration tests for `create_workplan`, `judge_workplan`, and `curate_context` tools, simulating GitHub interactions and verifying LLM calls through the LiteLLM integration.\n",
    "*   **End-to-End Tests**: Manual verification of the `yellhorn-mcp` CLI and its interaction with GitHub issues.\n",
    "*   **Test Coverage**: Maintain minimum 70% test coverage for all new and modified code, enforced by `pytest-cov`.\n",
    "*   **Local Execution**: Tests can be run locally using `poetry run pytest`.\n",
    "*   **Environment Variables/Secrets**: API keys for LiteLLM (and underlying providers if needed) will be managed via environment variables (`LITELLM_API_KEY` or provider-specific keys like `OPENAI_API_KEY`, `GEMINI_API_KEY` as LiteLLM supports them).\n",
    "*   **Async Helpers/Fixtures**: Existing `pytest-asyncio` fixtures will be leveraged.\n",
    "\n",
    "### Files to Modify / New Files to Create\n",
    "*   **Files to Modify**:\n",
    "    *   `pyproject.toml`: Add `litellm` dependency, remove `google-genai`, `openai`, `tenacity`, `google-api-core`.\n",
    "    *   `yellhorn_mcp/llm_manager.py`: Refactor `LLMManager` to use LiteLLM, update `UsageMetadata`, remove `api_retry` decorator.\n",
    "    *   `yellhorn_mcp/server.py`: Update `app_lifespan` to initialize LiteLLM, remove direct client initializations.\n",
    "    *   `yellhorn_mcp/cli.py`: Adjust API key validation and model selection logic.\n",
    "    *   `yellhorn_mcp/token_counter.py`: Potentially integrate LiteLLM's token counting.\n",
    "    *   `yellhorn_mcp/utils/cost_tracker_utils.py`: Update `MODEL_PRICING` and `calculate_cost` for LiteLLM models.\n",
    "    *   `yellhorn_mcp/utils/search_grounding_utils.py`: Ensure compatibility with LiteLLM's `tools` parameter.\n",
    "    *   `LLMManagerREADME.md`: Update LLM Manager usage and architecture.\n",
    "    *   `docs/USAGE.md`: Update installation, configuration, and tool usage sections.\n",
    "*   **New Files to Create**:\n",
    "    *   None, aiming for refactoring existing files.\n",
    "[MOCK GitHub CLI] In directory: /Users/sravanj/project_work/yellhorn-mcp\n",
    "[MOCK GitHub CLI] Updated issue 2760\n",
    "[INFO] Successfully updated GitHub issue #2760 with generated workplan and metrics\n",
    "[MOCK GitHub CLI] Running: gh issue comment 2760 --body ## ✅ Workplan generated successfully\n",
    "\n",
    "### Generation Details\n",
    "**Time**: 46.1 seconds  \n",
    "**Completed**: 2025-08-10 23:09:33 UTC  \n",
    "**Model Used**: `gemini-2.5-flash`  \n",
    "\n",
    "### Token Usage\n",
    "**Input Tokens**: 113,934  \n",
    "**Output Tokens**: 4,109  \n",
    "**Total Tokens**: 124,466  \n",
    "**Estimated Cost**: $0.0445  \n",
    "\n",
    "**Search Results Used**: 1  \n",
    "**Context Size**: 398,011 characters \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 16:19:05,107 INFO AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Getting codebase snapshot in mode: full\n",
      "[INFO] Found .gitignore with 86 patterns\n",
      "[MOCK GitHub CLI] Running: gh issue comment 123 --body <details>\n",
      "<summary>Debug: Full prompt used for generation</summary>\n",
      "\n",
      "```\n",
      "You are an expert software developer tasked with revising an existing workplan based on revision instructions.\n",
      "\n",
      "# Original Workplan\n",
      "Original workplan content here...\n",
      "\n",
      "# Revision Instructions\n",
      "Focus on building test notebooks for each of the major changes. Add more detailed testing steps and include error handling requirements\n",
      "\n",
      "# Codebase Context\n",
      "<codebase_tree>\n",
      ".\n",
      "├── CLAUDE.md\n",
      "├── LLMManagerREADME.md\n",
      "├── pyproject.toml\n",
      "├── docs/\n",
      "│   └── USAGE.md\n",
      "├── notebooks/\n",
      "│   ├── llm_manager.ipynb\n",
      "│   └── test_curate_context_v2.ipynb\n",
      "├── yellhorn_mcp/\n",
      "│   ├── cli.py\n",
      "│   ├── llm_manager.py\n",
      "│   ├── server.py\n",
      "│   └── token_counter.py\n",
      "│   ├── formatters/\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── codebase_snapshot.py\n",
      "│   │   ├── context_fetcher.py\n",
      "│   │   └── prompt_formatter.py\n",
      "│   ├── integrations/\n",
      "│   │   └── gemini_integration.py\n",
      "│   ├── models/\n",
      "│   │   └── metadata_models.py\n",
      "│   ├── processors/\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── context_processor.py\n",
      "│   │   ├── judgement_processor.py\n",
      "│   │   └── workplan_processor.py\n",
      "│   ├── utils/\n",
      "│   │   ├── cost_tracker_utils.py\n",
      "│   │   ├── lsp_utils.py\n",
      "│   │   └── search_grounding_utils.py\n",
      "</codebase_tree>\n",
      "\n",
      "<file_contents>\n",
      "\n",
      "--- File: CLAUDE.md ---\n",
      "# CLAUDE.md - Guidelines for AI Assistants\n",
      "\n",
      "## Testing\n",
      "\n",
      "- **Unit Tests**: All code must be covered by unit tests. Use `pytest` for writing and running tests.\n",
      "- **Test Coverage**: Maintain minimum 70% test coverage for all new code\n",
      "- **Integration Tests**: Include integration tests for LLM Manager and API interactions\n",
      "- **Mock Testing**: Use proper mocking for external API calls (OpenAI, Gemini, GitHub)\n",
      "\n",
      "## Code Style Guidelines\n",
      "\n",
      "- **Python Version**: 3.10+ (use modern typing with `|` operator)\n",
      "- **Formatting**: black with default settings\n",
      "- **Linting**: Use black for code formatting and isort for import ordering (no flake8)\n",
      "- **Imports**: Use isort to organize imports automatically with black-compatible settings\n",
      "- **Types**: Use modern type hints for all functions and class attributes, ie. prefer `list[str]` over `List[str]` and `sometype | None` over `Optional[sometype]`.\n",
      "- **Documentation**: Standard triple-quote docstrings with parameter descriptions for all public methods and classes. Use Google-style docstrings for clarity.\n",
      "\n",
      "## Architecture Guidelines\n",
      "\n",
      "- **LLM Manager**: All LLM interactions should go through the unified `LLMManager` class\n",
      "- **Error Handling**: Use proper exception handling with retry logic for external API calls\n",
      "- **Token Management**: Always check token limits before making API calls\n",
      "- **Cost Tracking**: Include usage metadata and cost tracking for all LLM calls\n",
      "- **Chunking**: Implement intelligent chunking for large prompts that exceed context limits\n",
      "\n",
      "## Formatting Commands\n",
      "\n",
      "Before committing code, always format with:\n",
      "\n",
      "```bash\n",
      "# Format code with black\n",
      "python -m black yellhorn_mcp tests\n",
      "\n",
      "# Sort imports with isort\n",
      "python -m isort yellhorn_mcp tests\n",
      "```\n",
      "\n",
      "Remember to run these commands automatically when making changes to ensure consistent code style.\n",
      "\n",
      "## LLM Manager Usage\n",
      "\n",
      "When working with LLM calls, always use the unified `LLMManager`:\n",
      "\n",
      "```python\n",
      "# Good - using LLM Manager\n",
      "llm_manager = LLMManager(openai_client=openai_client, gemini_client=gemini_client)\n",
      "response = await llm_manager.call_llm(prompt, model, temperature=0.7)\n",
      "\n",
      "# Good - with usage tracking\n",
      "result = await llm_manager.call_llm_with_usage(prompt, model)\n",
      "content = result[\"content\"]\n",
      "usage = result[\"usage_metadata\"]\n",
      "\n",
      "# Bad - direct client calls\n",
      "response = await openai_client.chat.completions.create(...)\n",
      "```\n",
      "\n",
      "## Retry Logic\n",
      "\n",
      "Use the built-in retry decorator for external API calls:\n",
      "\n",
      "```python\n",
      "from yellhorn_mcp.llm_manager import api_retry\n",
      "\n",
      "@api_retry\n",
      "async def my_api_call():\n",
      "    # Your API call here\n",
      "    pass\n",
      "```\n",
      "\n",
      "--- File: LLMManagerREADME.md ---\n",
      "# LLM Manager Implementation Summary\n",
      "\n",
      "This document summarizes the exact changes made to the yellhorn-mcp codebase to implement the LLM Manager functionality, which adds intelligent token counting, automatic chunking, usage metrics tracking, and retry mechanisms for both OpenAI and Gemini models.\n",
      "\n",
      "## Overview\n",
      "\n",
      "The LLM Manager provides a unified interface for making LLM calls with:\n",
      "- Automatic token counting and context window management\n",
      "- Intelligent chunking for prompts that exceed model limits\n",
      "- Comprehensive usage metrics tracking\n",
      "- Exponential backoff retry for rate limits\n",
      "- Support for both OpenAI and Gemini models including GPT-4.1 and deep research models\n",
      "\n",
      "## Files Created\n",
      "\n",
      "### 1. `yellhorn_mcp/llm_manager.py`\n",
      "A comprehensive LLM management system with the following key components:\n",
      "\n",
      "**Core Classes:**\n",
      "- `UsageMetadata`: Unified usage tracking that handles both OpenAI and Gemini formats\n",
      "- `ChunkingStrategy`: Intelligent text splitting with paragraph and sentence-based strategies\n",
      "- `LLMManager`: Main manager class for all LLM operations\n",
      "\n",
      "**Key Features:**\n",
      "- Automatic chunking when prompts exceed model context windows\n",
      "- Configurable safety margins and overlap ratios for chunks\n",
      "- Deep research model support (o3, o4) with web search and code interpreter tools\n",
      "- Citation support for Gemini models with search grounding\n",
      "- Retry decorator with exponential backoff for rate limit handling\n",
      "\n",
      "### 2. `yellhorn_mcp/token_counter.py`\n",
      "Token counting utility using tiktoken for accurate token estimation:\n",
      "\n",
      "**Features:**\n",
      "- Model-specific token limits (including GPT-4.1 with 1M tokens)\n",
      "- Proper encoding mappings (o200k_base for GPT-4o family, cl100k_base for others)\n",
      "- Token estimation for responses\n",
      "- Context window fitting checks\n",
      "- Encoding caching for performance\n",
      "\n",
      "### 3. `examples/mock_context.py`\n",
      "Mock context implementation for testing yellhorn MCP tools:\n",
      "\n",
      "**Components:**\n",
      "- `mock_github_command`: Simulates GitHub CLI responses\n",
      "- `BackgroundTaskManager`: Tracks and waits for async tasks\n",
      "- `MockContext`, `MockLifespanContext`, `MockRequestContext`: Test infrastructure\n",
      "- Helper functions for running MCP tools in isolation\n",
      "\n",
      "### 4. `tests/test_llm_manager.py`\n",
      "Comprehensive test suite with 100+ tests covering:\n",
      "- Initialization with various configurations\n",
      "- Model detection (OpenAI vs Gemini)\n",
      "- Simple and chunked LLM calls\n",
      "- JSON response handling\n",
      "- Deep research model tool activation\n",
      "- Citation and grounding metadata\n",
      "- Retry logic and error handling\n",
      "- Usage metadata tracking\n",
      "\n",
      "### 5. `tests/test_token_counter.py`\n",
      "Test suite for token counting functionality:\n",
      "- Model limit retrieval\n",
      "- Token counting across different models\n",
      "- Response token estimation\n",
      "- Context window fitting checks\n",
      "- Special character handling\n",
      "- Configuration overrides\n",
      "\n",
      "### 6. `yellhorn_mcp/metadata_models.py`\n",
      "Data models for tracking completion and submission metadata:\n",
      "- `CompletionMetadata`: Tracks generation metrics, timing, and status\n",
      "- `SubmissionMetadata`: Records workplan submission details\n",
      "- Used for GitHub comment formatting and status tracking\n",
      "\n",
      "### 7. `yellhorn_mcp/search_grounding.py`\n",
      "Search grounding functionality for Gemini models:\n",
      "- `_get_gemini_search_tools`: Configures Google Search tools for supported models\n",
      "- `add_citations`: Adds inline citations to generated content\n",
      "- `add_citations_from_metadata`: Processes grounding metadata into citations\n",
      "- Enables web search integration for enhanced responses\n",
      "\n",
      "## Changes to Existing Files\n",
      "\n",
      "### `yellhorn_mcp/server.py`\n",
      "**Import Addition:**\n",
      "```python\n",
      "from yellhorn_mcp.llm_manager import LLMManager, UsageMetadata\n",
      "```\n",
      "\n",
      "**In `app_lifespan` function:**\n",
      "- Added LLMManager initialization with configuration:\n",
      "```python\n",
      "llm_manager = LLMManager(\n",
      "    openai_client=openai_client,\n",
      "    gemini_client=gemini_client,\n",
      "    config={\n",
      "        \"safety_margin_tokens\": 2000,\n",
      "        \"overlap_ratio\": 0.1,\n",
      "        \"chunk_strategy\": \"paragraph\",\n",
      "        \"aggregation_strategy\": \"concatenate\"\n",
      "    }\n",
      ")\n",
      "```\n",
      "\n",
      "**In `MODEL_PRICING` dictionary:**\n",
      "- Added GPT-4.1 pricing:\n",
      "```python\n",
      "\"gpt-4.1\": {\n",
      "    \"input\": {\"default\": 10.0},\n",
      "    \"output\": {\"default\": 30.0},\n",
      "}\n",
      "```\n",
      "\n",
      "**In `format_metrics_section` function:**\n",
      "- Updated to use `UsageMetadata` for consistent interface:\n",
      "```python\n",
      "usage = UsageMetadata(usage_metadata)\n",
      "```\n",
      "\n",
      "**In `process_workplan_async` and `process_judgement_async` functions:**\n",
      "- Replaced direct API calls with LLMManager calls\n",
      "- Added support for both `call_llm_with_usage` and `call_llm_with_citations`\n",
      "- Improved error handling and usage metadata extraction\n",
      "\n",
      "### `pyproject.toml`\n",
      "**Updated Dependencies:**\n",
      "```toml\n",
      "# Core dependencies for LLM Manager functionality\n",
      "\"tiktoken~=0.8.0\",           # For accurate token counting across models\n",
      "\"tenacity~=9.1.2\",           # For retry with exponential backoff (updated)\n",
      "\n",
      "# Google API dependencies (updated versions)\n",
      "\"google-genai~=1.16.1\",      # Updated from 1.8.0 for latest Gemini features\n",
      "\"google-api-core~=2.25.1\",   # Updated from 2.24.2 for better stability\n",
      "```\n",
      "\n",
      "These dependency updates ensure:\n",
      "- Compatibility with the latest Gemini API features\n",
      "- Improved retry logic with tenacity 9.x\n",
      "- Better error handling with updated google-api-core\n",
      "- Consistent API interfaces across all dependencies\n",
      "\n",
      "## Key Implementation Details\n",
      "\n",
      "### Token Counting and Chunking\n",
      "The system automatically detects when a prompt exceeds the model's context window and splits it into manageable chunks. It uses:\n",
      "- Binary search to find optimal split points\n",
      "- Natural boundaries (paragraphs, sentences, words) for splits\n",
      "- Configurable overlap between chunks for context preservation\n",
      "\n",
      "### Retry Mechanism\n",
      "Implements intelligent retry logic:\n",
      "```python\n",
      "@retry(\n",
      "    retry=retry_if_exception(is_retryable_error),\n",
      "    wait=wait_exponential(multiplier=1, min=4, max=60, exp_base=2),\n",
      "    stop=stop_after_attempt(5),\n",
      "    before_sleep=log_retry_attempt\n",
      ")\n",
      "```\n",
      "\n",
      "### Usage Metrics Tracking\n",
      "Provides unified tracking across both OpenAI and Gemini:\n",
      "- Automatic aggregation for chunked calls\n",
      "- Cost calculation based on model pricing\n",
      "- Support for both token count formats\n",
      "\n",
      "### Deep Research Models\n",
      "Special handling for o3 and o4 models:\n",
      "- Automatic enabling of web search and code interpreter tools\n",
      "- Temperature forced to 1.0 for reasoning models\n",
      "- Proper tool configuration in API calls\n",
      "\n",
      "## Testing\n",
      "\n",
      "The implementation includes comprehensive test coverage:\n",
      "- **Unit Tests**: Cover all major components and edge cases\n",
      "- **Integration Tests**: Via mock_context.py for testing in notebook environments\n",
      "- **Error Handling**: Tests for rate limits, API errors, and edge cases\n",
      "\n",
      "## Benefits\n",
      "\n",
      "1. **Reliability**: Automatic retry handling reduces failures from rate limits\n",
      "2. **Scalability**: Chunking enables processing of arbitrarily large prompts\n",
      "3. **Cost Tracking**: Built-in usage metrics and cost calculation\n",
      "4. **Flexibility**: Configurable strategies for different use cases\n",
      "5. **Compatibility**: Unified interface works with both OpenAI and Gemini models\n",
      "\n",
      "This implementation significantly enhances the robustness and capabilities of the yellhorn-mcp server while maintaining backward compatibility.\n",
      "\n",
      "## Integration Details\n",
      "\n",
      "### How the LLM Manager Integrates with the Server\n",
      "\n",
      "The LLM Manager is deeply integrated into the yellhorn-mcp server's core workflow, replacing direct API calls with a unified, robust interface. Here's how each component connects:\n",
      "\n",
      "#### 1. **Initialization Flow in `app_lifespan`**\n",
      "```python\n",
      "# The LLM Manager is initialized during server startup\n",
      "llm_manager = LLMManager(\n",
      "    openai_client=openai_client,\n",
      "    gemini_client=gemini_client,\n",
      "    config={...}\n",
      ")\n",
      "```\n",
      "- Created once per server instance in the lifespan context\n",
      "- Shared across all MCP tool invocations\n",
      "- Configuration is centralized for consistent behavior\n",
      "\n",
      "#### 2. **Usage in Async Processing Functions**\n",
      "\n",
      "**`process_workplan_async`** - The core workplan generation function:\n",
      "- Receives `llm_manager` as a parameter from the lifespan context\n",
      "- Builds prompts that can exceed model limits (includes entire codebase)\n",
      "- Uses `call_llm_with_usage` for OpenAI models (no citation support)\n",
      "- Uses `call_llm_with_citations` for Gemini models (with search grounding)\n",
      "- Automatically handles chunking when codebase + prompt exceeds context window\n",
      "- Extracts and formats usage metadata for cost tracking\n",
      "\n",
      "**`process_judgement_async`** - The diff judgement function:\n",
      "- Similar integration pattern as workplan generation\n",
      "- Can handle very large diffs that exceed model limits\n",
      "- Maintains consistent error handling and retry logic\n",
      "\n",
      "**`curate_context`** - The context curation tool:\n",
      "- Uses LLM Manager for analyzing directory structures\n",
      "- Benefits from automatic chunking for large codebases\n",
      "- Consistent usage tracking across all operations\n",
      "\n",
      "#### 3. **Token Counting Integration**\n",
      "\n",
      "The `TokenCounter` is used internally by `LLMManager` but also influences how the server operates:\n",
      "- **Context Window Awareness**: The server can now make intelligent decisions about what to include in prompts\n",
      "- **Safety Margins**: The 2000-token safety margin ensures room for system prompts and responses\n",
      "- **Model Detection**: Proper encoding selection (o200k_base vs cl100k_base) ensures accurate counting\n",
      "\n",
      "#### 4. **Usage Metadata Flow**\n",
      "\n",
      "The `UsageMetadata` class provides a unified interface that's used throughout:\n",
      "```python\n",
      "# In format_metrics_section\n",
      "usage = UsageMetadata(usage_metadata)  # Handles any format\n",
      "input_tokens = usage.prompt_tokens\n",
      "output_tokens = usage.completion_tokens\n",
      "```\n",
      "- Seamlessly handles OpenAI's format (`prompt_tokens`, `completion_tokens`)\n",
      "- Seamlessly handles Gemini's format (`prompt_token_count`, `candidates_token_count`)\n",
      "- Used by `calculate_cost()` for accurate pricing\n",
      "- Aggregated across chunks for total usage\n",
      "\n",
      "#### 5. **Error Handling and Retry Logic**\n",
      "\n",
      "The retry decorator with exponential backoff is crucial for production reliability:\n",
      "```python\n",
      "@api_retry  # Applied to _call_openai and _call_gemini\n",
      "async def _call_openai(...):\n",
      "    # Automatic retry on rate limits\n",
      "```\n",
      "- Detects various rate limit error formats\n",
      "- Exponential backoff: 4s, 8s, 16s, 32s, 60s (max)\n",
      "- Logs retry attempts for debugging\n",
      "- Handles both OpenAI and Google-specific exceptions\n",
      "\n",
      "#### 6. **Mock Context for Testing**\n",
      "\n",
      "The `mock_context.py` enables standalone testing and notebook usage:\n",
      "- **BackgroundTaskManager**: Tracks async tasks created by `asyncio.create_task`\n",
      "- **Mock GitHub Commands**: Simulates GitHub CLI without actual API calls\n",
      "- **Direct Function Access**: Allows calling internal functions like `process_workplan_async` directly\n",
      "- **Integration Testing**: Validates the entire flow including chunking and retries\n",
      "\n",
      "#### 7. **Citation and Search Grounding Support**\n",
      "\n",
      "For Gemini models with search grounding enabled:\n",
      "```python\n",
      "# In process_workplan_async\n",
      "if \"grounding_metadata\" in response_data:\n",
      "    workplan_content = add_citations_from_metadata(\n",
      "        workplan_content, \n",
      "        response_data[\"grounding_metadata\"]\n",
      "    )\n",
      "```\n",
      "- LLM Manager preserves grounding metadata from Gemini responses\n",
      "- Server processes citations and adds them to the output\n",
      "- Maintains full search result attribution\n",
      "\n",
      "#### 8. **Deep Research Model Support**\n",
      "\n",
      "The LLM Manager automatically configures tools for deep research models:\n",
      "```python\n",
      "if self._is_deep_research_model(model):\n",
      "    params[\"tools\"] = [\n",
      "        {\"type\": \"web_search_preview\"},\n",
      "        {\"type\": \"code_interpreter\", ...}\n",
      "    ]\n",
      "```\n",
      "- Server doesn't need model-specific logic\n",
      "- Tools are automatically enabled for o3, o4 models\n",
      "- Temperature is forced to 1.0 for reasoning models\n",
      "\n",
      "#### 9. **Chunking Strategy Integration**\n",
      "\n",
      "The paragraph-based chunking strategy aligns with the server's prompt structure:\n",
      "- Preserves markdown formatting in workplans\n",
      "- Maintains code block integrity\n",
      "- Overlapping chunks preserve context across boundaries\n",
      "- Aggregation strategy concatenates with clear separators\n",
      "\n",
      "#### 10. **Metrics and Cost Tracking**\n",
      "\n",
      "The integration provides comprehensive metrics:\n",
      "```python\n",
      "# Automatic cost calculation\n",
      "estimated_cost = calculate_cost(model, input_tokens, output_tokens)\n",
      "\n",
      "# Completion metadata tracking\n",
      "completion_metadata = CompletionMetadata(\n",
      "    input_tokens=input_tokens,\n",
      "    output_tokens=output_tokens,\n",
      "    estimated_cost=estimated_cost,\n",
      "    ...\n",
      ")\n",
      "```\n",
      "- Every LLM call is tracked and reported\n",
      "- Costs are calculated based on current model pricing\n",
      "- Metrics are posted as GitHub comments for transparency\n",
      "\n",
      "### Data Flow Example\n",
      "\n",
      "Here's how a typical workplan creation flows through the system:\n",
      "\n",
      "1. **User invokes `create_workplan` MCP tool**\n",
      "2. **Server creates placeholder GitHub issue**\n",
      "3. **Launches `process_workplan_async` as background task**\n",
      "4. **Builds prompt with codebase context** (can be 100K+ tokens)\n",
      "5. **LLM Manager detects prompt exceeds model limit**\n",
      "6. **Automatically chunks into smaller pieces**\n",
      "7. **Makes multiple API calls with retry on rate limits**\n",
      "8. **Aggregates responses maintaining context**\n",
      "9. **Returns combined result with usage metadata**\n",
      "10. **Server updates GitHub issue with workplan**\n",
      "11. **Posts completion comment with metrics**\n",
      "\n",
      "This seamless integration ensures reliability, scalability, and cost transparency while handling the complexities of large-scale LLM operations.\n",
      "\n",
      "--- File: docs/USAGE.md ---\n",
      "# Yellhorn MCP - Usage Guide\n",
      "\n",
      "## Overview\n",
      "\n",
      "Yellhorn MCP is a Model Context Protocol (MCP) server that allows Claude Code to interact with the Gemini 2.5 Pro and OpenAI API for software development tasks. Version 0.7.0 introduces major improvements including unified LLM management, automatic chunking, and robust retry logic.\n",
      "\n",
      "### What's New in v0.7.0\n",
      "\n",
      "- **🔄 Unified LLM Manager**: Centralized management for both OpenAI and Gemini models with automatic retry logic\n",
      "- **🧩 Smart Chunking**: Automatic prompt chunking when content exceeds model context limits\n",
      "- **📊 Enhanced Token Counting**: Support for latest models with accurate token estimation\n",
      "- **💰 Cost Tracking**: Real-time cost estimation and usage tracking\n",
      "- **⚡ Performance Optimizations**: Better handling of large codebases and rate limits\n",
      "\n",
      "## Main Tools\n",
      "\n",
      "1. **Create workplan**: Creates a GitHub issue with a detailed implementation plan based on your codebase and task description.\n",
      "2. **Get workplan**: Retrieves the workplan content from a GitHub issue.\n",
      "3. **Judge workplan**: Triggers an asynchronous code judgement for a Pull Request against its original workplan issue.\n",
      "4. **Curate context**: Analyzes your codebase structure to build an optimized .yellhorncontext file with directory filtering rules.\n",
      "5. **Revise workplan**: Updates an existing workplan based on revision instructions.\n",
      "\n",
      "## Installation\n",
      "\n",
      "```bash\n",
      "# Install from PyPI\n",
      "pip install yellhorn-mcp\n",
      "\n",
      "# Install from source\n",
      "git clone https://github.com/msnidal/yellhorn-mcp.git\n",
      "cd yellhorn-mcp\n",
      "pip install -e .\n",
      "```\n",
      "\n",
      "## Configuration\n",
      "\n",
      "The server requires the following environment variables:\n",
      "\n",
      "- `GEMINI_API_KEY` (required for Gemini models): Your Gemini API key\n",
      "- `OPENAI_API_KEY` (required for OpenAI models): Your OpenAI API key\n",
      "- `REPO_PATH` (optional): Path to your Git repository (defaults to current directory)\n",
      "- `YELLHORN_MCP_MODEL` (optional): Model to use (defaults to \"gemini-2.5-pro\"). Available options:\n",
      "  - **Gemini models**: \"gemini-2.5-pro\", \"gemini-2.5-flash\", \"gemini-2.5-flash-lite\"\n",
      "  - **OpenAI models**: \"gpt-4o\", \"gpt-4o-mini\", \"o4-mini\", \"o3\", \"gpt-4.1\"\n",
      "  - **Deep Research models**: \"o3-deep-research\", \"o4-mini-deep-research\"\n",
      "  - Note: Deep Research models automatically enable `web_search_preview` and `code_interpreter` tools for enhanced research capabilities\n",
      "- `YELLHORN_MCP_SEARCH` (optional): Enable/disable Google Search Grounding (defaults to \"on\" for Gemini models). Options:\n",
      "  - \"on\" - Search grounding enabled for Gemini models\n",
      "  - \"off\" - Search grounding disabled for all models\n",
      "\n",
      "### File Filtering with .yellhorncontext and .yellhornignore\n",
      "\n",
      "You can control which files are included in the AI context using either a `.yellhorncontext` or `.yellhornignore` file in your repository root. The `.yellhorncontext` file takes precedence if both exist:\n",
      "\n",
      "#### .yellhorncontext File (Recommended)\n",
      "\n",
      "The `.yellhorncontext` file is the recommended approach as it provides more comprehensive control, with both directory-specific patterns and inherited patterns from `.yellhornignore`:\n",
      "\n",
      "```\n",
      "# Yellhorn Context File - AI context optimization\n",
      "# Generated by yellhorn-mcp curate_context tool\n",
      "# Based on task: Implementing a user authentication system\n",
      "\n",
      "# Patterns from .yellhornignore file\n",
      "# Files and directories to exclude (blacklist)\n",
      "node_modules/\n",
      "dist/\n",
      "*.log\n",
      "\n",
      "# Explicitly included patterns (whitelist)\n",
      "!important.log\n",
      "\n",
      "# Task-specific directories for AI context\n",
      "# Important directories to specifically include\n",
      "!./\n",
      "!src/\n",
      "!src/auth/\n",
      "!src/models/\n",
      "!tests/\n",
      "!tests/auth/\n",
      "\n",
      "# Recommended: blacklist everything else (uncomment to enable)\n",
      "# **/*\n",
      "```\n",
      "\n",
      "#### .yellhornignore File (Legacy)\n",
      "\n",
      "You can also use a `.yellhornignore` file which works similar to `.gitignore` but is specific to the Yellhorn MCP server:\n",
      "\n",
      "```\n",
      "# Example .yellhornignore file\n",
      "# Blacklist patterns\n",
      "*.log\n",
      "node_modules/\n",
      "dist/\n",
      "*.min.js\n",
      "credentials/\n",
      "\n",
      "# Whitelist patterns (exceptions to blacklist)\n",
      "!important.log\n",
      "!node_modules/important-package.json\n",
      "```\n",
      "\n",
      "#### Pattern Syntax and Processing\n",
      "\n",
      "Both files use the same pattern syntax as `.gitignore`:\n",
      "\n",
      "- Lines starting with `#` are comments\n",
      "- Empty lines are ignored\n",
      "- Patterns use shell-style wildcards (e.g., `*.js`, `node_modules/`)\n",
      "- Patterns ending with `/` will match directories\n",
      "- Patterns containing `/` are relative to the repository root\n",
      "- Patterns starting with `!` are whitelist patterns (exceptions to ignores)\n",
      "\n",
      "**Blacklist and Whitelist Processing:**\n",
      "\n",
      "- Standard patterns (without `!`) will ignore matching files\n",
      "- Patterns starting with `!` will whitelist matching files, even if they match an ignore pattern\n",
      "- Whitelist patterns are checked first, so they take precedence over blacklist patterns\n",
      "- This allows you to have broad blacklist patterns with specific exceptions\n",
      "\n",
      "**Priority Order:**\n",
      "\n",
      "1. `.yellhorncontext` (if it exists) is used first\n",
      "2. `.yellhornignore` (if `.yellhorncontext` doesn't exist) is used as fallback\n",
      "3. `.gitignore` (already respected by Git commands used to list files)\n",
      "\n",
      "This feature is useful for:\n",
      "\n",
      "- Focusing AI on the most relevant directories for your specific task\n",
      "- Excluding large folders that wouldn't provide useful context (e.g., `node_modules/`)\n",
      "- Excluding sensitive or credential-related files\n",
      "- Reducing noise in the AI's context to improve focus on relevant code\n",
      "- Allowing specific important files through blacklist patterns (e.g., `!config/important.json`)\n",
      "\n",
      "The server requires GitHub CLI (`gh`) to be installed and authenticated:\n",
      "\n",
      "```bash\n",
      "# Install GitHub CLI (if not already installed)\n",
      "# For macOS:\n",
      "brew install gh\n",
      "\n",
      "# For Ubuntu/Debian:\n",
      "curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg\n",
      "echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main\" | sudo tee /etc/apt/sources.list.d/github-cli.list > /dev/null\n",
      "sudo apt update\n",
      "sudo apt install gh\n",
      "\n",
      "# Authenticate with GitHub\n",
      "gh auth login\n",
      "```\n",
      "\n",
      "```bash\n",
      "# Set environment variables for Gemini models\n",
      "export GEMINI_API_KEY=your_gemini_api_key_here\n",
      "export REPO_PATH=/path/to/your/repo\n",
      "export YELLHORN_MCP_MODEL=gemini-2.5-pro\n",
      "\n",
      "# OR for OpenAI models\n",
      "export OPENAI_API_KEY=your_openai_api_key_here\n",
      "export REPO_PATH=/path/to/your/repo\n",
      "export YELLHORN_MCP_MODEL=gpt-4o\n",
      "```\n",
      "\n",
      "### VSCode/Cursor Setup\n",
      "\n",
      "To configure Yellhorn MCP in VSCode or Cursor, create a `.vscode/mcp.json` file in your workspace root with the following content:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"inputs\": [\n",
      "    {\n",
      "      \"type\": \"promptString\",\n",
      "      \"id\": \"gemini-api-key\",\n",
      "      \"description\": \"Gemini API Key\"\n",
      "    }\n",
      "  ],\n",
      "  \"servers\": {\n",
      "    \"yellhorn-mcp\": {\n",
      "      \"type\": \"stdio\",\n",
      "      \"command\": \"/Users/msnidal/.pyenv/shims/yellhorn-mcp\",\n",
      "      \"args\": [],\n",
      "      \"env\": {\n",
      "        \"GEMINI_API_KEY\": \"${input:gemini-api-key}\",\n",
      "        \"REPO_PATH\": \"${workspaceFolder}\",\n",
      "        \"YELLHORN_MCP_SEARCH\": \"on\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "### Claude Code Setup\n",
      "\n",
      "To configure Yellhorn MCP with Claude Code directly, add a root-level `.mcp.json` file in your project with the following content:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"mcpServers\": {\n",
      "    \"yellhorn-mcp\": {\n",
      "      \"type\": \"stdio\",\n",
      "      \"command\": \"yellhorn-mcp\",\n",
      "      \"args\": [\"--model\",\"o3\"],\n",
      "      \"env\": {\n",
      "        \"YELLHORN_MCP_SEARCH\": \"on\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "## Getting Started\n",
      "\n",
      "Once the server is running, Claude Code can utilize the tools it exposes. Here's a typical workflow:\n",
      "\n",
      "### 1. Creating a workplan\n",
      "\n",
      "```\n",
      "Please generate a workplan for implementing a user authentication system in my application.\n",
      "```\n",
      "\n",
      "This will use the `create_workplan` tool to analyze your codebase, create a GitHub issue, and populate it with a detailed implementation plan. The tool will return the issue URL and number. The issue will initially show a placeholder message and will be updated asynchronously once the plan is generated.\n",
      "\n",
      "### 2. View the workplan\n",
      "\n",
      "To view a workplan, use the following command:\n",
      "\n",
      "```\n",
      "Please retrieve the workplan for issue #123.\n",
      "```\n",
      "\n",
      "This will use the `get_workplan` tool to fetch the latest content of the GitHub issue.\n",
      "\n",
      "### 3. Make Changes and Create a PR\n",
      "\n",
      "After making changes to implement the workplan, create a PR using your preferred method:\n",
      "\n",
      "```bash\n",
      "# Manual Git flow\n",
      "git add .\n",
      "git commit -m \"Implement user authentication\"\n",
      "git push origin HEAD\n",
      "\n",
      "# GitHub CLI\n",
      "gh pr create --title \"Implement User Authentication\" --body \"This PR adds JWT authentication with bcrypt password hashing.\"\n",
      "```\n",
      "\n",
      "### 4. Request a Judgement\n",
      "\n",
      "Once your PR is created, you can request a judgement against the original workplan:\n",
      "\n",
      "```\n",
      "Please judge the PR comparing \"main\" and \"feature-branch\" against the workplan in issue #456.\n",
      "```\n",
      "\n",
      "This will use the `judge_workplan` tool to:\n",
      "\n",
      "1. **Immediately create** a placeholder sub-issue with the judgement task details\n",
      "2. **Return the sub-issue URL** so you can track progress\n",
      "3. **Process the judgement asynchronously** by fetching the original workplan, generating a diff between the specified git references, and running AI analysis\n",
      "4. **Update the placeholder sub-issue** with the complete judgement results, including detailed analysis, citations, and completion metrics\n",
      "\n",
      "The placeholder sub-issue is labeled with `yellhorn-judgement-subissue` and linked to the original workplan issue for easy reference.\n",
      "\n",
      "## MCP Tools\n",
      "\n",
      "### create_workplan\n",
      "\n",
      "Creates a GitHub issue with a detailed workplan based on the title and detailed description. The issue is labeled with 'yellhorn-mcp' and the plan is generated asynchronously (with `codebase_reasoning=\"full\"`), with the issue being updated once it's ready. For faster creation without AI enhancement, use `codebase_reasoning=\"none\"`.\n",
      "\n",
      "**Input**:\n",
      "\n",
      "- `title`: Title for the GitHub issue (will be used as issue title and header)\n",
      "- `detailed_description`: Detailed description for the workplan. Any URLs provided here will be extracted and included in a References section in the workplan.\n",
      "- `codebase_reasoning`: (optional) Control whether AI enhancement is performed:\n",
      "  - `\"full\"`: (default) Use AI to enhance the workplan with full codebase context\n",
      "  - `\"lsp\"`: Use AI with lightweight codebase context (function/method signatures, class attributes and struct fields for Python and Go)\n",
      "  - `\"none\"`: Skip AI enhancement, use the provided description as-is\n",
      "- `debug`: (optional) If set to `true`, adds a comment to the issue with the full prompt used for generation\n",
      "- `disable_search_grounding`: (optional) If set to `true`, disables Google Search Grounding for this request\n",
      "\n",
      "**Output**:\n",
      "\n",
      "- JSON string containing:\n",
      "  - `issue_url`: URL to the created GitHub issue\n",
      "  - `issue_number`: The GitHub issue number\n",
      "\n",
      "**Error Handling**:\n",
      "\n",
      "If AI enhancement fails when using `codebase_reasoning=\"full\"`, a comment will be added to the issue with the error details, but the original issue body with title and description will be preserved.\n",
      "\n",
      "### get_workplan\n",
      "\n",
      "Retrieves the workplan content (GitHub issue body) associated with a specified GitHub issue.\n",
      "\n",
      "**Input**:\n",
      "\n",
      "- `issue_number`: The GitHub issue number for the workplan.\n",
      "- `disable_search_grounding`: (optional) If set to `true`, disables Google Search Grounding for this request\n",
      "\n",
      "**Output**:\n",
      "\n",
      "- The content of the workplan issue as a string\n",
      "\n",
      "### revise_workplan\n",
      "\n",
      "Updates an existing workplan based on revision instructions. The tool fetches the current workplan from the specified GitHub issue and uses AI to revise it according to your instructions.\n",
      "\n",
      "**Input**:\n",
      "\n",
      "- `issue_number`: The GitHub issue number containing the workplan to revise\n",
      "- `revision_instructions`: Instructions describing how to revise the workplan (e.g., \"Add more detail about testing\", \"Include database migration steps\", \"Update to use React instead of Vue\")\n",
      "- `codebase_reasoning`: (optional) Control whether AI enhancement is performed:\n",
      "  - `\"full\"`: (default) Use AI to revise with full codebase context\n",
      "  - `\"lsp\"`: Use AI with lightweight codebase context (function/method signatures only)\n",
      "  - `\"file_structure\"`: Use AI with directory structure only (fastest)\n",
      "  - `\"none\"`: Minimal codebase context\n",
      "- `debug`: (optional) If set to `true`, adds a comment to the issue with the full prompt used for generation\n",
      "- `disable_search_grounding`: (optional) If set to `true`, disables Google Search Grounding for this request\n",
      "\n",
      "**Output**:\n",
      "\n",
      "- JSON string containing:\n",
      "  - `issue_url`: URL to the updated GitHub issue\n",
      "  - `issue_number`: The GitHub issue number\n",
      "\n",
      "**Example Usage**:\n",
      "\n",
      "```\n",
      "Please revise workplan #123 to include more detail about error handling and recovery mechanisms.\n",
      "```\n",
      "\n",
      "This will:\n",
      "1. Fetch the existing workplan from issue #123\n",
      "2. Add a submission comment indicating the revision is in progress\n",
      "3. Launch a background task to revise the workplan based on your instructions\n",
      "4. Update the issue with the revised workplan once complete\n",
      "5. Add a completion comment with metrics\n",
      "\n",
      "### curate_context\n",
      "\n",
      "Analyzes the codebase structure to build a .yellhorncontext file with optimized directory filtering rules customized for your specific task. This tool creates a comprehensive context file that includes both blacklist/whitelist patterns to ensure the AI has access to the most relevant directories.\n",
      "\n",
      "**Input**:\n",
      "\n",
      "- `user_task`: Description of the task you're working on, used to customize directory selection.\n",
      "- `codebase_reasoning`: (optional) Analysis mode for codebase structure. Options:\n",
      "  - `\"full\"`: Performs deep analysis with all codebase context (default)\n",
      "  - `\"file_structure\"`: Lightweight analysis based only on file/directory structure\n",
      "  - `\"lsp\"`: Analysis using programming language constructs (functions, classes)\n",
      "- `ignore_file_path`: (optional) Path to the .yellhornignore file to use. Defaults to \".yellhornignore\".\n",
      "- `output_path`: (optional) Path where the .yellhorncontext file will be created. Defaults to \".yellhorncontext\".\n",
      "- `depth_limit`: (optional) Maximum directory depth to analyze (0 means no limit).\n",
      "- `disable_search_grounding`: (optional) If set to `true`, disables Google Search Grounding for this request\n",
      "\n",
      "**Output**:\n",
      "\n",
      "- Success message with path to created .yellhorncontext file and number of important directories.\n",
      "\n",
      "**Notes**:\n",
      "\n",
      "- This tool reads all files not blacklisted/whitelisted from the .yellhornignore file (if it exists).\n",
      "- It generates a new .yellhorncontext file or overwrites an existing one each time it's called.\n",
      "- The .yellhorncontext file is prioritized over .yellhornignore for file filtering in all tools.\n",
      "- It uses chunked LLM calls to process large codebases, analyzing directory structure and identifying important directories.\n",
      "- The generated file includes:\n",
      "  - Blacklist patterns from .yellhornignore (if it exists)\n",
      "  - Whitelist patterns from .yellhornignore (if it exists)\n",
      "  - Task-specific important directories as explicit whitelist patterns\n",
      "  - Recommended global blacklist pattern (commented out by default)\n",
      "- The tool customizes directory selection based on the specific task you're working on, ensuring the AI focuses on the most relevant parts of your codebase.\n",
      "- Three different reasoning modes allow for flexibility in how the codebase is analyzed:\n",
      "  - `full`: Most comprehensive analysis, includes file contents samples\n",
      "  - `file_structure`: Fastest analysis based only on file paths, best for large codebases\n",
      "  - `lsp`: Language-aware analysis, helps identify relevant code constructs\n",
      "- You can customize the output location using the `output_path` parameter if you don't want to use the default \".yellhorncontext\" at the repository root.\n",
      "\n",
      "**Example Usage**:\n",
      "\n",
      "```\n",
      "Please create a .yellhorncontext file optimized for \"implementing a user authentication system with JWT tokens\".\n",
      "```\n",
      "\n",
      "This will analyze your codebase with your specific task in mind, creating a .yellhorncontext file that:\n",
      "\n",
      "1. Preserves existing filtering from .yellhornignore (if it exists)\n",
      "2. Identifies the most important directories relevant to authentication and JWT tokens\n",
      "3. Creates explicit whitelist rules for those directories\n",
      "4. Provides a commented-out global blacklist rule that can be enabled for stricter filtering\n",
      "\n",
      "### judge_workplan\n",
      "\n",
      "Triggers an asynchronous code judgement comparing two git refs (branches or commits) against a workplan described in a GitHub issue. Creates a placeholder GitHub sub-issue immediately and then processes the AI judgement asynchronously, updating the sub-issue with results.\n",
      "\n",
      "**Workflow**:\n",
      "\n",
      "1. **Immediate Response**: Creates a placeholder sub-issue with task details and metadata\n",
      "2. **Asynchronous Processing**: Generates the AI judgement in the background  \n",
      "3. **Sub-issue Update**: Updates the placeholder with the complete judgement, including comparison metadata, detailed analysis, citations (if available), and completion metrics\n",
      "\n",
      "**Input**:\n",
      "\n",
      "- `issue_number`: The GitHub issue number for the workplan.\n",
      "- `base_ref`: Base Git ref (commit SHA, branch name, tag) for comparison. Defaults to 'main'.\n",
      "- `head_ref`: Head Git ref (commit SHA, branch name, tag) for comparison. Defaults to 'HEAD'.\n",
      "- `codebase_reasoning`: (optional) Control which codebase context is provided:\n",
      "  - `\"full\"`: (default) Use full codebase context\n",
      "  - `\"lsp\"`: Use lighter codebase context (function signatures, class attributes, etc. for Python and Go, plus full diff files)\n",
      "  - `\"file_structure\"`: Use only directory structure without file contents for faster processing\n",
      "  - `\"none\"`: Skip codebase context completely for fastest processing\n",
      "- `debug`: (optional) If set to `true`, adds a comment to the sub-issue with the full prompt used for generation\n",
      "- `disable_search_grounding`: (optional) If set to `true`, disables Google Search Grounding for this request\n",
      "\n",
      "Any URLs mentioned in the workplan will be extracted and preserved in a References section in the judgement.\n",
      "\n",
      "**Output**:\n",
      "\n",
      "- JSON string containing:\n",
      "  - `message`: Confirmation that the judgement task has been initiated\n",
      "  - `subissue_url`: URL to the created placeholder sub-issue where results will be posted\n",
      "  - `subissue_number`: The GitHub issue number of the placeholder sub-issue\n",
      "\n",
      "**Labels**:\n",
      "\n",
      "The created sub-issue is labeled with `yellhorn-judgement-subissue` for easy identification and filtering.\n",
      "\n",
      "## MCP Resources\n",
      "\n",
      "Yellhorn MCP implements the standard MCP resource API to provide easy access to workplans:\n",
      "\n",
      "### Resource Type: yellhorn_workplan\n",
      "\n",
      "Represents a GitHub issue created by the Yellhorn MCP server with a detailed implementation plan.\n",
      "\n",
      "**Resource Fields**:\n",
      "\n",
      "- `id`: The GitHub issue number\n",
      "- `type`: Always \"yellhorn_workplan\"\n",
      "- `name`: The title of the GitHub issue\n",
      "- `metadata`: Additional information about the issue, including its URL\n",
      "\n",
      "### Accessing Resources\n",
      "\n",
      "Use the standard MCP commands to list and access workplans:\n",
      "\n",
      "```bash\n",
      "# List all workplans\n",
      "mcp list-resources yellhorn-mcp\n",
      "\n",
      "# Get a specific workplan by issue number\n",
      "mcp get-resource yellhorn-mcp 123\n",
      "```\n",
      "\n",
      "Or programmatically with the MCP client API:\n",
      "\n",
      "```python\n",
      "# List workplans\n",
      "resources = await session.list_resources()\n",
      "\n",
      "# Get a workplan by ID\n",
      "workplan = await session.get_resource(\"123\")\n",
      "```\n",
      "\n",
      "## Integration with Other Programs\n",
      "\n",
      "### HTTP API\n",
      "\n",
      "When running in standalone mode, Yellhorn MCP exposes a standard HTTP API that can be accessed by any HTTP client:\n",
      "\n",
      "```bash\n",
      "# Run the server with default settings (search grounding enabled for Gemini models)\n",
      "yellhorn-mcp --host 127.0.0.1 --port 8000\n",
      "\n",
      "# Run the server with search grounding disabled\n",
      "yellhorn-mcp --host 127.0.0.1 --port 8000 --no-search-grounding\n",
      "```\n",
      "\n",
      "You can then make requests to the server's API endpoints:\n",
      "\n",
      "```bash\n",
      "# Get the OpenAPI schema\n",
      "curl http://127.0.0.1:8000/openapi.json\n",
      "\n",
      "# List available tools\n",
      "curl http://127.0.0.1:8000/tools\n",
      "\n",
      "# Call a tool (create_workplan with full codebase context)\n",
      "curl -X POST http://127.0.0.1:8000/tools/create_workplan \\\n",
      "  -H \"Content-Type: application/json\" \\\n",
      "  -d '{\"title\": \"User Authentication System\", \"detailed_description\": \"Implement a secure authentication system using JWT tokens and bcrypt for password hashing\", \"codebase_reasoning\": \"full\"}'\n",
      "\n",
      "# Call a tool (create_workplan with lightweight LSP context - function signatures only)\n",
      "curl -X POST http://127.0.0.1:8000/tools/create_workplan \\\n",
      "  -H \"Content-Type: application/json\" \\\n",
      "  -d '{\"title\": \"User Authentication System\", \"detailed_description\": \"Implement a secure authentication system using JWT tokens and bcrypt for password hashing\", \"codebase_reasoning\": \"lsp\"}'\n",
      "\n",
      "# Call a tool (create_workplan without AI enhancement)\n",
      "curl -X POST http://127.0.0.1:8000/tools/create_workplan \\\n",
      "  -H \"Content-Type: application/json\" \\\n",
      "  -d '{\"title\": \"User Authentication System\", \"detailed_description\": \"Implement a secure authentication system using JWT tokens and bcrypt for password hashing\", \"codebase_reasoning\": \"none\"}'\n",
      "\n",
      "# Call a tool (get_workplan)\n",
      "curl -X POST http://127.0.0.1:8000/tools/get_workplan \\\n",
      "  -H \"Content-Type: application/json\" \\\n",
      "  -d '{\"issue_number\": \"123\"}'\n",
      "\n",
      "# Call a tool (revise_workplan)\n",
      "curl -X POST http://127.0.0.1:8000/tools/revise_workplan \\\n",
      "  -H \"Content-Type: application/json\" \\\n",
      "  -d '{\"issue_number\": \"123\", \"revision_instructions\": \"Add more detail about error handling and recovery mechanisms\", \"codebase_reasoning\": \"full\"}'\n",
      "\n",
      "# Call a tool (curate_context)\n",
      "curl -X POST http://127.0.0.1:8000/tools/curate_context \\\n",
      "  -H \"Content-Type: application/json\" \\\n",
      "  -d '{\"user_task\": \"Implementing a user authentication system with JWT tokens\", \"codebase_reasoning\": \"file_structure\", \"output_path\": \".yellhorncontext\"}'\n",
      "\n",
      "# Call a tool (judge_workplan with full codebase context - default)\n",
      "curl -X POST http://127.0.0.1:8000/tools/judge_workplan \\\n",
      "  -H \"Content-Type: application/json\" \\\n",
      "  -d '{\"issue_number\": \"456\", \"base_ref\": \"main\", \"head_ref\": \"feature-branch\"}'\n",
      "  \n",
      "# Call a tool (judge_workplan with lightweight LSP context)\n",
      "curl -X POST http://127.0.0.1:8000/tools/judge_workplan \\\n",
      "  -H \"Content-Type: application/json\" \\\n",
      "  -d '{\"issue_number\": \"456\", \"base_ref\": \"main\", \"head_ref\": \"feature-branch\", \"codebase_reasoning\": \"lsp\"}'\n",
      "```\n",
      "\n",
      "### Example Client\n",
      "\n",
      "The package includes an example client that demonstrates how to interact with the server programmatically:\n",
      "\n",
      "```bash\n",
      "# List available tools\n",
      "python -m examples.client_example list\n",
      "\n",
      "# Generate a workplan with full codebase context (default)\n",
      "python -m examples.client_example plan --title \"User Authentication System\" --description \"Implement a secure authentication system using JWT tokens and bcrypt for password hashing\"\n",
      "\n",
      "# Generate a workplan with lightweight LSP context (function signatures only)\n",
      "python -m examples.client_example plan --title \"User Authentication System\" --description \"Implement a secure authentication system using JWT tokens and bcrypt for password hashing\" --codebase-reasoning lsp\n",
      "\n",
      "# Generate a basic workplan without AI enhancement\n",
      "python -m examples.client_example plan --title \"User Authentication System\" --description \"Implement a secure authentication system using JWT tokens and bcrypt for password hashing\" --codebase-reasoning none\n",
      "\n",
      "# Get workplan\n",
      "python -m examples.client_example getplan --issue-number \"123\"\n",
      "\n",
      "# Generate a .yellhorncontext file (using file_structure mode for fast analysis)\n",
      "python -m examples.client_example curate-context --user-task \"Implementing a user authentication system with JWT tokens\" --codebase-reasoning file_structure\n",
      "\n",
      "# Judge work with full codebase context (default)\n",
      "python -m examples.client_example judge --issue-number \"456\" --base-ref \"main\" --head-ref \"feature-branch\"\n",
      "\n",
      "# Judge work with lightweight LSP context (function signatures + full diff files)\n",
      "python -m examples.client_example judge --issue-number \"456\" --base-ref \"main\" --head-ref \"feature-branch\" --codebase-reasoning lsp\n",
      "```\n",
      "\n",
      "The example client uses the MCP client API to interact with the server through stdio transport, which is the same approach Claude Code uses.\n",
      "\n",
      "## Debugging and Troubleshooting\n",
      "\n",
      "### Common Issues\n",
      "\n",
      "1. **API Key Not Set**: Make sure your `GEMINI_API_KEY` or `OPENAI_API_KEY` environment variable is set depending on your chosen model.\n",
      "2. **Not a Git Repository**: Ensure that `REPO_PATH` points to a valid Git repository.\n",
      "3. **GitHub CLI Issues**: Ensure GitHub CLI (`gh`) is installed, accessible in your PATH, and authenticated.\n",
      "4. **MCP Connection Issues**: If you have trouble connecting to the server, check that you're using the latest version of the MCP SDK.\n",
      "5. **Rate Limit Errors**: The server now automatically retries with exponential backoff, but persistent rate limits may indicate quota issues.\n",
      "6. **Large Codebase Issues**: The server automatically chunks large prompts, but very large codebases may still hit limits.\n",
      "\n",
      "### Error Messages\n",
      "\n",
      "- `GEMINI_API_KEY is required`: Set your Gemini API key as an environment variable.\n",
      "- `OPENAI_API_KEY is required`: Set your OpenAI API key as an environment variable.\n",
      "- `Not a Git repository`: The specified path is not a Git repository.\n",
      "- `Git executable not found`: Ensure Git is installed and accessible in your PATH.\n",
      "- `GitHub CLI not found`: Ensure GitHub CLI (`gh`) is installed and accessible in your PATH.\n",
      "- `GitHub CLI command failed`: Check that GitHub CLI is authenticated and has appropriate permissions.\n",
      "- `Failed to generate workplan`: Check the API key and model name for your chosen provider.\n",
      "- `Failed to create GitHub issue`: Check GitHub CLI authentication and permissions.\n",
      "- `Failed to fetch GitHub issue/PR content`: The issue or PR URL may be invalid or inaccessible.\n",
      "- `Failed to fetch GitHub PR diff`: The PR URL may be invalid or inaccessible.\n",
      "- `Failed to post GitHub PR review`: Check GitHub CLI permissions for posting PR comments.\n",
      "- `Rate limit exceeded`: The server will automatically retry, but check your API quota if persistent.\n",
      "- `Context window exceeded`: The server will automatically chunk large prompts, but very large codebases may still hit limits.\n",
      "\n",
      "## CI/CD\n",
      "\n",
      "The project includes GitHub Actions workflows for automated testing and deployment.\n",
      "\n",
      "### Testing Workflow\n",
      "\n",
      "The testing workflow automatically runs when:\n",
      "\n",
      "- Pull requests are opened against the main branch\n",
      "- Pushes are made to the main branch\n",
      "\n",
      "It performs the following steps:\n",
      "\n",
      "1. Sets up Python environments (3.10 and 3.11)\n",
      "2. Installs dependencies\n",
      "3. Runs linting with flake8\n",
      "4. Checks formatting with black\n",
      "5. Runs tests with pytest\n",
      "6. Checks test coverage against required thresholds (≥ 70% line coverage)\n",
      "\n",
      "The workflow configuration is in `.github/workflows/tests.yml`.\n",
      "\n",
      "### Publishing Workflow\n",
      "\n",
      "The publishing workflow automatically runs when:\n",
      "\n",
      "- A version tag (v*) is pushed to the repository\n",
      "\n",
      "It performs the following steps:\n",
      "\n",
      "1. Sets up Python 3.10\n",
      "2. Verifies that the tag version matches the version in pyproject.toml\n",
      "3. Builds the package\n",
      "4. Publishes the package to PyPI\n",
      "\n",
      "The workflow configuration is in `.github/workflows/publish.yml`.\n",
      "\n",
      "#### Publishing Requirements\n",
      "\n",
      "To publish to PyPI, you need to:\n",
      "\n",
      "1. Create a PyPI API token\n",
      "2. Store it as a repository secret in GitHub named `PYPI_API_TOKEN`\n",
      "\n",
      "#### Creating a PyPI API Token\n",
      "\n",
      "1. Log in to your PyPI account\n",
      "2. Go to Account Settings > API tokens\n",
      "3. Create a new token with scope \"Entire account\" or specific to the yellhorn-mcp project\n",
      "4. Copy the token value\n",
      "\n",
      "#### Adding the Secret to GitHub\n",
      "\n",
      "1. Go to your GitHub repository\n",
      "2. Navigate to Settings > Secrets and variables > Actions\n",
      "3. Click \"New repository secret\"\n",
      "4. Set the name to `PYPI_API_TOKEN`\n",
      "5. Paste the token value\n",
      "6. Click \"Add secret\"\n",
      "\n",
      "#### Releasing a New Version\n",
      "\n",
      "1. Update the version in pyproject.toml\n",
      "2. Update the version in yellhorn_mcp/**init**.py (if needed)\n",
      "3. Commit changes: `git commit -am \"Bump version to X.Y.Z\"`\n",
      "4. Tag the commit: `git tag vX.Y.Z`\n",
      "5. Push changes and tag: `git push && git push --tags`\n",
      "\n",
      "The publishing workflow will automatically run when the tag is pushed, building and publishing the package to PyPI.\n",
      "\n",
      "## Advanced Configuration\n",
      "\n",
      "For advanced use cases, you can modify the server's behavior by editing the source code:\n",
      "\n",
      "- Adjust the prompt templates in `process_workplan_async` and `process_judgement_async` functions\n",
      "- Modify the codebase preprocessing in `get_codebase_snapshot` and `format_codebase_for_prompt`\n",
      "- Change the Gemini model version with the `YELLHORN_MCP_MODEL` environment variable\n",
      "- Toggle Google Search Grounding with the `YELLHORN_MCP_SEARCH` environment variable\n",
      "- Customize the directory tree representation in `tree_utils.py`\n",
      "- Add support for additional languages in the LSP mode by extending `lsp_utils.py`\n",
      "\n",
      "## LSP Mode Language Support\n",
      "\n",
      "The \"lsp\" codebase reasoning mode provides a lightweight representation of your codebase by extracting language constructs rather than including full file contents. This mode reduces token usage while still providing useful context for AI reasoning.\n",
      "\n",
      "### Python Language Features\n",
      "\n",
      "The LSP mode extracts the following Python language constructs:\n",
      "\n",
      "- **Function signatures** with parameter types and return types\n",
      "- **Class definitions** with inheritance information\n",
      "- **Class attributes** including type annotations when available\n",
      "- **Method signatures** with parameter types and return types\n",
      "- **Enum definitions** with their literal values\n",
      "- **Docstrings** (first line only) for functions, classes, and methods\n",
      "\n",
      "Example Python LSP extraction:\n",
      "\n",
      "```\n",
      "class Size(Enum)  # Pizza size options\n",
      "    SMALL\n",
      "    MEDIUM\n",
      "    LARGE\n",
      "class Pizza  # Delicious disc of dough\n",
      "    name: str\n",
      "    radius: float\n",
      "    toppings: List[T]\n",
      "    def Pizza.calculate_price(self, tax_rate: float = 0.1) -> float  # Calculate price with tax\n",
      "    def Pizza.add_topping(self, topping: T) -> None  # Add a topping\n",
      "def top_level_helper(x: int) -> int  # Helper function\n",
      "```\n",
      "\n",
      "### Go Language Features\n",
      "\n",
      "The LSP mode extracts the following Go language constructs:\n",
      "\n",
      "- **Function signatures** with parameter types and return types\n",
      "- **Struct definitions** with field names and types\n",
      "- **Interface definitions**\n",
      "- **Type definitions** (e.g., type aliases)\n",
      "- **Receiver methods** with support for pointer receivers and generics\n",
      "\n",
      "Example Go LSP extraction:\n",
      "\n",
      "```\n",
      "type Size int\n",
      "struct Topping { Name string Price float64 Vegetarian bool }\n",
      "struct Oven { Temperature int ModelName string }\n",
      "func (o *Oven) Heat(temperature int) error\n",
      "func (o *Oven) Bake[T any](p Pizza[T]) (err error)\n",
      "func (p *Pizza[T]) AddTopping(t T)\n",
      "func Calculate(radius float64) float64\n",
      "```\n",
      "\n",
      "### Diff-aware Processing\n",
      "\n",
      "The LSP mode is aware of file differences when using the `judge_workplan` tool:\n",
      "\n",
      "1. It first extracts lightweight signatures for all files\n",
      "2. Then it identifies which files are included in the diff\n",
      "3. Those diff-affected files are included in full, rather than just their signatures\n",
      "4. This provides complete context for changed files while keeping the overall token count low\n",
      "\n",
      "### Server Dependencies\n",
      "\n",
      "The server declares its dependencies using the FastMCP dependencies parameter:\n",
      "\n",
      "```python\n",
      "mcp = FastMCP(\n",
      "    name=\"yellhorn-mcp\",\n",
      "    dependencies=[\"google-genai~=1.8.0\", \"aiohttp~=3.11.14\", \"pydantic~=2.11.1\"],\n",
      "    lifespan=app_lifespan,\n",
      ")\n",
      "```\n",
      "\n",
      "This ensures that when the server is installed in Claude Desktop or used with the MCP CLI, all required dependencies are installed automatically.\n",
      "\n",
      "--- File: notebooks/llm_manager.ipynb ---\n",
      "{\n",
      " \"cells\": [\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"# Yellhorn MCP Example in Notebook with LLM Manager\\n\",\n",
      "    \"\\n\",\n",
      "    \"Instruction: Swap model to get different behavior LLM Manager config (Normal, Test full chunking, Test full with retry, Test full chunking & retry)\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"## Setup Environment\\n\",\n",
      "    \"\\n\",\n",
      "    \"First, let's set up our environment and import the necessary modules:\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 1,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": [\n",
      "    \"%load_ext autoreload\\n\",\n",
      "    \"%autoreload 2\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 2,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": [\n",
      "    \"import os\\n\",\n",
      "    \"import asyncio\\n\",\n",
      "    \"import json\\n\",\n",
      "    \"from typing import Dict, List, Optional, Union, Any\\n\",\n",
      "    \"from pathlib import Path\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Import required Yellhorn MCP components\\n\",\n",
      "    \"from yellhorn_mcp.token_counter import TokenCounter\\n\",\n",
      "    \"from yellhorn_mcp.llm_manager import LLMManager\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Import API clients\\n\",\n",
      "    \"from google import genai\\n\",\n",
      "    \"from openai import AsyncOpenAI\\n\",\n",
      "    \"\\n\",\n",
      "    \"from yellhorn_mcp.server import format_metrics_section, calculate_cost\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"## Configure API Keys\\n\",\n",
      "    \"\\n\",\n",
      "    \"Set up API keys for Gemini and/or OpenAI. You can either set them in environment variables or directly in this notebook:\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 3,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": [\n",
      "    \"# Option 1: Set API keys directly (not recommended for production)\\n\",\n",
      "    \"GEMINI_API_KEY = \\\"AIzaSyAiyZ2CHqbju1pJUjHlR6XeuuSPPB2uadg\\\"\\n\",\n",
      "    \"OPENAI_API_KEY = \\\"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\\\"\\n\",\n",
      "    \"\\n\",\n",
      "    \"MODEL = \\\"gemini-2.5-flash\\\"  # or any OpenAI model like \\\"gpt-4o\\\"\\n\",\n",
      "    \"REPO_PATH = os.path.dirname(os.getcwd())\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Option 2: Get API keys from environment variables (recommended)\\n\",\n",
      "    \"# GEMINI_API_KEY = os.environ.get(\\\"GEMINI_API_KEY\\\")\\n\",\n",
      "    \"# OPENAI_API_KEY = os.environ.get(\\\"OPENAI_API_KEY\\\")\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Set environment variables for server access\\n\",\n",
      "    \"# os.environ[\\\"GEMINI_API_KEY\\\"] = GEMINI_API_KEY\\n\",\n",
      "    \"# os.environ[\\\"OPENAI_API_KEY\\\"] = OPENAI_API_KEY\\n\",\n",
      "    \"# os.environ[\\\"REPO_PATH\\\"] = REPO_PATH\\n\",\n",
      "    \"# os.environ[\\\"YELLHORN_MCP_MODEL\\\"] = MODEL\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"## 1. Working with LLMManager\\n\",\n",
      "    \"\\n\",\n",
      "    \"Now let's set up and use LLMManager, which provides unified access to different LLM APIs with automatic chunking.\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 4,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": [\n",
      "    \"# Initialize API clients\\n\",\n",
      "    \"gemini_client = genai.Client(api_key=GEMINI_API_KEY)\\n\",\n",
      "    \"openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Create LLMManager with custom configuration\\n\",\n",
      "    \"config = {\\n\",\n",
      "    \"    \\\"safety_margin_tokens\\\": 200,\\n\",\n",
      "    \"    \\\"overlap_ratio\\\": 0.1,\\n\",\n",
      "    \"    \\\"aggregation_strategy\\\": \\\"concatenate\\\",\\n\",\n",
      "    \"    \\\"chunk_strategy\\\": \\\"paragraph\\\",\\n\",\n",
      "    \"    # Experimental limits to test chunking & retry behavior\\n\",\n",
      "    \"    \\\"model_limits\\\" : {\\n\",\n",
      "    \"        \\\"gpt-4o\\\": 30000,\\n\",\n",
      "    \"        \\\"gemini-2.0-flash-exp\\\": 10000, \\n\",\n",
      "    \"        \\\"o4-mini\\\": 30000,\\n\",\n",
      "    \"        \\\"gemini-2.5-pro\\\": 1_000_000\\n\",\n",
      "    \"    }\\n\",\n",
      "    \"}\\n\",\n",
      "    \"\\n\",\n",
      "    \"llm_manager = LLMManager(\\n\",\n",
      "    \"    openai_client=openai_client,\\n\",\n",
      "    \"    gemini_client=gemini_client,\\n\",\n",
      "    \"    config=config\\n\",\n",
      "    \")\\n\",\n",
      "    \"\\n\",\n",
      "    \"def log_callback(level, message):\\n\",\n",
      "    \"    \\\"\\\"\\\"Custom log callback function.\\\"\\\"\\\"\\n\",\n",
      "    \"    print(f\\\"[{level.upper()}] {message}\\\")\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 5,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": [\n",
      "    \"# Function to make API calls and handle async operations\\n\",\n",
      "    \"async def call_model(prompt, model, system_message=None, response_format=None):\\n\",\n",
      "    \"    \\\"\\\"\\\"Helper function to call a model using LLMManager.\\\"\\\"\\\"\\n\",\n",
      "    \"    try:\\n\",\n",
      "    \"        response_dict = await llm_manager.call_llm_with_usage(\\n\",\n",
      "    \"            prompt=prompt,\\n\",\n",
      "    \"            model=model,\\n\",\n",
      "    \"            temperature=0.0,\\n\",\n",
      "    \"            system_message=system_message,\\n\",\n",
      "    \"            response_format=response_format\\n\",\n",
      "    \"        )\\n\",\n",
      "    \"        return response_dict\\n\",\n",
      "    \"    except Exception as e:\\n\",\n",
      "    \"        return f\\\"Error: {str(e)}\\\"\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"## 2. Test LLM Manager Simple vs Chunking\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"### Simple vs Chunked\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 6,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"data\": {\n",
      "      \"text/html\": [\n",
      "       \"<pre style=\\\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\\\"><span style=\\\"color: #7fbfbf; text-decoration-color: #7fbfbf\\\">[07/12/25 17:34:35] </span><span style=\\\"color: #000080; text-decoration-color: #000080\\\">INFO    </span> HTTP Request: <span style=\\\"color: #808000; text-decoration-color: #808000; font-weight: bold\\\">POST</span> <span style=\\\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\\\">https://api.openai.com/v1/responses</span> <span style=\\\"color: #008000; text-decoration-color: #008000\\\">\\\"HTTP/1.1 200 </span>  <a href=\\\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\\\" target=\\\"_blank\\\"><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">_client.py</span></a><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">:</span><a href=\\\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\\\" target=\\\"_blank\\\"><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">1740</span></a>\\n\",\n",
      "       \"<span style=\\\"color: #7fbfbf; text-decoration-color: #7fbfbf\\\">                    </span>         <span style=\\\"color: #008000; text-decoration-color: #008000\\\">OK\\\"</span>                                                                    <span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">               </span>\\n\",\n",
      "       \"</pre>\\n\"\n",
      "      ],\n",
      "      \"text/plain\": [\n",
      "       \"\\u001b[2;36m[07/12/25 17:34:35]\\u001b[0m\\u001b[2;36m \\u001b[0m\\u001b[34mINFO    \\u001b[0m HTTP Request: \\u001b[1;33mPOST\\u001b[0m \\u001b[4;94mhttps://api.openai.com/v1/responses\\u001b[0m \\u001b[32m\\\"HTTP/1.1 200 \\u001b[0m  \\u001b]8;id=313958;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\\u001b\\\\\\u001b[2m_client.py\\u001b[0m\\u001b]8;;\\u001b\\\\\\u001b[2m:\\u001b[0m\\u001b]8;id=288659;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\\u001b\\\\\\u001b[2m1740\\u001b[0m\\u001b]8;;\\u001b\\\\\\n\",\n",
      "       \"\\u001b[2;36m                    \\u001b[0m         \\u001b[32mOK\\\"\\u001b[0m                                                                    \\u001b[2m               \\u001b[0m\\n\"\n",
      "      ]\n",
      "     },\n",
      "     \"metadata\": {},\n",
      "     \"output_type\": \"display_data\"\n",
      "    },\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"OpenAI JSON Response:\\n\",\n",
      "      \"--------------------------------------------------\\n\",\n",
      "      \"<yellhorn_mcp.llm_manager.UsageMetadata object at 0x114cf6f50>\\n\",\n",
      "      \"Sure! Here’s a list of 3 programming languages with their key features:\\n\",\n",
      "      \"\\n\",\n",
      "      \"---\\n\",\n",
      "      \"\\n\",\n",
      "      \"**1. Python**\\n\",\n",
      "      \"- Easy-to-read, clean syntax\\n\",\n",
      "      \"- Extensive standard library\\n\",\n",
      "      \"- Dynamically typed and interpreted\\n\",\n",
      "      \"- Large community support\\n\",\n",
      "      \"- Widely used for web development, data science, automation, and AI\\n\",\n",
      "      \"\\n\",\n",
      "      \"---\\n\",\n",
      "      \"\\n\",\n",
      "      \"**2. JavaScript**\\n\",\n",
      "      \"- Runs natively in web browsers\\n\",\n",
      "      \"- Event-driven, asynchronous programming support\\n\",\n",
      "      \"- Prototype-based object orientation\\n\",\n",
      "      \"- Essential for front-end web development\\n\",\n",
      "      \"- Large ecosystem with frameworks like React, Angular, and Vue\\n\",\n",
      "      \"\\n\",\n",
      "      \"---\\n\",\n",
      "      \"\\n\",\n",
      "      \"**3. Java**\\n\",\n",
      "      \"- Statically typed and compiled to bytecode (runs on JVM)\\n\",\n",
      "      \"- Strong object-oriented programming support\\n\",\n",
      "      \"- Platform-independent (“write once, run anywhere”)\\n\",\n",
      "      \"- Robust standard library and tools\\n\",\n",
      "      \"- Commonly used for enterprise applications, Android development, and backend systems\\n\",\n",
      "      \"\\n\",\n",
      "      \"---\\n\",\n",
      "      \"\\n\",\n",
      "      \"\\n\",\n",
      "      \"---\\n\",\n",
      "      \"## Completion Metrics\\n\",\n",
      "      \"*   **Model Used**: N/A\\n\",\n",
      "      \"*   **Input Tokens**: N/A\\n\",\n",
      "      \"*   **Output Tokens**: N/A\\n\",\n",
      "      \"*   **Total Tokens**: N/A\\n\",\n",
      "      \"*   **Estimated Cost**: N/A\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"# Example 2: OpenAI call with JSON response\\n\",\n",
      "    \"json_prompt = \\\"Generate a list of 3 programming languages with their key features.\\\"\\n\",\n",
      "    \"\\n\",\n",
      "    \"model = \\\"gpt-4.1\\\"\\n\",\n",
      "    \"openai_json_response = await call_model(\\n\",\n",
      "    \"    prompt=json_prompt,\\n\",\n",
      "    \"    model=model,  # or any available OpenAI model\\n\",\n",
      "    \"    # response_format=\\\"json\\\"\\n\",\n",
      "    \")\\n\",\n",
      "    \"\\n\",\n",
      "    \"print(\\\"OpenAI JSON Response:\\\")\\n\",\n",
      "    \"print(\\\"-\\\" * 50)\\n\",\n",
      "    \"print(openai_json_response[\\\"usage_metadata\\\"])\\n\",\n",
      "    \"print(openai_json_response[\\\"content\\\"])\\n\",\n",
      "    \"print(format_metrics_section(\\\"gpt-4.1\\\",openai_json_response[\\\"usage_metadata\\\"]))\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": null,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": [\n",
      "    \"from yellhorn_mcp.llm_manager import ChunkingStrategy\\n\",\n",
      "    \"from yellhorn_mcp.token_counter import TokenCounter\\n\",\n",
      "    \"\\n\",\n",
      "    \"chunks = ChunkingStrategy.split_by_paragraphs(\\n\",\n",
      "    \"    text=json_prompt,\\n\",\n",
      "    \"    max_tokens=5000,\\n\",\n",
      "    \"    token_counter=TokenCounter(),\\n\",\n",
      "    \"    model=\\\"gemini-2.0-flash-exp\\\"\\n\",\n",
      "    \")\\n\",\n",
      "    \"\\n\",\n",
      "    \"for chunk in chunks:\\n\",\n",
      "    \"    print(TokenCounter().count_tokens(chunk, \\\"gemini-2.0-flash-exp\\\"))\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 6,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"data\": {\n",
      "      \"text/html\": [\n",
      "       \"<pre style=\\\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\\\"><span style=\\\"color: #7fbfbf; text-decoration-color: #7fbfbf\\\">[07/13/25 19:48:48] </span><span style=\\\"color: #000080; text-decoration-color: #000080\\\">INFO    </span> HTTP Request: <span style=\\\"color: #808000; text-decoration-color: #808000; font-weight: bold\\\">POST</span> <span style=\\\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\\\">https://api.openai.com/v1/responses</span> <span style=\\\"color: #008000; text-decoration-color: #008000\\\">\\\"HTTP/1.1 200 </span>  <a href=\\\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\\\" target=\\\"_blank\\\"><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">_client.py</span></a><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">:</span><a href=\\\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\\\" target=\\\"_blank\\\"><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">1740</span></a>\\n\",\n",
      "       \"<span style=\\\"color: #7fbfbf; text-decoration-color: #7fbfbf\\\">                    </span>         <span style=\\\"color: #008000; text-decoration-color: #008000\\\">OK\\\"</span>                                                                    <span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">               </span>\\n\",\n",
      "       \"</pre>\\n\"\n",
      "      ],\n",
      "      \"text/plain\": [\n",
      "       \"\\u001b[2;36m[07/13/25 19:48:48]\\u001b[0m\\u001b[2;36m \\u001b[0m\\u001b[34mINFO    \\u001b[0m HTTP Request: \\u001b[1;33mPOST\\u001b[0m \\u001b[4;94mhttps://api.openai.com/v1/responses\\u001b[0m \\u001b[32m\\\"HTTP/1.1 200 \\u001b[0m  \\u001b]8;id=576016;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\\u001b\\\\\\u001b[2m_client.py\\u001b[0m\\u001b]8;;\\u001b\\\\\\u001b[2m:\\u001b[0m\\u001b]8;id=881369;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\\u001b\\\\\\u001b[2m1740\\u001b[0m\\u001b]8;;\\u001b\\\\\\n\",\n",
      "       \"\\u001b[2;36m                    \\u001b[0m         \\u001b[32mOK\\\"\\u001b[0m                                                                    \\u001b[2m               \\u001b[0m\\n\"\n",
      "      ]\n",
      "     },\n",
      "     \"metadata\": {},\n",
      "     \"output_type\": \"display_data\"\n",
      "    },\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"Gemini Response:\\n\",\n",
      "      \"--------------------------------------------------\\n\",\n",
      "      \"**Token Chunking: An Overview**\\n\",\n",
      "      \"\\n\",\n",
      "      \"**What is Token Chunking?**\\n\",\n",
      "      \"Token chunking is a technique used in natural language processing (NLP) to break down text into smaller, manageable pieces called \\\"tokens.\\\" These tokens can be words, phrases, or even characters, depending on the context and the specific application. The process involves segmenting a continuous stream of text into discrete units that can be analyzed or processed by algorithms.\\n\",\n",
      "      \"\\n\",\n",
      "      \"**Why is Token Chunking Important for Large Language Models?**\\n\",\n",
      "      \"\\n\",\n",
      "      \"1. **Efficiency**: Large language models (LLMs) often deal with vast amounts of text data. Token chunking allows these models to process text in smaller segments, making computations more efficient and manageable.\\n\",\n",
      "      \"\\n\",\n",
      "      \"2. **Context Preservation**: By chunking text into meaningful units, models can better understand the context and relationships between words or phrases. This is crucial for tasks like sentiment analysis, translation, and summarization.\\n\",\n",
      "      \"\\n\",\n",
      "      \"3. **Memory Management**: LLMs have limitations on the amount of text they can process at once (often referred to as the \\\"context window\\\"). Token chunking helps fit larger texts into these constraints by breaking them into smaller, coherent parts.\\n\",\n",
      "      \"\\n\",\n",
      "      \"4. **Improved Performance**: Models trained on chunked data can achieve better performance in various NLP tasks. This is because chunking helps maintain the semantic integrity of the text, allowing the model to learn more effectively.\\n\",\n",
      "      \"\\n\",\n",
      "      \"5. **Flexibility**: Different applications may require different chunking strategies (e.g., word-level, sentence-level, or paragraph-level). Token chunking provides the flexibility to adapt to these varying needs.\\n\",\n",
      "      \"\\n\",\n",
      "      \"In summary, token chunking is a fundamental technique that enhances the efficiency, context understanding, and overall performance of large language models in processing and analyzing text data.\\n\",\n",
      "      \"\\n\",\n",
      "      \"\\n\",\n",
      "      \"---\\n\",\n",
      "      \"## Completion Metrics\\n\",\n",
      "      \"*   **Model Used**: `gpt-4o-mini`\\n\",\n",
      "      \"*   **Input Tokens**: 15024\\n\",\n",
      "      \"*   **Output Tokens**: 356\\n\",\n",
      "      \"*   **Total Tokens**: 15380\\n\",\n",
      "      \"*   **Estimated Cost**: $0.0025\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"# Example 1: Simple Gemini call\\n\",\n",
      "    \"prompt = \\\"Explain what token chunking is and why it's important for large language models.\\\"*1000\\n\",\n",
      "    \"system_message = \\\"You are a helpful AI assistant that provides clear and concise explanations.\\\"\\n\",\n",
      "    \"\\n\",\n",
      "    \"model = \\\"gpt-4o-mini\\\"\\n\",\n",
      "    \"gemini_response = await call_model(\\n\",\n",
      "    \"    prompt=prompt,\\n\",\n",
      "    \"    model=model,\\n\",\n",
      "    \"    system_message=system_message\\n\",\n",
      "    \")\\n\",\n",
      "    \"\\n\",\n",
      "    \"print(\\\"Gemini Response:\\\")\\n\",\n",
      "    \"print(\\\"-\\\" * 50)\\n\",\n",
      "    \"print(gemini_response[\\\"content\\\"])\\n\",\n",
      "    \"print(format_metrics_section(model,gemini_response[\\\"usage_metadata\\\"]))\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 11,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"data\": {\n",
      "      \"text/plain\": [\n",
      "       \"<yellhorn_mcp.llm_manager.UsageMetadata at 0x10cfdd810>\"\n",
      "      ]\n",
      "     },\n",
      "     \"execution_count\": 11,\n",
      "     \"metadata\": {},\n",
      "     \"output_type\": \"execute_result\"\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"gemini_response[\\\"usage_metadata\\\"]\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"### Test Grounded Search\\n\",\n",
      "    \"\\n\",\n",
      "    \"Google Search Grounding is a feature available for Gemini models that allows them to search the web and include citations in their responses. This is particularly useful for getting up-to-date information and verifying facts.\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 31,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": [\n",
      "    \"# Import the search grounding utilities\\n\",\n",
      "    \"from yellhorn_mcp.search_grounding import _get_gemini_search_tools, add_citations_from_metadata\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 32,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"data\": {\n",
      "      \"text/html\": [\n",
      "       \"<pre style=\\\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\\\"><span style=\\\"color: #7fbfbf; text-decoration-color: #7fbfbf\\\">[07/12/25 17:22:16] </span><span style=\\\"color: #000080; text-decoration-color: #000080\\\">INFO    </span> AFC is enabled with max remote calls: <span style=\\\"color: #008080; text-decoration-color: #008080; font-weight: bold\\\">10</span>.                               <a href=\\\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py\\\" target=\\\"_blank\\\"><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">models.py</span></a><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">:</span><a href=\\\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py#7118\\\" target=\\\"_blank\\\"><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">7118</span></a>\\n\",\n",
      "       \"</pre>\\n\"\n",
      "      ],\n",
      "      \"text/plain\": [\n",
      "       \"\\u001b[2;36m[07/12/25 17:22:16]\\u001b[0m\\u001b[2;36m \\u001b[0m\\u001b[34mINFO    \\u001b[0m AFC is enabled with max remote calls: \\u001b[1;36m10\\u001b[0m.                               \\u001b]8;id=502880;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py\\u001b\\\\\\u001b[2mmodels.py\\u001b[0m\\u001b]8;;\\u001b\\\\\\u001b[2m:\\u001b[0m\\u001b]8;id=125787;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py#7118\\u001b\\\\\\u001b[2m7118\\u001b[0m\\u001b]8;;\\u001b\\\\\\n\"\n",
      "      ]\n",
      "     },\n",
      "     \"metadata\": {},\n",
      "     \"output_type\": \"display_data\"\n",
      "    },\n",
      "    {\n",
      "     \"data\": {\n",
      "      \"text/html\": [\n",
      "       \"<pre style=\\\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\\\"><span style=\\\"color: #7fbfbf; text-decoration-color: #7fbfbf\\\">[07/12/25 17:22:40] </span><span style=\\\"color: #000080; text-decoration-color: #000080\\\">INFO    </span> HTTP Request: <span style=\\\"color: #808000; text-decoration-color: #808000; font-weight: bold\\\">POST</span>                                                     <a href=\\\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\\\" target=\\\"_blank\\\"><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">_client.py</span></a><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">:</span><a href=\\\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\\\" target=\\\"_blank\\\"><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">1740</span></a>\\n\",\n",
      "       \"<span style=\\\"color: #7fbfbf; text-decoration-color: #7fbfbf\\\">                    </span>         <span style=\\\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\\\">https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro</span> <span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">               </span>\\n\",\n",
      "       \"<span style=\\\"color: #7fbfbf; text-decoration-color: #7fbfbf\\\">                    </span>         <span style=\\\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\\\">:generateContent</span> <span style=\\\"color: #008000; text-decoration-color: #008000\\\">\\\"HTTP/1.1 200 OK\\\"</span>                                     <span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">               </span>\\n\",\n",
      "       \"</pre>\\n\"\n",
      "      ],\n",
      "      \"text/plain\": [\n",
      "       \"\\u001b[2;36m[07/12/25 17:22:40]\\u001b[0m\\u001b[2;36m \\u001b[0m\\u001b[34mINFO    \\u001b[0m HTTP Request: \\u001b[1;33mPOST\\u001b[0m                                                     \\u001b]8;id=471735;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\\u001b\\\\\\u001b[2m_client.py\\u001b[0m\\u001b]8;;\\u001b\\\\\\u001b[2m:\\u001b[0m\\u001b]8;id=61373;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\\u001b\\\\\\u001b[2m1740\\u001b[0m\\u001b]8;;\\u001b\\\\\\n\",\n",
      "       \"\\u001b[2;36m                    \\u001b[0m         \\u001b[4;94mhttps://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro\\u001b[0m \\u001b[2m               \\u001b[0m\\n\",\n",
      "       \"\\u001b[2;36m                    \\u001b[0m         \\u001b[4;94m:generateContent\\u001b[0m \\u001b[32m\\\"HTTP/1.1 200 OK\\\"\\u001b[0m                                     \\u001b[2m               \\u001b[0m\\n\"\n",
      "      ]\n",
      "     },\n",
      "     \"metadata\": {},\n",
      "     \"output_type\": \"display_data\"\n",
      "    },\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"Response WITHOUT Search Grounding:\\n\",\n",
      "      \"--------------------------------------------------\\n\",\n",
      "      \"Of course. Here is a summary of Tesla's (TSLA) stock price and its recent performance.\\n\",\n",
      "      \"\\n\",\n",
      "      \"As an AI, I cannot give you real-time, up-to-the-second stock data. Stock prices are highly volatile and change constantly during market hours.\\n\",\n",
      "      \"\\n\",\n",
      "      \"However, I can provide you with the most recent closing price and a summary of its recent performance.\\n\",\n",
      "      \"\\n\",\n",
      "      \"### **Tesla (TSLA) Stock Price**\\n\",\n",
      "      \"\\n\",\n",
      "      \"For the most current, live price, please check a reliable financial news source like:\\n\",\n",
      "      \"\\n\",\n",
      "      \"*   **Google Finance**\\n\",\n",
      "      \"*   **Yahoo Finance**\\n\",\n",
      "      \"*   **Bloomberg**\\n\",\n",
      "      \"*   **Reuters**\\n\",\n",
      "      \"\\n\",\n",
      "      \"As of the market close on **June 17, 2024**, the approximate stock price for Tesla (TSLA) was:\\n\",\n",
      "      \"\\n\",\n",
      "      \"*   **~$187.44**\\n\",\n",
      "      \"\\n\",\n",
      "      \"### **Recent Performance Summary**\\n\",\n",
      "      \"\\n\",\n",
      "      \"Tesla's stock has had a very eventful and volatile year in 2024. Here is a breakdown of its recent performance:\\n\",\n",
      "      \"\\n\",\n",
      "      \"*   **Last Trading Day (June 17, 2024):** The stock saw a significant gain of over **+5%**. This surge was largely attributed to reports that Tesla has received approval to test its advanced driver-assistance system (FSD) on some streets in Shanghai, a key step for its rollout in China.\\n\",\n",
      "      \"*   **Last Week:** The stock has been on an upward trend, driven by optimism following the shareholder meeting where investors approved Elon Musk's $56 billion pay package and the company's move of incorporation to Texas. This was seen as a vote of confidence in Musk's leadership.\\n\",\n",
      "      \"*   **Year-to-Date (YTD):** Despite recent gains, TSLA is still down significantly in 2024. The stock started the year around $250, and its price is still down approximately **-25%** YTD.\\n\",\n",
      "      \"*   **One-Year Performance:** Over the past 12 months, the stock has underperformed the broader market (like the S&P 500), showing a decline of roughly **-28%**.\\n\",\n",
      "      \"\\n\",\n",
      "      \"### **Key Factors Influencing Recent Performance**\\n\",\n",
      "      \"\\n\",\n",
      "      \"Several key factors are driving the volatility and performance of Tesla's stock:\\n\",\n",
      "      \"\\n\",\n",
      "      \"1.  **Increased Competition:** Competition in the EV market has intensified, particularly from Chinese automakers like BYD, which has put pressure on Tesla's global market share.\\n\",\n",
      "      \"2.  **Price Cuts and Margins:** Tesla has implemented several price cuts globally to spur demand, which has concerned investors about declining automotive gross margins.\\n\",\n",
      "      \"3.  **Delivery Numbers:** Tesla's Q1 2024 delivery numbers missed analyst expectations, marking the first year-over-year decline in deliveries in four years. All eyes are now on the upcoming Q2 delivery report.\\n\",\n",
      "      \"4.  **Future Growth Narrative:** The stock's valuation is heavily dependent on future growth. Positive news around the **Cybertruck production ramp**, the potential for a **lower-cost \\\"Model 2\\\"**, and advancements in **Full Self-Driving (FSD)** and the **Optimus robot** are critical for investor sentiment.\\n\",\n",
      "      \"5.  **CEO and Shareholder Votes:** The recent overwhelming approval of Elon Musk's compensation package has removed a major overhang of uncertainty about his future focus and leadership at the company.\\n\",\n",
      "      \"\\n\",\n",
      "      \"In summary, while Tesla has seen a strong rebound in the last week on positive news, the stock has faced significant headwinds throughout 2024 due to rising competition and concerns about slowing growth.\\n\",\n",
      "      \"\\n\",\n",
      "      \"***Disclaimer:** This information is for informational purposes only and does not constitute financial advice. You should consult with a qualified financial professional before making any investment decisions.*\\n\",\n",
      "      \"\\n\",\n",
      "      \"======================================================================\\n\",\n",
      "      \"\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"# Example 2: Compare responses with and without search grounding\\n\",\n",
      "    \"comparison_prompt = \\\"What is the current stock price of Tesla (TSLA) and its recent performance?\\\"\\n\",\n",
      "    \"\\n\",\n",
      "    \"# First, make a call without search grounding by not passing tools\\n\",\n",
      "    \"response_without_search = await llm_manager.call_llm_with_usage(\\n\",\n",
      "    \"    prompt=comparison_prompt,\\n\",\n",
      "    \"    model=\\\"gemini-2.5-pro\\\",\\n\",\n",
      "    \"    temperature=0.0\\n\",\n",
      "    \")\\n\",\n",
      "    \"\\n\",\n",
      "    \"print(\\\"Response WITHOUT Search Grounding:\\\")\\n\",\n",
      "    \"print(\\\"-\\\" * 50)\\n\",\n",
      "    \"print(response_without_search[\\\"content\\\"])\\n\",\n",
      "    \"\\n\",\n",
      "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*70 + \\\"\\\\n\\\")\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 35,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"data\": {\n",
      "      \"text/html\": [\n",
      "       \"<pre style=\\\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\\\"><span style=\\\"color: #7fbfbf; text-decoration-color: #7fbfbf\\\">[07/12/25 17:25:01] </span><span style=\\\"color: #000080; text-decoration-color: #000080\\\">INFO    </span> AFC is enabled with max remote calls: <span style=\\\"color: #008080; text-decoration-color: #008080; font-weight: bold\\\">10</span>.                               <a href=\\\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py\\\" target=\\\"_blank\\\"><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">models.py</span></a><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">:</span><a href=\\\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py#7118\\\" target=\\\"_blank\\\"><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">7118</span></a>\\n\",\n",
      "       \"</pre>\\n\"\n",
      "      ],\n",
      "      \"text/plain\": [\n",
      "       \"\\u001b[2;36m[07/12/25 17:25:01]\\u001b[0m\\u001b[2;36m \\u001b[0m\\u001b[34mINFO    \\u001b[0m AFC is enabled with max remote calls: \\u001b[1;36m10\\u001b[0m.                               \\u001b]8;id=429424;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py\\u001b\\\\\\u001b[2mmodels.py\\u001b[0m\\u001b]8;;\\u001b\\\\\\u001b[2m:\\u001b[0m\\u001b]8;id=622662;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py#7118\\u001b\\\\\\u001b[2m7118\\u001b[0m\\u001b]8;;\\u001b\\\\\\n\"\n",
      "      ]\n",
      "     },\n",
      "     \"metadata\": {},\n",
      "     \"output_type\": \"display_data\"\n",
      "    },\n",
      "    {\n",
      "     \"data\": {\n",
      "      \"text/html\": [\n",
      "       \"<pre style=\\\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\\\"><span style=\\\"color: #7fbfbf; text-decoration-color: #7fbfbf\\\">[07/12/25 17:25:17] </span><span style=\\\"color: #000080; text-decoration-color: #000080\\\">INFO    </span> HTTP Request: <span style=\\\"color: #808000; text-decoration-color: #808000; font-weight: bold\\\">POST</span>                                                     <a href=\\\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\\\" target=\\\"_blank\\\"><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">_client.py</span></a><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">:</span><a href=\\\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\\\" target=\\\"_blank\\\"><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">1740</span></a>\\n\",\n",
      "       \"<span style=\\\"color: #7fbfbf; text-decoration-color: #7fbfbf\\\">                    </span>         <span style=\\\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\\\">https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro</span> <span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">               </span>\\n\",\n",
      "       \"<span style=\\\"color: #7fbfbf; text-decoration-color: #7fbfbf\\\">                    </span>         <span style=\\\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\\\">:generateContent</span> <span style=\\\"color: #008000; text-decoration-color: #008000\\\">\\\"HTTP/1.1 200 OK\\\"</span>                                     <span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">               </span>\\n\",\n",
      "       \"</pre>\\n\"\n",
      "      ],\n",
      "      \"text/plain\": [\n",
      "       \"\\u001b[2;36m[07/12/25 17:25:17]\\u001b[0m\\u001b[2;36m \\u001b[0m\\u001b[34mINFO    \\u001b[0m HTTP Request: \\u001b[1;33mPOST\\u001b[0m                                                     \\u001b]8;id=455330;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\\u001b\\\\\\u001b[2m_client.py\\u001b[0m\\u001b]8;;\\u001b\\\\\\u001b[2m:\\u001b[0m\\u001b]8;id=543820;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\\u001b\\\\\\u001b[2m1740\\u001b[0m\\u001b]8;;\\u001b\\\\\\n\",\n",
      "       \"\\u001b[2;36m                    \\u001b[0m         \\u001b[4;94mhttps://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro\\u001b[0m \\u001b[2m               \\u001b[0m\\n\",\n",
      "       \"\\u001b[2;36m                    \\u001b[0m         \\u001b[4;94m:generateContent\\u001b[0m \\u001b[32m\\\"HTTP/1.1 200 OK\\\"\\u001b[0m                                     \\u001b[2m               \\u001b[0m\\n\"\n",
      "      ]\n",
      "     },\n",
      "     \"metadata\": {},\n",
      "     \"output_type\": \"display_data\"\n",
      "    },\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"Response WITH Search Grounding:\\n\",\n",
      "      \"--------------------------------------------------\\n\",\n",
      "      \"## Tesla Stock Shows Volatility with Recent Dip But Long-Term Gains\\n\",\n",
      "      \"\\n\",\n",
      "      \"**As of Friday, July 11, 2025, Tesla (TSLA) closed at approximately $313.51, reflecting a slight increase of 1.17% in the last 24 hours of trading.** This comes amid a period of mixed performance for the electric vehicle giant.\\n\",\n",
      "      \"\\n\",\n",
      "      \"While the daily performance shows a modest gain, a broader look reveals a recent downturn. The stock has seen a decline of 1.41% over the past week and a more significant drop of 6.25% over the last month.\\n\",\n",
      "      \"\\n\",\n",
      "      \"However, looking at the longer-term picture, Tesla's stock has demonstrated substantial growth. Over the last year, it has surged by 19.07%, and in the last 12 months, the price has risen by 26.40%. This indicates underlying strength and investor confidence in the company's future prospects. The 52-week trading range for the stock has been between a low of $182.00 and a high of $488.54.\\n\",\n",
      "      \"\\n\",\n",
      "      \"Tesla's market capitalization currently stands at a robust $1.01 trillion. The company is a key player in the consumer cyclical sector, specializing in auto manufacturing. Beyond its well-known electric vehicles like the Model S, Model 3, Model X, and Model Y, Tesla is also a significant force in the energy generation and storage sector with products like Powerwall, Powerpack, and Megapack.\\n\",\n",
      "      \"\\n\",\n",
      "      \"✓ Found 4 citation sources\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"# Now make the same call with search grounding\\n\",\n",
      "    \"response_with_search = await llm_manager.call_llm_with_citations(\\n\",\n",
      "    \"    prompt=comparison_prompt,\\n\",\n",
      "    \"    model=\\\"gemini-2.5-pro\\\",\\n\",\n",
      "    \"    temperature=0.0,\\n\",\n",
      "    \"    tools=_get_gemini_search_tools(\\\"gemini-2.5-pro\\\")\\n\",\n",
      "    \")\\n\",\n",
      "    \"\\n\",\n",
      "    \"print(\\\"Response WITH Search Grounding:\\\")\\n\",\n",
      "    \"print(\\\"-\\\" * 50)\\n\",\n",
      "    \"print(response_with_search[\\\"content\\\"])\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Show if citations were found\\n\",\n",
      "    \"if \\\"grounding_metadata\\\" in response_with_search:\\n\",\n",
      "    \"    grounding_meta = response_with_search[\\\"grounding_metadata\\\"]\\n\",\n",
      "    \"    if hasattr(grounding_meta, 'grounding_chunks') and grounding_meta.grounding_chunks:\\n\",\n",
      "    \"        print(f\\\"\\\\n✓ Found {len(grounding_meta.grounding_chunks)} citation sources\\\")\\n\",\n",
      "    \"else:\\n\",\n",
      "    \"    print(\\\"\\\\n✗ No grounding metadata found\\\")\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"### Test OpenAI Deep Research\\n\",\n",
      "    \"\\n\",\n",
      "    \"OpenAI Deep Research models (`o3-deep-research` and `o4-mini-deep-research`) are specialized models that can perform in-depth research and analysis. According to the CHANGELOG, these models automatically have access to `web_search_preview` and `code_interpreter` tools.\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": null,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"Comparing Regular vs Deep Research Models\\n\",\n",
      "      \"\\n\"\n",
      "     ]\n",
      "    },\n",
      "    {\n",
      "     \"data\": {\n",
      "      \"text/html\": [\n",
      "       \"<pre style=\\\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\\\"><span style=\\\"color: #7fbfbf; text-decoration-color: #7fbfbf\\\">[07/12/25 17:47:38] </span><span style=\\\"color: #000080; text-decoration-color: #000080\\\">INFO    </span> HTTP Request: <span style=\\\"color: #808000; text-decoration-color: #808000; font-weight: bold\\\">POST</span> <span style=\\\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\\\">https://api.openai.com/v1/responses</span> <span style=\\\"color: #008000; text-decoration-color: #008000\\\">\\\"HTTP/1.1 200 </span>  <a href=\\\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\\\" target=\\\"_blank\\\"><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">_client.py</span></a><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">:</span><a href=\\\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\\\" target=\\\"_blank\\\"><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">1740</span></a>\\n\",\n",
      "       \"<span style=\\\"color: #7fbfbf; text-decoration-color: #7fbfbf\\\">                    </span>         <span style=\\\"color: #008000; text-decoration-color: #008000\\\">OK\\\"</span>                                                                    <span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">               </span>\\n\",\n",
      "       \"</pre>\\n\"\n",
      "      ],\n",
      "      \"text/plain\": [\n",
      "       \"\\u001b[2;36m[07/12/25 17:47:38]\\u001b[0m\\u001b[2;36m \\u001b[0m\\u001b[34mINFO    \\u001b[0m HTTP Request: \\u001b[1;33mPOST\\u001b[0m \\u001b[4;94mhttps://api.openai.com/v1/responses\\u001b[0m \\u001b[32m\\\"HTTP/1.1 200 \\u001b[0m  \\u001b]8;id=242221;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\\u001b\\\\\\u001b[2m_client.py\\u001b[0m\\u001b]8;;\\u001b\\\\\\u001b[2m:\\u001b[0m\\u001b]8;id=662087;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\\u001b\\\\\\u001b[2m1740\\u001b[0m\\u001b]8;;\\u001b\\\\\\n\",\n",
      "       \"\\u001b[2;36m                    \\u001b[0m         \\u001b[32mOK\\\"\\u001b[0m                                                                    \\u001b[2m               \\u001b[0m\\n\"\n",
      "      ]\n",
      "     },\n",
      "     \"metadata\": {},\n",
      "     \"output_type\": \"display_data\"\n",
      "    },\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"Regular Model response:  {'### Implementation Plan for Real-Time Collaboration in a Code Editor\\\\n\\\\n#### 1. Choose a Synchronization Model\\\\n- **Operational Transformation (OT)**: Suitable for text-based collaboration, widely used in Google Docs.\\\\n- **CRDTs (Conflict-free Replicated Data Types)**: Better for decentralized systems, handles conflicts naturally.\\\\n\\\\n**Decision**: Use OT for simplicity and existing library support.\\\\n\\\\n#### 2. Communication Protocol\\\\n- **WebSocket**: Efficient for real-time, bidirectional communication.\\\\n- **WebRTC**: More complex, used for peer-to-peer connections.\\\\n\\\\n**Decision**: Use WebSocket for server-client architecture.\\\\n\\\\n#### 3. Conflict Resolution Strategy\\\\n- Implement server-side logic to handle conflicts using OT.\\\\n- Use a central server to maintain the document state and broadcast changes.\\\\n\\\\n#### 4. Performance at Scale\\\\n- Use horizontal scaling with load balancers.\\\\n- Implement sharding for document storage.\\\\n- Optimize WebSocket connections with libraries like `gevent` or `asyncio`.\\\\n\\\\n#### 5. Example Code\\\\n\\\\n```python\\\\n# Install necessary libraries\\\\n# pip install autobahn twisted\\\\n\\\\nfrom autobahn.twisted.websocket import WebSocketServerProtocol, WebSocketServerFactory\\\\nfrom twisted.internet import reactor\\\\nfrom twisted.python import log\\\\nimport sys\\\\n\\\\nlog.startLogging(sys.stdout)\\\\n\\\\nclass CodeEditorServerProtocol(WebSocketServerProtocol):\\\\n    def onConnect(self, request):\\\\n        print(f\\\"Client connecting: {request.peer}\\\")\\\\n\\\\n    def onOpen(self):\\\\n        print(\\\"WebSocket connection open.\\\")\\\\n\\\\n    def onMessage(self, payload, isBinary):\\\\n        if not isBinary:\\\\n            message = payload.decode(\\\\'utf8\\\\')\\\\n            print(f\\\"Text message received: {message}\\\")\\\\n            # Here, apply OT logic to update document state\\\\n            self.broadcastUpdate(message)\\\\n\\\\n    def onClose(self, wasClean, code, reason):\\\\n        print(f\\\"WebSocket connection closed: {reason}\\\")\\\\n\\\\n    def broadcastUpdate(self, message):\\\\n        # Broadcast the updated document state to all connected clients\\\\n        for client in self.factory.clients:\\\\n            client.sendMessage(message.encode(\\\\'utf8\\\\'))\\\\n\\\\nclass CodeEditorServerFactory(WebSocketServerFactory):\\\\n    def __init__(self, url):\\\\n        super().__init__(url)\\\\n        self.clients = []\\\\n\\\\n    def buildProtocol(self, addr):\\\\n        protocol = CodeEditorServerProtocol()\\\\n        protocol.factory = self\\\\n        self.clients.append(protocol)\\\\n        return protocol\\\\n\\\\nif __name__ == \\\\'__main__\\\\':\\\\n    factory = CodeEditorServerFactory(\\\"ws://localhost:9000\\\")\\\\n    reactor.listenTCP(9000, factory)\\\\n    reactor.run()\\\\n```\\\\n\\\\n#### 6. Testing and Deployment\\\\n- Test with multiple clients to ensure real-time updates.\\\\n- Deploy using a cloud provider with auto-scaling capabilities.\\\\n\\\\n#### 7. Future Enhancements\\\\n- Consider CRDTs for decentralized collaboration.\\\\n- Explore WebRTC for peer-to-peer connections if needed.\\\\n\\\\nThis plan provides a basic framework for implementing real-time collaboration in a code editor using Python.'}\\n\",\n",
      "      \"Regular Model (gpt-4o) Response Length: 2947 chars\\n\",\n",
      "      \"Token Usage: 0\\n\",\n",
      "      \"Estimated Cost: $0.0000\\n\",\n",
      "      \"\\n\",\n",
      "      \"--------------------------------------------------\\n\",\n",
      "      \"\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"# Example 3: Compare regular model vs Deep Research model on the same task\\n\",\n",
      "    \"comparison_task = \\\"\\\"\\\"\\n\",\n",
      "    \"Create a very short and concise implementation plan for adding real-time collaboration features \\n\",\n",
      "    \"to a code editor, similar to Google Docs but for code. Consider the most relevant python libraries and write example code and run it:\\n\",\n",
      "    \"- Operational Transformation vs CRDTs\\n\",\n",
      "    \"- WebSocket vs WebRTC\\n\",\n",
      "    \"- Conflict resolution strategies\\n\",\n",
      "    \"- Performance at scale\\n\",\n",
      "    \"\\\"\\\"\\\"\\n\",\n",
      "    \"\\n\",\n",
      "    \"print(\\\"Comparing Regular vs Deep Research Models\\\\n\\\")\\n\",\n",
      "    \"\\n\",\n",
      "    \"# First try with regular gpt-4o\\n\",\n",
      "    \"regular_model = \\\"gpt-4o\\\"\\n\",\n",
      "    \"try:\\n\",\n",
      "    \"    regular_response = await llm_manager.call_llm_with_usage(\\n\",\n",
      "    \"        prompt=comparison_task,\\n\",\n",
      "    \"        model=regular_model,\\n\",\n",
      "    \"        temperature=0.0\\n\",\n",
      "    \"    )\\n\",\n",
      "    \"\\n\",\n",
      "    \"    print(f\\\"Regular Model response: \\\", {regular_response[\\\"content\\\"]})\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    print(f\\\"Regular Model ({regular_model}) Response Length: {len(regular_response['content'])} chars\\\")\\n\",\n",
      "    \"    print(f\\\"Token Usage: {regular_response['usage_metadata'].total_tokens}\\\")\\n\",\n",
      "    \"    estimated_cost = calculate_cost(\\n\",\n",
      "    \"        regular_model,\\n\",\n",
      "    \"        regular_response['usage_metadata'].prompt_tokens,\\n\",\n",
      "    \"        regular_response['usage_metadata'].completion_tokens\\n\",\n",
      "    \"    )\\n\",\n",
      "    \"    print(f\\\"Estimated Cost: ${estimated_cost:.4f}\\\")\\n\",\n",
      "    \"    \\n\",\n",
      "    \"except Exception as e:\\n\",\n",
      "    \"    print(f\\\"Error with {regular_model}: {str(e)}\\\")\\n\",\n",
      "    \"\\n\",\n",
      "    \"print(\\\"\\\\n\\\" + \\\"-\\\"*50 + \\\"\\\\n\\\")\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 9,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"data\": {\n",
      "      \"text/html\": [\n",
      "       \"<pre style=\\\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\\\"><span style=\\\"color: #7fbfbf; text-decoration-color: #7fbfbf\\\">[07/12/25 17:47:53] </span><span style=\\\"color: #000080; text-decoration-color: #000080\\\">INFO    </span> Enabling Deep Research tools for model o4-mini-deep-research        <a href=\\\"file:///Users/sravanj/project_work/yellhorn-mcp/yellhorn_mcp/llm_manager.py\\\" target=\\\"_blank\\\"><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">llm_manager.py</span></a><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">:</span><a href=\\\"file:///Users/sravanj/project_work/yellhorn-mcp/yellhorn_mcp/llm_manager.py#508\\\" target=\\\"_blank\\\"><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">508</span></a>\\n\",\n",
      "       \"</pre>\\n\"\n",
      "      ],\n",
      "      \"text/plain\": [\n",
      "       \"\\u001b[2;36m[07/12/25 17:47:53]\\u001b[0m\\u001b[2;36m \\u001b[0m\\u001b[34mINFO    \\u001b[0m Enabling Deep Research tools for model o4-mini-deep-research        \\u001b]8;id=964967;file:///Users/sravanj/project_work/yellhorn-mcp/yellhorn_mcp/llm_manager.py\\u001b\\\\\\u001b[2mllm_manager.py\\u001b[0m\\u001b]8;;\\u001b\\\\\\u001b[2m:\\u001b[0m\\u001b]8;id=52942;file:///Users/sravanj/project_work/yellhorn-mcp/yellhorn_mcp/llm_manager.py#508\\u001b\\\\\\u001b[2m508\\u001b[0m\\u001b]8;;\\u001b\\\\\\n\"\n",
      "      ]\n",
      "     },\n",
      "     \"metadata\": {},\n",
      "     \"output_type\": \"display_data\"\n",
      "    },\n",
      "    {\n",
      "     \"data\": {\n",
      "      \"text/html\": [\n",
      "       \"<pre style=\\\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\\\"><span style=\\\"color: #7fbfbf; text-decoration-color: #7fbfbf\\\">[07/12/25 17:50:43] </span><span style=\\\"color: #000080; text-decoration-color: #000080\\\">INFO    </span> HTTP Request: <span style=\\\"color: #808000; text-decoration-color: #808000; font-weight: bold\\\">POST</span> <span style=\\\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\\\">https://api.openai.com/v1/responses</span> <span style=\\\"color: #008000; text-decoration-color: #008000\\\">\\\"HTTP/1.1 200 </span>  <a href=\\\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\\\" target=\\\"_blank\\\"><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">_client.py</span></a><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">:</span><a href=\\\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\\\" target=\\\"_blank\\\"><span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">1740</span></a>\\n\",\n",
      "       \"<span style=\\\"color: #7fbfbf; text-decoration-color: #7fbfbf\\\">                    </span>         <span style=\\\"color: #008000; text-decoration-color: #008000\\\">OK\\\"</span>                                                                    <span style=\\\"color: #7f7f7f; text-decoration-color: #7f7f7f\\\">               </span>\\n\",\n",
      "       \"</pre>\\n\"\n",
      "      ],\n",
      "      \"text/plain\": [\n",
      "       \"\\u001b[2;36m[07/12/25 17:50:43]\\u001b[0m\\u001b[2;36m \\u001b[0m\\u001b[34mINFO    \\u001b[0m HTTP Request: \\u001b[1;33mPOST\\u001b[0m \\u001b[4;94mhttps://api.openai.com/v1/responses\\u001b[0m \\u001b[32m\\\"HTTP/1.1 200 \\u001b[0m  \\u001b]8;id=449305;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\\u001b\\\\\\u001b[2m_client.py\\u001b[0m\\u001b]8;;\\u001b\\\\\\u001b[2m:\\u001b[0m\\u001b]8;id=618910;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\\u001b\\\\\\u001b[2m1740\\u001b[0m\\u001b]8;;\\u001b\\\\\\n\",\n",
      "       \"\\u001b[2;36m                    \\u001b[0m         \\u001b[32mOK\\\"\\u001b[0m                                                                    \\u001b[2m               \\u001b[0m\\n\"\n",
      "      ]\n",
      "     },\n",
      "     \"metadata\": {},\n",
      "     \"output_type\": \"display_data\"\n",
      "    },\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"Deep Research Model (o4-mini-deep-research) Response Length: 3340 chars\\n\",\n",
      "      \"Token Usage: 0\\n\",\n",
      "      \"Estimated Cost: $0.0000\\n\",\n",
      "      \"\\n\",\n",
      "      \"First 500 chars of Deep Research response:\\n\",\n",
      "      \"- **OT vs CRDT:** Modern editors often favor CRDTs (like Ypy/Y-CRDT) for simpler merge semantics and offline edits.  CRDTs ensure *eventual consistency* without explicit coordination ([www.codingeasypeasy.com](https://www.codingeasypeasy.com/blog/build-a-real-time-collaborative-editor-with-fastapi-and-crdts-a-comprehensive-guide#:~:text=scenario,are%20applied%20in%20different%20orders)), whereas OT requires complex transform functions and a central server.  For example, a toy CRDT string merge (...\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"# Now try with deep research model\\n\",\n",
      "    \"deep_model = \\\"o4-mini-deep-research\\\"\\n\",\n",
      "    \"try:\\n\",\n",
      "    \"    deep_response = await llm_manager.call_llm_with_usage(\\n\",\n",
      "    \"        prompt=comparison_task,\\n\",\n",
      "    \"        model=deep_model,\\n\",\n",
      "    \"        temperature=0.0\\n\",\n",
      "    \"    )\\n\",\n",
      "    \"\\n\",\n",
      "    \"    print(f\\\"Deep Model response: \\\", {deep_response[\\\"content\\\"]})\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    print(f\\\"Deep Research Model ({deep_model}) Response Length: {len(deep_response['content'])} chars\\\")\\n\",\n",
      "    \"    print(f\\\"Token Usage: {deep_response['usage_metadata'].total_tokens}\\\")\\n\",\n",
      "    \"    estimated_cost = calculate_cost(\\n\",\n",
      "    \"        deep_model,\\n\",\n",
      "    \"        deep_response['usage_metadata'].prompt_tokens,\\n\",\n",
      "    \"        deep_response['usage_metadata'].completion_tokens\\n\",\n",
      "    \"    )\\n\",\n",
      "    \"    print(f\\\"Estimated Cost: ${estimated_cost:.4f}\\\")\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    # Show a snippet of the response to see the difference\\n\",\n",
      "    \"    print(\\\"\\\\nFirst 500 chars of Deep Research response:\\\")\\n\",\n",
      "    \"    print(deep_response['content'][:500] + \\\"...\\\")\\n\",\n",
      "    \"    \\n\",\n",
      "    \"except Exception as e:\\n\",\n",
      "    \"    print(f\\\"Error with {deep_model}: {str(e)}\\\")\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 11,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"- **OT vs CRDT:** Modern editors often favor CRDTs (like Ypy/Y-CRDT) for simpler merge semantics and offline edits.  CRDTs ensure *eventual consistency* without explicit coordination ([www.codingeasypeasy.com](https://www.codingeasypeasy.com/blog/build-a-real-time-collaborative-editor-with-fastapi-and-crdts-a-comprehensive-guide#:~:text=scenario,are%20applied%20in%20different%20orders)), whereas OT requires complex transform functions and a central server.  For example, a toy CRDT string merge (using unique IDs) might look like:\\n\",\n",
      "      \"  \\n\",\n",
      "      \"  ```python\\n\",\n",
      "      \"  doc1 = []\\n\",\n",
      "      \"  doc2 = []\\n\",\n",
      "      \"  doc1.append(('1_A', 'A'))   # user1 inserts 'A'\\n\",\n",
      "      \"  doc2.append(('1_B', 'B'))   # user2 inserts 'B'\\n\",\n",
      "      \"  merged = sorted(doc1 + doc2, key=lambda x: x[0])\\n\",\n",
      "      \"  print(''.join(char for _,char in merged))  # AB\\n\",\n",
      "      \"  ```\\n\",\n",
      "      \"\\n\",\n",
      "      \"- **WebSocket vs WebRTC:** Use WebSockets for a server-based broadcast model (e.g. with Python’s `websockets` or FastAPI), as in many CRDT stacks.  (WebRTC/P2P is possible but adds browser–signaling overhead.)  For instance, a simple WebSocket echo server/client pair with `websockets`:\\n\",\n",
      "      \"  \\n\",\n",
      "      \"  ```python\\n\",\n",
      "      \"  import asyncio, websockets\\n\",\n",
      "      \"\\n\",\n",
      "      \"  async def handler(ws, path):\\n\",\n",
      "      \"      async for msg in ws:\\n\",\n",
      "      \"          await ws.send(f\\\"Echo: {msg}\\\")\\n\",\n",
      "      \"\\n\",\n",
      "      \"  async def main():\\n\",\n",
      "      \"      server = await websockets.serve(handler, \\\"localhost\\\", 8765)\\n\",\n",
      "      \"      async with websockets.connect(\\\"ws://localhost:8765\\\") as ws:\\n\",\n",
      "      \"          await ws.send(\\\"Hello\\\")\\n\",\n",
      "      \"          print(await ws.recv())  # Echo: Hello\\n\",\n",
      "      \"      server.close()\\n\",\n",
      "      \"      await server.wait_closed()\\n\",\n",
      "      \"\\n\",\n",
      "      \"  asyncio.run(main())\\n\",\n",
      "      \"  ```\\n\",\n",
      "      \"\\n\",\n",
      "      \"- **Conflict resolution strategies:** With CRDTs, conflicts are resolved automatically by merge rules (each character or operation has a globally unique ID), avoiding manual conflict logic ([www.codingeasypeasy.com](https://www.codingeasypeasy.com/blog/build-a-real-time-collaborative-editor-with-fastapi-and-crdts-a-comprehensive-guide#:~:text=scenario,are%20applied%20in%20different%20orders)).  With OT, you must implement transform functions (like inclusion/exclusion transforms) to reorder edits.  For simple data, one might fall back to *last-writer-wins* or version vectors.  In practice, libraries like Ypy handle the merge under the hood, while OT libraries (e.g. `python-ottype`) provide transform APIs.\\n\",\n",
      "      \"\\n\",\n",
      "      \"- **Performance at scale:** Use incremental updates and broadcasts rather than full-text snaps. Batch operations or diffs (state-vectors or deltas) for network efficiency.  Scale WebSocket servers with async frameworks (Uvicorn/Gunicorn or `pycrdt-websocket` like Jupyter’s) and use horizontal sharding or pub/sub (Redis) for multi-server sync.  Persist history (e.g. YDoc checkpoints) so new clients can catch up without replaying all ops.  In short, optimize by compressing CRDT updates and distributing load across nodes.  \\n\",\n",
      "      \"\\n\",\n",
      "      \"**Sources:** CRDTs guarantee eventual consistency without coordination ([www.codingeasypeasy.com](https://www.codingeasypeasy.com/blog/build-a-real-time-collaborative-editor-with-fastapi-and-crdts-a-comprehensive-guide#:~:text=scenario,are%20applied%20in%20different%20orders)); JupyterLab’s collaborative editing uses a shared Y-CRDT (YDoc) over WebSocket endpoints ([jupyterlab-realtime-collaboration.readthedocs.io](https://jupyterlab-realtime-collaboration.readthedocs.io/en/latest/developer/architecture.html#:~:text=,file%20management%20and%20kernel%20system)).\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"print(deep_response['content'])\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"### Key Differences Between Search Grounding and Deep Research\\n\",\n",
      "    \"\\n\",\n",
      "    \"1. **Search Grounding (Gemini)**:\\n\",\n",
      "    \"   - Automatically searches the web for relevant information\\n\",\n",
      "    \"   - Adds inline citations to responses\\n\",\n",
      "    \"   - Best for factual queries requiring current information\\n\",\n",
      "    \"   - No additional cost beyond regular API usage\\n\",\n",
      "    \"\\n\",\n",
      "    \"2. **Deep Research Models (OpenAI)**:\\n\",\n",
      "    \"   - Specialized models with web search and code interpreter tools\\n\",\n",
      "    \"   - Designed for complex, multi-step research tasks\\n\",\n",
      "    \"   - Can execute code and analyze results\\n\",\n",
      "    \"   - Higher cost but more comprehensive analysis\\n\",\n",
      "    \"   - May require special API access\\n\",\n",
      "    \"\\n\",\n",
      "    \"Both features enhance the LLM's ability to provide accurate, up-to-date information, but they serve different use cases and have different cost/performance tradeoffs.\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 6,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": [\n",
      "    \"title = \\\"Replace existing model with unified Model Calling Service (OpenRouter or Litellm)\\\"\\n\",\n",
      "    \"description = \\\"\\\"\\\"\\n\",\n",
      "    \"Describe how to replace and consolidate LLM model calls in yellhorn with Gemini, Open AI, etc. with OpenRouter or LiteLLM\\n\",\n",
      "    \"\\\"\\\"\\\"\\n\",\n",
      "    \"\\n\",\n",
      "    \"user_task = f\\\"Title: {title}, Description: {description}\\\"\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"## 3. Test Curate Context at scale\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 15,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": [\n",
      "    \"from examples.mock_context import run_curate_context, mock_github_command\\n\",\n",
      "    \"from yellhorn_mcp.server import curate_context\\n\",\n",
      "    \"from yellhorn_mcp.utils.git_utils import run_git_command_with_set_cwd, run_github_command_with_set_cwd\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 18,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"name\": \"stderr\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"2025-08-10 16:02:06,394 INFO AFC is enabled with max remote calls: 10.\\n\"\n",
      "     ]\n",
      "    },\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"[INFO] Starting context curation process\\n\",\n",
      "      \"[INFO] Deleted existing .yellhorncontext file before analysis\\n\",\n",
      "      \"[INFO] Getting codebase context using full mode\\n\",\n",
      "      \"[INFO] Getting codebase snapshot in mode: full\\n\",\n",
      "      \"[INFO] Found .gitignore with 86 patterns\\n\",\n",
      "      \"[INFO] Codebase context metrics: 12009 files, 110173 tokens based on (gemini-2.5-flash)\\n\",\n",
      "      \"[INFO] Extracted 11 directories from 38 filtered files\\n\",\n",
      "      \"[INFO] Directory context:\\n\",\n",
      "      \"<codebase_tree>\\n\",\n",
      "      \".\\n\",\n",
      "      \"├── .mcp.json\\n\",\n",
      "      \"├── .python-version\\n\",\n",
      "      \"├── CHANGELOG.md\\n\",\n",
      "      \"├── CLAUDE.md\\n\",\n",
      "      \"├── LLMManagerREADME.md\\n\",\n",
      "      \"├── README.md\\n\",\n",
      "      \"├── coverage_stats.txt\\n\",\n",
      "      \"├── pyproject.toml\\n\",\n",
      "      \"├── pyrightconfig.json\\n\",\n",
      "      \"│   ├── workflows/\\n\",\n",
      "      \"│   │   ├── publish.yml\\n\",\n",
      "      \"│   │   └── tests.yml\\n\",\n",
      "      \"├── docs/\\n\",\n",
      "      \"│   ├── USAGE.md\\n\",\n",
      "      \"│   └── coverage_baseline.md\\n\",\n",
      "      \"├── notebooks/\\n\",\n",
      "      \"│   ├── file_structure.ipynb\\n\",\n",
      "      \"│   ├── llm_manager.ipynb\\n\",\n",
      "      \"│   └── test_curate_context_v2.ipynb\\n\",\n",
      "      \"├── yellhorn_mcp/\\n\",\n",
      "      \"│   ├── __init__.py\\n\",\n",
      "      \"│   ├── cli.py\\n\",\n",
      "      \"│   ├── llm_manager.py\\n\",\n",
      "      \"│   ├── me...\\n\",\n",
      "      \"[INFO] Analyzing directory structure with gemini-2.5-flash\\n\",\n",
      "      \"[INFO] [DEBUG] System message: You are an expert software developer tasked with analyzing a codebase structure to identify important directories for building and executing a workplan.\\n\",\n",
      "      \"\\n\",\n",
      "      \"Your goal is to identify the most important directories that should be included for the user's task.\\n\",\n",
      "      \"\\n\",\n",
      "      \"Analyze the directories and identify the ones that:\\n\",\n",
      "      \"1. Contain core application code relevant to the user's task\\n\",\n",
      "      \"2. Likely contain important business logic\\n\",\n",
      "      \"3. Would be essential for understanding the codebase architecture\\n\",\n",
      "      \"4. Are needed to implement the requested task\\n\",\n",
      "      \"5. Contain SDKs or libraries relevant to the user's task\\n\",\n",
      "      \"\\n\",\n",
      "      \"Ignore directories that:\\n\",\n",
      "      \"1. Contain only build artifacts or generated code\\n\",\n",
      "      \"2. Store dependencies or vendor code\\n\",\n",
      "      \"3. Contain temporary or cache files\\n\",\n",
      "      \"4. Probably aren't relevant to the user's specific task\\n\",\n",
      "      \"\\n\",\n",
      "      \"User Task: Title: Replace existing model with unified Model Calling Service (OpenRouter or Litellm), Description: \\n\",\n",
      "      \"Describe how to replace and consolidate LLM model calls in yellhorn with Gemini, Open AI, etc. with OpenRouter or LiteLLM\\n\",\n",
      "      \"\\n\",\n",
      "      \"\\n\",\n",
      "      \"Return your analysis as a list of important directories, one per line, without any additional text or formatting as below:\\n\",\n",
      "      \"\\n\",\n",
      "      \"```context\\n\",\n",
      "      \"dir1/subdir1/\\n\",\n",
      "      \"dir2/\\n\",\n",
      "      \"dir3/subdir3/file3.filetype\\n\",\n",
      "      \"```\\n\",\n",
      "      \"\\n\",\n",
      "      \"Prefer to include directories, and not just file paths but include just file paths when appropriate.\\n\",\n",
      "      \"Don't include explanations for your choices, just return the list in the specified format.\\n\",\n",
      "      \"[INFO] [DEBUG] User prompt (461525 chars): <codebase_tree>\\n\",\n",
      "      \".\\n\",\n",
      "      \"├── .mcp.json\\n\",\n",
      "      \"├── .python-version\\n\",\n",
      "      \"├── CHANGELOG.md\\n\",\n",
      "      \"├── CLAUDE.md\\n\",\n",
      "      \"├── LLMManagerREADME.md\\n\",\n",
      "      \"├── README.md\\n\",\n",
      "      \"├── coverage_stats.txt\\n\",\n",
      "      \"├── pyproject.toml\\n\",\n",
      "      \"├── pyrightconfig.json\\n\",\n",
      "      \"│   ├── workflows/\\n\",\n",
      "      \"│   │   ├── publish.yml\\n\",\n",
      "      \"│   │   └── tests.yml\\n\",\n",
      "      \"├── docs/\\n\",\n",
      "      \"│   ├── USAGE.md\\n\",\n",
      "      \"│   └── coverage_baseline.md\\n\",\n",
      "      \"├── notebooks/\\n\",\n",
      "      \"│   ├── file_structure.ipynb\\n\",\n",
      "      \"│   ├── llm_manager.ipynb\\n\",\n",
      "      \"│   └── test_curate_context_v2.ipynb\\n\",\n",
      "      \"├── yellhorn_mcp/\\n\",\n",
      "      \"│   ├── __init__.py\\n\",\n",
      "      \"│   ├── cli.py\\n\",\n",
      "      \"│   ├── llm_manager.py\\n\",\n",
      "      \"│   ├── metadata_models.py\\n\",\n",
      "      \"│   ├── server.py\\n\",\n",
      "      \"│   └── token_counter.py\\n\",\n",
      "      \"│   ├── formatters/\\n\",\n",
      "      \"│   │   ├── __init__.py\\n\",\n",
      "      \"│   │   ├── codebase_snapshot.py\\n\",\n",
      "      \"│   │   ├── context_fetcher.py\\n\",\n",
      "      \"│   │   └── prompt_formatter.py\\n\",\n",
      "      \"│   ├── integrations/\\n\",\n",
      "      \"│   │   ├── gemini_integration.py\\n\",\n",
      "      \"│   │   └── github_integration.py\\n\",\n",
      "      \"│   ├── models/\\n\",\n",
      "      \"│   │   └── metadata_models.py\\n\",\n",
      "      \"│   ├── processors/\\n\",\n",
      "      \"│   │   ├── __init__.py\\n\",\n",
      "      \"│   │   ├── context_processor.py\\n\",\n",
      "      \"│   │   ├── judgement_processor.py\\n\",\n",
      "      \"│   │   └── workplan_processor.py\\n\",\n",
      "      \"│   ├── utils/\\n\",\n",
      "      \"│   │   ├── comment_utils.py\\n\",\n",
      "      \"│   │   ├── cost_tracker_utils.py\\n\",\n",
      "      \"│   │   ├── git_utils.py\\n\",\n",
      "      \"│   │   ├── lsp_utils.py\\n\",\n",
      "      \"│   │   └── search_grounding_utils.py\\n\",\n",
      "      \"</codebase_tree>\\n\",\n",
      "      \"\\n\",\n",
      "      \"<file_contents>\\n\",\n",
      "      \"\\n\",\n",
      "      \"--- File: .github/workflows/publish.yml ---\\n\",\n",
      "      \"name: Publish to PyPI\\n\",\n",
      "      \"\\n\",\n",
      "      \"on:\\n\",\n",
      "      \"  push:\\n\",\n",
      "      \"    tags:\\n\",\n",
      "      \"      - 'v*'\\n\",\n",
      "      \"\\n\",\n",
      "      \"jobs:\\n\",\n",
      "      \"  deploy:\\n\",\n",
      "      \"    runs-on: ubuntu-latest\\n\",\n",
      "      \"    steps:\\n\",\n",
      "      \"    - uses: actions/checkout@v3\\n\",\n",
      "      \"    - name: Set up Python\\n\",\n",
      "      \"      uses: actions/setup-python@v4\\n\",\n",
      "      \"      with:\\n\",\n",
      "      \"        python-version: '3.10'\\n\",\n",
      "      \"    - name: Install dependencies\\n\",\n",
      "      \"      run: |\\n\",\n",
      "      \"        python -m pip install --upgrade pip\\n\",\n",
      "      \"        pip install build twine\\n\",\n",
      "      \"    - name: Verify version matches tag\\n\",\n",
      "      \"      run: |\\n\",\n",
      "      \"        # Extract version from the tag (remove 'v' prefix)\\n\",\n",
      "      \"        TAG_VERSION=${GITHUB_REF#refs/tags/v}\\n\",\n",
      "      \"        # Extract version from pyproject.toml\\n\",\n",
      "      \"        PROJECT_VERSION=$(grep -m 1 'version = ' pyproject.toml | cut -d '\\\"' -f 2)\\n\",\n",
      "      \"        echo \\\"Tag version: $TAG_VERSION\\\"\\n\",\n",
      "      \"        echo \\\"Project version: $PROJECT_VERSION\\\"\\n\",\n",
      "      \"        if [ \\\"$TAG_VERSION\\\" != \\\"$PROJECT_VERSION\\\" ]; then\\n\",\n",
      "      \"          echo \\\"::error::Version mismatch! Tag ($TAG_VERSION) does not match pyproject.toml ($PROJECT_VERSION)\\\"\\n\",\n",
      "      \"          exit 1\\n\",\n",
      "      \"        fi\\n\",\n",
      "      \"    - name: Build package\\n\",\n",
      "      \"      run: python -m build\\n\",\n",
      "      \"    - name: Publish package\\n\",\n",
      "      \"      uses: pypa/gh-action-pypi-publish@release/v1\\n\",\n",
      "      \"      with:\\n\",\n",
      "      \"        password: ${{ secrets.PYPI_API_TOKEN }}\\n\",\n",
      "      \"\\n\",\n",
      "      \"--- File: .github/workflows/tests.yml ---\\n\",\n",
      "      \"name: Tests\\n\",\n",
      "      \"\\n\",\n",
      "      \"on:\\n\",\n",
      "      \"  push:\\n\",\n",
      "      \"    branches: [ main ]\\n\",\n",
      "      \"  pull_request:\\n\",\n",
      "      \"    branches: [ main ]\\n\",\n",
      "      \"\\n\",\n",
      "      \"jobs:\\n\",\n",
      "      \"  test:\\n\",\n",
      "      \"    runs-on: ubuntu-latest\\n\",\n",
      "      \"    strategy:\\n\",\n",
      "      \"      matrix:\\n\",\n",
      "      \"        python-version: [\\\"3.10\\\", \\\"3.11\\\"]\\n\",\n",
      "      \"\\n\",\n",
      "      \"    steps:\\n\",\n",
      "      \"    - uses: actions/checkout@v3\\n\",\n",
      "      \"    - name: Set up Python ${{ matrix.python-version }}\\n\",\n",
      "      \"      uses: actions/setup-python@v4\\n\",\n",
      "      \"      with:\\n\",\n",
      "      \"        python-version: ${{ matrix.python-version }}\\n\",\n",
      "      \"        cache: 'pip'\\n\",\n",
      "      \"    - name: Install dependencies\\n\",\n",
      "      \"      run: |\\n\",\n",
      "      \"        python -m pip install --upgrade pip\\n\",\n",
      "      \"        pip install .[dev]\\n\",\n",
      "      \"    - name: Format check with black\\n\",\n",
      "      \"      run: |\\n\",\n",
      "      \"        black --check yellhorn_mcp tests\\n\",\n",
      "      \"    - name: Import sort check with isort\\n\",\n",
      "      \"      run: |\\n\",\n",
      "      \"        isort --check yellhorn_mcp tests\\n\",\n",
      "      \"    - name: Test with pytest (coverage)\\n\",\n",
      "      \"      run: |\\n\",\n",
      "      \"        pytest --cov=yellhorn_mcp --cov=examples --cov-report=xml --cov-report=term\\n\",\n",
      "      \"    - name: Fail if coverage below threshold\\n\",\n",
      "      \"      run: |\\n\",\n",
      "      \"        python - <<'PY'\\n\",\n",
      "      \"        import xml.etree.ElementTree as ET, sys\\n\",\n",
      "      \"        tree = ET.parse('coverage.xml')\\n\",\n",
      "      \"        root = tree.getroot()\\n\",\n",
      "      \"        total = root.attrib.get('line-rate')\\n\",\n",
      "      \"        if float(total) < 0.70:\\n\",\n",
      "      \"            print(f\\\"::error::Coverage too low: {float(total)*100:.2f}% (required 70%)\\\")\\n\",\n",
      "      \"            sys.exit(1)\\n\",\n",
      "      \"        else:\\n\",\n",
      "      \"            print(f\\\"Coverage passed: {float(total)*100:.2f}% (required ≥70%)\\\")\\n\",\n",
      "      \"        PY\\n\",\n",
      "      \"\\n\",\n",
      "      \"--- File: .mcp.json ---\\n\",\n",
      "      \"{\\n\",\n",
      "      \"  \\\"mcpServers\\\": {\\n\",\n",
      "      \"    \\\"yellhorn-mcp\\\": {\\n\",\n",
      "      \"      \\\"type\\\": \\\"stdio\\\",\\n\",\n",
      "      \"      \\\"command\\\": \\\"yellhorn-mcp\\\",\\n\",\n",
      "      \"      \\\"args\\\": [],\\n\",\n",
      "      \"      \\\"env\\\": {}\\n\",\n",
      "      \"    }\\n\",\n",
      "      \"  }\\n\",\n",
      "      \"}\\n\",\n",
      "      \"\\n\",\n",
      "      \"--- File: .python-version ---\\n\",\n",
      "      \"yellhorn-mcp-3.12.2\\n\",\n",
      "      \"\\n\",\n",
      "      \"--- File: CHANGELOG.md ---\\n\",\n",
      "      \"# Changelog\\n\",\n",
      "      \"\\n\",\n",
      "      \"All notable changes to this project will be documented in this file.\\n\",\n",
      "      \"\\n\",\n",
      "      \"The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\\n\",\n",
      "      \"and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\\n\",\n",
      "      \"\\n\",\n",
      "      \"## [Unreleased]\\n\",\n",
      "      \"\\n\",\n",
      "      \"## [0.7.0] - 2025-07-18\\n\",\n",
      "      \"\\n\",\n",
      "      \"### Added\\n\",\n",
      "      \"\\n\",\n",
      "      \"- **Unified LLM Manager**: New centralized LLM management system (`LLMManager` class) that provides:\\n\",\n",
      "      \"  - Unified interface for both OpenAI and Gemini models\\n\",\n",
      "      \"  - Automatic prompt chunking when content exceeds model context limits\\n\",\n",
      "      \"  - Intelligent chunking strategies (paragraph-based and sentence-based)\\n\",\n",
      "      \"  - Response aggregation for chunked calls\\n\",\n",
      "      \"  - Configurable overlap between chunks for better context preservation\\n\",\n",
      "      \"\\n\",\n",
      "      \"- **Enhanced Retry Logic**: Robust retry mechanism with exponential backoff:\\n\",\n",
      "      \"  - Automatic retry on rate limits and transient failures\\n\",\n",
      "      \"  - Configurable retry attempts (default: 5) with exponential backoff\\n\",\n",
      "      \"  - Support for both OpenAI `RateLimitError` and Gemini `Resour...\\n\",\n",
      "      \"[INFO] Found .yellhornignore with 1 patterns\\n\",\n",
      "      \"[INFO] File categorization results out of 72 files:\\n\",\n",
      "      \"[INFO]   - 5 always ignored (images, binaries, configs, etc.)\\n\",\n",
      "      \"[INFO]   - 0 in yellhorncontext whitelist (included)\\n\",\n",
      "      \"[INFO]   - 0 in yellhorncontext blacklist (excluded)\\n\",\n",
      "      \"[INFO]   - 0 in yellhornignore whitelist (included)\\n\",\n",
      "      \"[INFO]   - 29 in yellhornignore blacklist (excluded)\\n\",\n",
      "      \"[INFO]   - 38 other files (included - no .yellhorncontext)\\n\",\n",
      "      \"[INFO] Total included: 38 files (excluded 5 always-ignored files)\\n\",\n",
      "      \"[INFO] Read contents of 38 files\\n\",\n",
      "      \"[INFO] Analysis complete, found 17 important directories: CLAUDE.md, LLMManagerREADME.md, docs/USAGE.md, notebooks/llm_manager.ipynb, notebooks/test_curate_context_v2.ipynb, ... (12 more)\\n\",\n",
      "      \"[INFO] Processing complete, identified 17 important directories\\n\",\n",
      "      \"[INFO] Successfully wrote .yellhorncontext file to /Users/sravanj/project_work/yellhorn-mcp/.yellhorncontext\\n\",\n",
      "      \"[INFO] Waiting for 0 background tasks...\\n\",\n",
      "      \"[INFO] All background tasks completed\\n\",\n",
      "      \"Path to yellhorn context\\n\",\n",
      "      \"{\\\"status\\\": \\\"\\\\u2705 Context curation completed successfully\\\", \\\"message\\\": \\\"Successfully created .yellhorncontext file at /Users/sravanj/project_work/yellhorn-mcp/.yellhorncontext with 15 files and 2 directories.\\\"}\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"# Call create_workplan with our mock context\\n\",\n",
      "    \"context_result = await (run_curate_context(\\n\",\n",
      "    \"        user_task=user_task,\\n\",\n",
      "    \"        repo_path=REPO_PATH,\\n\",\n",
      "    \"        gemini_client=gemini_client,\\n\",\n",
      "    \"        openai_client=openai_client,\\n\",\n",
      "    \"        llm_manager=llm_manager,\\n\",\n",
      "    \"        model=MODEL,\\n\",\n",
      "    \"        codebase_reasoning=\\\"full\\\",  # Use \\\"none\\\" for faster processing\\n\",\n",
      "    \"        log_callback=log_callback,\\n\",\n",
      "    \"        git_command_func=run_git_command_with_set_cwd(REPO_PATH),\\n\",\n",
      "    \"        debug=True\\n\",\n",
      "    \"    )\\n\",\n",
      "    \")\\n\",\n",
      "    \"\\n\",\n",
      "    \"print(\\\"Path to yellhorn context\\\")\\n\",\n",
      "    \"print(context_result)\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"#### Load .yellhorncontext\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 19,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"Extracted path: /Users/sravanj/project_work/yellhorn-mcp/.yellhorncontext\\n\",\n",
      "      \"Yellhorn context:\\n\",\n",
      "      \"# Yellhorn Context File - AI context optimization\\n\",\n",
      "      \"# Generated by yellhorn-mcp curate_context tool\\n\",\n",
      "      \"# Based on task: Title: Replace existing model with unified Model Calling Service (OpenRouter or \\n\",\n",
      "      \"\\n\",\n",
      "      \"# Important directories to specifically include\\n\",\n",
      "      \"CLAUDE.md\\n\",\n",
      "      \"LLMManagerREADME.md\\n\",\n",
      "      \"docs/USAGE.md\\n\",\n",
      "      \"notebooks/llm_manager.ipynb\\n\",\n",
      "      \"notebooks/test_curate_context_v2.ipynb\\n\",\n",
      "      \"pyproject.toml\\n\",\n",
      "      \"yellhorn_mcp/cli.py\\n\",\n",
      "      \"yellhorn_mcp/integrations/gemini_integration.py\\n\",\n",
      "      \"yellhorn_mcp/llm_manager.py\\n\",\n",
      "      \"yellhorn_mcp/models/metadata_models.py\\n\",\n",
      "      \"yellhorn_mcp/server.py\\n\",\n",
      "      \"yellhorn_mcp/token_counter.py\\n\",\n",
      "      \"yellhorn_mcp/utils/cost_tracker_utils.py\\n\",\n",
      "      \"yellhorn_mcp/utils/lsp_utils.py\\n\",\n",
      "      \"yellhorn_mcp/utils/search_grounding_utils.py\\n\",\n",
      "      \"yellhorn_mcp/formatters/\\n\",\n",
      "      \"yellhorn_mcp/processors/\\n\",\n",
      "      \"\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"# Parse the path from the context_result string\\n\",\n",
      "    \"import re\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Extract the file path using regex\\n\",\n",
      "    \"match = re.search(r'at\\\\s+(.+?)\\\\s+with', context_result)\\n\",\n",
      "    \"if match:\\n\",\n",
      "    \"    context_file_path = match.group(1)\\n\",\n",
      "    \"    print(f\\\"Extracted path: {context_file_path}\\\")\\n\",\n",
      "    \"else:\\n\",\n",
      "    \"    # Fallback: try to find any path-like string\\n\",\n",
      "    \"    match = re.search(r'(/[^\\\\s]+\\\\.yellhorncontext)', context_result)\\n\",\n",
      "    \"    if match:\\n\",\n",
      "    \"        context_file_path = match.group(1)\\n\",\n",
      "    \"        print(f\\\"Extracted path: {context_file_path}\\\")\\n\",\n",
      "    \"    else:\\n\",\n",
      "    \"        print(\\\"Could not extract path from result string\\\")\\n\",\n",
      "    \"        context_file_path = None\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Now you can use context_file_path to read the file\\n\",\n",
      "    \"if context_file_path:\\n\",\n",
      "    \"    with open(context_file_path, 'r') as f:\\n\",\n",
      "    \"        context = f.read()\\n\",\n",
      "    \"    print(\\\"Yellhorn context:\\\")\\n\",\n",
      "    \"    print(context)\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"## 4. Using Create Workplan\\n\",\n",
      "    \"\\n\",\n",
      "    \"Now let's demonstrate how to use the create_workplan MCP tool to generate implementation plans.\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 21,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": [\n",
      "    \"from examples.mock_context import run_create_workplan, mock_github_command\\n\",\n",
      "    \"from yellhorn_mcp.server import process_workplan_async\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": null,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"name\": \"stderr\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"2025-08-10 16:08:47,515 INFO AFC is enabled with max remote calls: 10.\\n\"\n",
      "     ]\n",
      "    },\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"[INFO] Getting codebase snapshot in mode: full\\n\",\n",
      "      \"[INFO] Found .gitignore with 86 patterns\\n\",\n",
      "      \"[INFO] Attempting to enable search grounding for model gemini-2.5-flash\\n\",\n",
      "      \"[INFO] Search grounding enabled for model gemini-2.5-flash\\n\",\n",
      "      \"[INFO] Found .yellhornignore with 1 patterns\\n\",\n",
      "      \"[INFO] Found .yellhorncontext with 17 whitelist, 0 blacklist, and 0 negation patterns\\n\",\n",
      "      \"[INFO] File categorization results out of 73 files:\\n\",\n",
      "      \"[INFO]   - 6 always ignored (images, binaries, configs, etc.)\\n\",\n",
      "      \"[INFO]   - 23 in yellhorncontext whitelist (included)\\n\",\n",
      "      \"[INFO]   - 0 in yellhorncontext blacklist (excluded)\\n\",\n",
      "      \"[INFO]   - 0 in yellhornignore whitelist (included)\\n\",\n",
      "      \"[INFO]   - 0 in yellhornignore blacklist (excluded)\\n\",\n",
      "      \"[INFO]   - 0 other files (excluded - .yellhorncontext exists)\\n\",\n",
      "      \"[INFO] Total included: 23 files (excluded 6 always-ignored files)\\n\",\n",
      "      \"[INFO] Read contents of 23 files\\n\",\n",
      "      \"[MOCK GitHub CLI] Running: gh issue edit 2760 --body # Replace existing model with unified Model Calling Service (OpenRouter or Litellm)\\n\",\n",
      "      \"\\n\",\n",
      "      \"## Refactor LLM Management with Unified Model Calling Service (LiteLLM)\\n\",\n",
      "      \"\\n\",\n",
      "      \"### Summary\\n\",\n",
      "      \"The current `yellhorn-mcp` codebase utilizes direct API calls to OpenAI and Google Gemini models, leading to fragmented LLM interaction logic and increased maintenance overhead for supporting new providers. This work plan proposes to replace and consolidate all LLM model calls through a unified model calling service, LiteLLM. This integration will simplify LLM API interactions, streamline the addition of new models and providers, and centralize advanced features like retry logic and cost tracking, thereby enhancing the system's flexibility, maintainability, and scalability. The primary subsystems affected will be `yellhorn_mcp/llm_manager.py`, `yellhorn_mcp/server.py`, and related utility and configuration files.\\n\",\n",
      "      \"\\n\",\n",
      "      \"### Technical Details\\n\",\n",
      "      \"*   **Languages & Frameworks**: Python 3.10+\\n\",\n",
      "      \"*   **External Dependencies**:\\n\",\n",
      "      \"    *   `litellm~=1.33.0`: This new dependency will be added to `pyproject.toml` under `[project] dependencies`. LiteLLM provides a unified interface to over 100 LLM APIs, including OpenAI and Gemini, simplifying API calls and offering built-in features like retry logic and cost tracking.[1](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG4Ty1kIv8-x706DKH3z0MyTlkEO8W57vOoDQOfso2nMsh9xh2BNH93Zg-7QbiF8-0CBxgxiyt3VuInjEF2v0egmfYTmiNcdHEDT6Id4ChPxXEX3wuBraaQMTjxKfKfsdgM_OdlbKP6n4hHZpfv8Z0Dq_1WI8VknpKOaGbluD5pvvgochg=), [2](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNO9XSCuQVloTCtVZqw58jsi_tTJFmx2aAJdBRQlPbj69lF2ZDEeGXdC4DOfLIXQ9wnw8vmYjsUreGa5rXUZogzcTaVIPrrHdIc-MoZp93nZIdnsLsynw=), [3](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHlyoBNK5-LKhYp37fYMMonXDdEhq0L2HOStpzDqK6Q8k29EGQBFuCvilzsLfcHyYt9xJdTW8jE2Wz8PAE91IOy5TT3WTDN4NOE9z84iS97Ix0-gVv17hI4sYRhfiK3Kh5Ak2KBhLJUVGxRzPqdJMp3sCjRj1nPP8ZAIOJasfsJrkzXU8Rh5ztGaAkie4Iz)\\n\",\n",
      "      \"*   **Dependency Management & Pinning Strategy**: `poetry` (as indicated by `pyproject.toml`). The new `litellm` dependency will be pinned using the `~=` operator for compatible releases.\\n\",\n",
      "      \"*   **Build, Lint, Formatting, Type-checking Commands**:\\n\",\n",
      "      \"    *   `python -m black yellhorn_mcp tests`\\n\",\n",
      "      \"    *   `python -m isort yellhorn_mcp tests`\\n\",\n",
      "      \"    *   `python -m pytest`\\n\",\n",
      "      \"    *   Type-checking will be performed via `pyright` (configured in `pyrightconfig.json`).\\n\",\n",
      "      \"*   **Logging & Observability**: Existing `logging` module will be leveraged. Ensure LiteLLM's internal logging integrates seamlessly or is configured to use the existing logging setup.\\n\",\n",
      "      \"*   **Analytics/KPIs**: Existing cost tracking and usage metadata (`yellhorn_mcp/utils/cost_tracker_utils.py`, `yellhorn_mcp/models/metadata_models.py`) will be adapted to consume LiteLLM's unified usage reports.\\n\",\n",
      "      \"*   **Testing Frameworks & Helpers**: `pytest`, `pytest-asyncio`, `httpx`, `pytest-cov` will continue to be used. Existing mock testing infrastructure (`examples/mock_context.py`) will be adapted to mock LiteLLM calls.\\n\",\n",
      "      \"\\n\",\n",
      "      \"### Architecture\\n\",\n",
      "      \"*   **Existing Components Leveraged**:\\n\",\n",
      "      \"    *   `yellhorn_mcp/llm_manager.py`: The core `LLMManager` class will be refactored to use LiteLLM as its underlying LLM interaction layer. Its existing logic for chunking (`ChunkingStrategy`), token counting (`TokenCounter`), and usage metadata aggregation (`UsageMetadata`) will be adapted to work with LiteLLM's outputs.\\n\",\n",
      "      \"    *   `yellhorn_mcp/token_counter.py`: Will continue to provide model-specific token limits and counting, potentially integrating with LiteLLM's internal token counting utilities if beneficial.\\n\",\n",
      "      \"    *   `yellhorn_mcp/models/metadata_models.py`: `UsageMetrics` and `CompletionMetadata` will continue to track LLM usage, adapted for LiteLLM's reporting format.\\n\",\n",
      "      \"    *   `yellhorn_mcp/utils/cost_tracker_utils.py`: `calculate_cost` and `format_metrics_section` will be updated to map LiteLLM's model identifiers to existing pricing.\\n\",\n",
      "      \"    *   `yellhorn_mcp/utils/search_grounding_utils.py`: The logic for configuring Google Search tools and adding citations will be adapted to pass through LiteLLM's API.\\n\",\n",
      "      \"    *   `yellhorn_mcp/processors/`: All processors (`context_processor.py`, `judgement_processor.py`, `workplan_processor.py`) will continue to use `LLMManager` as their LLM interface.\\n\",\n",
      "      \"*   **New Components Introduced**: No new top-level components will be introduced. The change is primarily an internal refactoring of the `LLMManager` to use LiteLLM.\\n\",\n",
      "      \"*   **Control-flow & Data-flow Diagram (ASCII)**:\\n\",\n",
      "      \"    ```\\n\",\n",
      "      \"    +-----------------+       +-------------------+       +-------------------+\\n\",\n",
      "      \"    | MCP Tools (CLI, |       | yellhorn_mcp/     |       | yellhorn_mcp/     |\\n\",\n",
      "      \"    | VSCode, Claude) |------>| server.py         |------>| llm_manager.py    |\\n\",\n",
      "      \"    +-----------------+       | (Tool Dispatcher) |       | (Unified LLM API) |\\n\",\n",
      "      \"                               +-------------------+       +-------------------+\\n\",\n",
      "      \"                                         ^                         |\\n\",\n",
      "      \"                                         |                         |\\n\",\n",
      "      \"                                         |                         |\\n\",\n",
      "      \"                                         |                         |\\n\",\n",
      "      \"                                         |                         v\\n\",\n",
      "      \"                                         |                   +-----------------+\\n\",\n",
      "      \"                                         |                   | LiteLLM Library |\\n\",\n",
      "      \"                                         |                   | (litellm.py)    |\\n\",\n",
      "      \"                                         |                   +-----------------+\\n\",\n",
      "      \"                                         |                         |\\n\",\n",
      "      \"                                         |                         |\\n\",\n",
      "      \"                                         |                         |\\n\",\n",
      "      \"                                         |                         v\\n\",\n",
      "      \"                                         |                   +-----------------+\\n\",\n",
      "      \"                                         |                   | OpenAI API      |\\n\",\n",
      "      \"                                         |                   | Gemini API      |\\n\",\n",
      "      \"                                         |                   | Other LLM APIs  |\\n\",\n",
      "      \"                                         |                   +-----------------+\\n\",\n",
      "      \"                                         |\\n\",\n",
      "      \"                                         +-------------------------------------+\\n\",\n",
      "      \"                                         (Usage Metadata, Responses, Errors)\\n\",\n",
      "      \"    ```\\n\",\n",
      "      \"*   **State-management, Retry/Fallback, and Error-handling Patterns**: LiteLLM offers robust built-in retry and fallback mechanisms.[2](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNO9XSCuQVloTCtVZqw58jsi_tTJFmx2aAJdBRQlPbj69lF2ZDEeGXdC4DOfLIXQ9wnw8vmYjsUreGa5rXUZogzcTaVIPrrHdIc-MoZp93nZIdnsLsynw=) The existing `api_retry` decorator in `llm_manager.py` will be removed, and `LLMManager` will rely on LiteLLM's internal retry logic. Error handling will be adapted to catch LiteLLM-specific exceptions and translate them into `YellhornMCPError` where appropriate.\\n\",\n",
      "      \"\\n\",\n",
      "      \"### Completion Criteria & Metrics\\n\",\n",
      "      \"*   **Engineering Metrics**:\\n\",\n",
      "      \"    *   All `pytest` unit and integration tests pass with 100% success rate.\\n\",\n",
      "      \"    *   Test coverage for `yellhorn_mcp` remains at or above 70% line coverage.\\n\",\n",
      "      \"    *   `black` and `isort` formatting checks pass.\\n\",\n",
      "      \"    *   `pyright` type-checking runs clean with no errors.\\n\",\n",
      "      \"    *   `LLMManager` successfully makes calls to both OpenAI and Gemini models via LiteLLM.\\n\",\n",
      "      \"    *   Cost tracking and usage metadata reported in GitHub comments remain accurate and consistent with previous versions.\\n\",\n",
      "      \"*   **Business Metrics**:\\n\",\n",
      "      \"    *   Reduced time-to-integrate new LLM providers by leveraging LiteLLM's unified interface.\\n\",\n",
      "      \"    *   Improved system reliability due to LiteLLM's robust retry and fallback mechanisms.\\n\",\n",
      "      \"*   **Code-state Definition of Done**:\\n\",\n",
      "      \"    *   All CI jobs (linting, formatting, testing) are green.\\n\",\n",
      "      \"    *   `pyproject.toml` is updated with the new `litellm` dependency.\\n\",\n",
      "      \"    *   `LLMManagerREADME.md` and `docs/USAGE.md` are updated to reflect the LiteLLM integration.\\n\",\n",
      "      \"\\n\",\n",
      "      \"### References\\n\",\n",
      "      \"*   `yellhorn_mcp/llm_manager.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/server.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/cli.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/token_counter.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/integrations/gemini_integration.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/utils/cost_tracker_utils.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/utils/search_grounding_utils.py`\\n\",\n",
      "      \"*   `pyproject.toml`\\n\",\n",
      "      \"*   LiteLLM GitHub Repository: `https://github.com/BerriAI/litellm`[4](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHJqrTQMqVpo30aziDctGWaQBTciLRLQ7cxR_-2L18AA-WDtAeW9Tc15Iulaz3EocAd6-xx6fScsj_J_o8JGGnbw4Umaj6-fP6WKin0aBn91jDU9DRzRDn6aHscPA==)\\n\",\n",
      "      \"*   LiteLLM Documentation: `https://docs.litellm.ai/`[5](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE7TAfZPbbVvO9V1PNxONobzzviteU9UhFpNZDcEBmmds4AFNHD3ve1SKZzeXa0X9_gmmsP5LkeBeFliaiF_qbJbV6oETAKOuLHrSY5jSvChIQC6H3FfQG08-jmKCzRQv0=), [2](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNO9XSCuQVloTCtVZqw58jsi_tTJFmx2aAJdBRQlPbj69lF2ZDEeGXdC4DOfLIXQ9wnw8vmYjsUreGa5rXUZogzcTaVIPrrHdIc-MoZp93nZIdnsLsynw=), [6](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG7LN0z9TdU0oWVk8IIjcNM2V5AwitnYaZ-qI9rf39ZkfkJY2RKVT_n-_7ZGXVmSnpQUZhy7xYafWgPWoH-uLFo1hTUD_FR4gh70OMHcD0r3Xnyj2xp1C1MS7iM3jgMnBsDCjkduQDfz5uk_xOGbD0ptegACqLFiQsWs1bcvY6MW1DJ6yc52PuDzj2LAvvO8tin), [7](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGqLP_Q1VgUi5mp07zBgowNwY4Jc-LmtXBEAwG-NmrcwdEvaEjV4dMKVwWxOPCN4gyhmyl-Ccj5RiSuhOzyq-f-Ym9flHxUOiNu344Rxvqvk8wJ)\\n\",\n",
      "      \"*   LiteLLM PyPI: `https://pypi.org/project/litellm/`\\n\",\n",
      "      \"\\n\",\n",
      "      \"### Implementation Steps\\n\",\n",
      "      \"### - [ ] Step 1: Add LiteLLM Dependency and Remove Direct API Clients\\n\",\n",
      "      \"**Description**: Introduce `litellm` as a core dependency in `pyproject.toml`. Remove direct `google-genai` and `openai` client dependencies, as LiteLLM will abstract these. Remove `tenacity` as LiteLLM provides its own retry logic.\\n\",\n",
      "      \"**Files**:\\n\",\n",
      "      \"*   `pyproject.toml`\\n\",\n",
      "      \"*   `yellhorn_mcp/server.py`\\n\",\n",
      "      \"**Reference(s)**:\\n\",\n",
      "      \"*   `pyproject.toml`\\n\",\n",
      "      \"*   `yellhorn_mcp/server.py`\\n\",\n",
      "      \"*   LiteLLM PyPI: `https://pypi.org/project/litellm/`\\n\",\n",
      "      \"**Test(s)**:\\n\",\n",
      "      \"*   Run `poetry install` to verify dependency resolution.\\n\",\n",
      "      \"*   Ensure `yellhorn-mcp` CLI still starts without errors: `python -m yellhorn_mcp.cli --help`.\\n\",\n",
      "      \"\\n\",\n",
      "      \"### - [ ] Step 2: Refactor `LLMManager` Initialization and Core Call Logic\\n\",\n",
      "      \"**Description**: Modify `LLMManager.__init__` to accept a `litellm` client (or initialize it internally) instead of separate OpenAI and Gemini clients. Update `_single_call` to use `litellm.acompletion()` for all LLM interactions. Remove the `api_retry` decorator from `_call_openai` and `_call_gemini` (or remove these methods entirely if `_single_call` becomes the sole entry point).\\n\",\n",
      "      \"**Files**:\\n\",\n",
      "      \"*   `yellhorn_mcp/llm_manager.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/server.py`\\n\",\n",
      "      \"**Reference(s)**:\\n\",\n",
      "      \"*   `yellhorn_mcp/llm_manager.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/server.py`\\n\",\n",
      "      \"*   LiteLLM Documentation: `https://docs.litellm.ai/docs/completion`\\n\",\n",
      "      \"**Test(s)**:\\n\",\n",
      "      \"*   Unit tests for `LLMManager`'s `_single_call` method, ensuring it correctly routes to LiteLLM.\\n\",\n",
      "      \"*   Mock LiteLLM's `acompletion` to verify call parameters.\\n\",\n",
      "      \"\\n\",\n",
      "      \"### - [ ] Step 3: Adapt `UsageMetadata` and `TokenCounter` for LiteLLM\\n\",\n",
      "      \"**Description**: Update `UsageMetadata` to correctly parse LiteLLM's unified usage format. Investigate if `TokenCounter` can leverage LiteLLM's `token_counter` utility for more accurate or consistent token estimation across models.\\n\",\n",
      "      \"**Files**:\\n\",\n",
      "      \"*   `yellhorn_mcp/llm_manager.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/token_counter.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/models/metadata_models.py`\\n\",\n",
      "      \"**Reference(s)**:\\n\",\n",
      "      \"*   `yellhorn_mcp/llm_manager.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/token_counter.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/models/metadata_models.py`\\n\",\n",
      "      \"*   LiteLLM API: `https://docs.litellm.ai/docs/token_counter`[8](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGO7UZuLkO5CgKE-QOmpelvPzFXNHEcEyR-rll1KfKNLtvPxjmI1sxV0MFp3BiMU0CQxaoQb1h7uJBwtC2MOBdhw2fNZtrmaBRIrDy3pp3Xrb1448DCO1X_TBcT7TY=)\\n\",\n",
      "      \"**Test(s)**:\\n\",\n",
      "      \"*   Unit tests for `UsageMetadata` to ensure correct parsing of LiteLLM's usage data.\\n\",\n",
      "      \"*   Unit tests for `TokenCounter` to verify token counts using LiteLLM's method (if adopted) or ensuring compatibility.\\n\",\n",
      "      \"\\n\",\n",
      "      \"### - [ ] Step 4: Integrate Search Grounding and Deep Research Model Logic with LiteLLM\\n\",\n",
      "      \"**Description**: Adapt the logic for Google Search Grounding (`_get_gemini_search_tools`, `add_citations_from_metadata`) and Deep Research model tool activation (`_is_deep_research_model` in `LLMManager`) to work seamlessly with LiteLLM's unified API. LiteLLM supports passing `tools` parameters.\\n\",\n",
      "      \"**Files**:\\n\",\n",
      "      \"*   `yellhorn_mcp/llm_manager.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/utils/search_grounding_utils.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/integrations/gemini_integration.py` (will be refactored or removed)\\n\",\n",
      "      \"**Reference(s)**:\\n\",\n",
      "      \"*   `yellhorn_mcp/llm_manager.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/utils/search_grounding_utils.py`\\n\",\n",
      "      \"*   LiteLLM API: `https://docs.litellm.ai/docs/completion` (for `tools` parameter)\\n\",\n",
      "      \"*   LiteLLM API: `https://docs.litellm.ai/docs/providers` (for model support)[5](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE7TAfZPbbVvO9V1PNxONobzzviteU9UhFpNZDcEBmmds4AFNHD3ve1SKZzeXa0X9_gmmsP5LkeBeFliaiF_qbJbV6oETAKOuLHrSY5jSvChIQC6H3FfQG08-jmKCzRQv0=)\\n\",\n",
      "      \"**Test(s)**:\\n\",\n",
      "      \"*   Integration tests for `call_llm_with_citations` to ensure search grounding works via LiteLLM.\\n\",\n",
      "      \"*   Unit tests for `LLMManager` to verify deep research model tool activation.\\n\",\n",
      "      \"\\n\",\n",
      "      \"### - [ ] Step 5: Update `server.py` and `cli.py` for LiteLLM Integration\\n\",\n",
      "      \"**Description**: Modify `app_lifespan` in `server.py` to initialize LiteLLM instead of separate OpenAI and Gemini clients. Adjust `cli.py` to handle API keys and model selection in a LiteLLM-compatible manner, potentially simplifying environment variable requirements. Remove `yellhorn_mcp/integrations/gemini_integration.py` as its functionality will be absorbed by `llm_manager.py` and LiteLLM.\\n\",\n",
      "      \"**Files**:\\n\",\n",
      "      \"*   `yellhorn_mcp/server.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/cli.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/integrations/gemini_integration.py` (delete)\\n\",\n",
      "      \"**Reference(s)**:\\n\",\n",
      "      \"*   `yellhorn_mcp/server.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/cli.py`\\n\",\n",
      "      \"*   LiteLLM Getting Started: `https://docs.litellm.ai/docs/getting_started`[2](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNO9XSCuQVloTCtVZqw58jsi_tTJFmx2aAJdBRQlPbj69lF2ZDEeGXdC4DOfLIXQ9wnw8vmYjsUreGa5rXUZogzcTaVIPrrHdIc-MoZp93nZIdnsLsynw=)\\n\",\n",
      "      \"**Test(s)**:\\n\",\n",
      "      \"*   End-to-end integration tests for `create_workplan`, `judge_workplan`, and `curate_context` via the MCP server to ensure full functionality.\\n\",\n",
      "      \"*   Verify CLI commands still function correctly.\\n\",\n",
      "      \"\\n\",\n",
      "      \"### - [ ] Step 6: Update Cost Tracking and Documentation\\n\",\n",
      "      \"**Description**: Review and update `yellhorn_mcp/utils/cost_tracker_utils.py` to ensure accurate cost calculation with LiteLLM's model naming conventions and potentially leverage LiteLLM's cost tracking features if they offer more granularity. Update `LLMManagerREADME.md` and `docs/USAGE.md` to reflect the new LiteLLM-based architecture and usage.\\n\",\n",
      "      \"**Files**:\\n\",\n",
      "      \"*   `yellhorn_mcp/utils/cost_tracker_utils.py`\\n\",\n",
      "      \"*   `LLMManagerREADME.md`\\n\",\n",
      "      \"*   `docs/USAGE.md`\\n\",\n",
      "      \"**Reference(s)**:\\n\",\n",
      "      \"*   `yellhorn_mcp/utils/cost_tracker_utils.py`\\n\",\n",
      "      \"*   `LLMManagerREADME.md`\\n\",\n",
      "      \"*   `docs/USAGE.md`\\n\",\n",
      "      \"*   LiteLLM Model Cost Map: `https://docs.litellm.ai/docs/proxy/model_cost_map`[8](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGO7UZuLkO5CgKE-QOmpelvPzFXNHEcEyR-rll1KfKNLtvPxjmI1sxV0MFp3BiMU0CQxaoQb1h7uJBwtC2MOBdhw2fNZtrmaBRIrDy3pp3Xrb1448DCO1X_TBcT7TY=)\\n\",\n",
      "      \"**Test(s)**:\\n\",\n",
      "      \"*   Verify cost calculations in generated GitHub comments are correct.\\n\",\n",
      "      \"*   Review documentation for clarity and accuracy.\\n\",\n",
      "      \"\\n\",\n",
      "      \"### Global Test Strategy\\n\",\n",
      "      \"*   **Unit Tests**: All modified functions and classes will have comprehensive unit tests using `pytest` and `pytest-asyncio`. Mocking will be used for external API calls (LiteLLM).\\n\",\n",
      "      \"*   **Integration Tests**: The `examples/mock_context.py` will be updated and used to run integration tests for `create_workplan`, `judge_workplan`, and `curate_context` tools, simulating GitHub interactions and verifying LLM calls through the LiteLLM integration.\\n\",\n",
      "      \"*   **End-to-End Tests**: Manual verification of the `yellhorn-mcp` CLI and its interaction with GitHub issues.\\n\",\n",
      "      \"*   **Test Coverage**: Maintain minimum 70% test coverage for all new and modified code, enforced by `pytest-cov`.\\n\",\n",
      "      \"*   **Local Execution**: Tests can be run locally using `poetry run pytest`.\\n\",\n",
      "      \"*   **Environment Variables/Secrets**: API keys for LiteLLM (and underlying providers if needed) will be managed via environment variables (`LITELLM_API_KEY` or provider-specific keys like `OPENAI_API_KEY`, `GEMINI_API_KEY` as LiteLLM supports them).\\n\",\n",
      "      \"*   **Async Helpers/Fixtures**: Existing `pytest-asyncio` fixtures will be leveraged.\\n\",\n",
      "      \"\\n\",\n",
      "      \"### Files to Modify / New Files to Create\\n\",\n",
      "      \"*   **Files to Modify**:\\n\",\n",
      "      \"    *   `pyproject.toml`: Add `litellm` dependency, remove `google-genai`, `openai`, `tenacity`, `google-api-core`.\\n\",\n",
      "      \"    *   `yellhorn_mcp/llm_manager.py`: Refactor `LLMManager` to use LiteLLM, update `UsageMetadata`, remove `api_retry` decorator.\\n\",\n",
      "      \"    *   `yellhorn_mcp/server.py`: Update `app_lifespan` to initialize LiteLLM, remove direct client initializations.\\n\",\n",
      "      \"    *   `yellhorn_mcp/cli.py`: Adjust API key validation and model selection logic.\\n\",\n",
      "      \"    *   `yellhorn_mcp/token_counter.py`: Potentially integrate LiteLLM's token counting.\\n\",\n",
      "      \"    *   `yellhorn_mcp/utils/cost_tracker_utils.py`: Update `MODEL_PRICING` and `calculate_cost` for LiteLLM models.\\n\",\n",
      "      \"    *   `yellhorn_mcp/utils/search_grounding_utils.py`: Ensure compatibility with LiteLLM's `tools` parameter.\\n\",\n",
      "      \"    *   `LLMManagerREADME.md`: Update LLM Manager usage and architecture.\\n\",\n",
      "      \"    *   `docs/USAGE.md`: Update installation, configuration, and tool usage sections.\\n\",\n",
      "      \"*   **New Files to Create**:\\n\",\n",
      "      \"    *   None, aiming for refactoring existing files.\\n\",\n",
      "      \"[MOCK GitHub CLI] In directory: /Users/sravanj/project_work/yellhorn-mcp\\n\",\n",
      "      \"[MOCK GitHub CLI] Updated issue 2760\\n\",\n",
      "      \"[INFO] Successfully updated GitHub issue #2760 with generated workplan and metrics\\n\",\n",
      "      \"[MOCK GitHub CLI] Running: gh issue comment 2760 --body ## ✅ Workplan generated successfully\\n\",\n",
      "      \"\\n\",\n",
      "      \"### Generation Details\\n\",\n",
      "      \"**Time**: 46.1 seconds  \\n\",\n",
      "      \"**Completed**: 2025-08-10 23:09:33 UTC  \\n\",\n",
      "      \"**Model Used**: `gemini-2.5-flash`  \\n\",\n",
      "      \"\\n\",\n",
      "      \"### Token Usage\\n\",\n",
      "      \"**Input Tokens**: 113,934  \\n\",\n",
      "      \"**Output Tokens**: 4,109  \\n\",\n",
      "      \"**Total Tokens**: 124,466  \\n\",\n",
      "      \"**Estimated Cost**: $0.0445  \\n\",\n",
      "      \"\\n\",\n",
      "      \"**Search Results Used**: 1  \\n\",\n",
      "      \"**Context Size**: 398,011 characters  \\n\",\n",
      "      \"[MOCK GitHub CLI] In directory: /Users/sravanj/project_work/yellhorn-mcp\\n\",\n",
      "      \"[MOCK GitHub CLI] Added comment to issue 2760\\n\",\n",
      "      \"[INFO] Waiting for 0 background tasks...\\n\",\n",
      "      \"[INFO] All background tasks completed\\n\",\n",
      "      \"Workplan Created:\\n\",\n",
      "      \"--------------------------------------------------\\n\",\n",
      "      \"Issue URL: https://github.com/mock/repo/issues/2760\\n\",\n",
      "      \"Issue Number: 2760\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"# Call process_workplan_async with our mock context\\n\",\n",
      "    \"workplan_result = await (run_create_workplan(\\n\",\n",
      "    \"        title=title,\\n\",\n",
      "    \"        detailed_description=description,\\n\",\n",
      "    \"        repo_path=REPO_PATH,\\n\",\n",
      "    \"        gemini_client=gemini_client,\\n\",\n",
      "    \"        openai_client=openai_client,\\n\",\n",
      "    \"        llm_manager=llm_manager,\\n\",\n",
      "    \"        model=MODEL,\\n\",\n",
      "    \"        codebase_reasoning=\\\"full\\\",\\n\",\n",
      "    \"        log_callback=log_callback,\\n\",\n",
      "    \"        github_command_func=mock_github_command,\\n\",\n",
      "    \"        git_command_func=run_git_command_with_set_cwd(REPO_PATH),\\n\",\n",
      "    \"        background_task_timeout=180\\n\",\n",
      "    \"    )\\n\",\n",
      "    \")\\n\",\n",
      "    \"\\n\",\n",
      "    \"print(\\\"Workplan Created:\\\")\\n\",\n",
      "    \"print(\\\"-\\\" * 50)\\n\",\n",
      "    \"print(f\\\"Issue URL: {workplan_result['issue_url']}\\\")\\n\",\n",
      "    \"print(f\\\"Issue Number: {workplan_result['issue_number']}\\\")\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"## 5. Using Review Workplan\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 26,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": [\n",
      "    \"from examples.mock_context import run_revise_workplan, mock_github_command\\n\",\n",
      "    \"from yellhorn_mcp.server import process_revision_async\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 24,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": [\n",
      "    \"original_workplan = \\\"\\\"\\\"## Refactor LLM Management with Unified Model Calling Service (LiteLLM)\\n\",\n",
      "    \"\\n\",\n",
      "    \"### Summary\\n\",\n",
      "    \"The current `yellhorn-mcp` codebase utilizes direct API calls to OpenAI and Google Gemini models, leading to fragmented LLM interaction logic and increased maintenance overhead for supporting new providers. This work plan proposes to replace and consolidate all LLM model calls through a unified model calling service, LiteLLM. This integration will simplify LLM API interactions, streamline the addition of new models and providers, and centralize advanced features like retry logic and cost tracking, thereby enhancing the system's flexibility, maintainability, and scalability. The primary subsystems affected will be `yellhorn_mcp/llm_manager.py`, `yellhorn_mcp/server.py`, and related utility and configuration files.\\n\",\n",
      "    \"\\n\",\n",
      "    \"### Technical Details\\n\",\n",
      "    \"*   **Languages & Frameworks**: Python 3.10+\\n\",\n",
      "    \"*   **External Dependencies**:\\n\",\n",
      "    \"    *   `litellm~=1.33.0`: This new dependency will be added to `pyproject.toml` under `[project] dependencies`. LiteLLM provides a unified interface to over 100 LLM APIs, including OpenAI and Gemini, simplifying API calls and offering built-in features like retry logic and cost tracking.[1](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG4Ty1kIv8-x706DKH3z0MyTlkEO8W57vOoDQOfso2nMsh9xh2BNH93Zg-7QbiF8-0CBxgxiyt3VuInjEF2v0egmfYTmiNcdHEDT6Id4ChPxXEX3wuBraaQMTjxKfKfsdgM_OdlbKP6n4hHZpfv8Z0Dq_1WI8VknpKOaGbluD5pvvgochg=), [2](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNO9XSCuQVloTCtVZqw58jsi_tTJFmx2aAJdBRQlPbj69lF2ZDEeGXdC4DOfLIXQ9wnw8vmYjsUreGa5rXUZogzcTaVIPrrHdIc-MoZp93nZIdnsLsynw=), [3](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHlyoBNK5-LKhYp37fYMMonXDdEhq0L2HOStpzDqK6Q8k29EGQBFuCvilzsLfcHyYt9xJdTW8jE2Wz8PAE91IOy5TT3WTDN4NOE9z84iS97Ix0-gVv17hI4sYRhfiK3Kh5Ak2KBhLJUVGxRzPqdJMp3sCjRj1nPP8ZAIOJasfsJrkzXU8Rh5ztGaAkie4Iz)\\n\",\n",
      "    \"*   **Dependency Management & Pinning Strategy**: `poetry` (as indicated by `pyproject.toml`). The new `litellm` dependency will be pinned using the `~=` operator for compatible releases.\\n\",\n",
      "    \"*   **Build, Lint, Formatting, Type-checking Commands**:\\n\",\n",
      "    \"    *   `python -m black yellhorn_mcp tests`\\n\",\n",
      "    \"    *   `python -m isort yellhorn_mcp tests`\\n\",\n",
      "    \"    *   `python -m pytest`\\n\",\n",
      "    \"    *   Type-checking will be performed via `pyright` (configured in `pyrightconfig.json`).\\n\",\n",
      "    \"*   **Logging & Observability**: Existing `logging` module will be leveraged. Ensure LiteLLM's internal logging integrates seamlessly or is configured to use the existing logging setup.\\n\",\n",
      "    \"*   **Analytics/KPIs**: Existing cost tracking and usage metadata (`yellhorn_mcp/utils/cost_tracker_utils.py`, `yellhorn_mcp/models/metadata_models.py`) will be adapted to consume LiteLLM's unified usage reports.\\n\",\n",
      "    \"*   **Testing Frameworks & Helpers**: `pytest`, `pytest-asyncio`, `httpx`, `pytest-cov` will continue to be used. Existing mock testing infrastructure (`examples/mock_context.py`) will be adapted to mock LiteLLM calls.\\n\",\n",
      "    \"\\n\",\n",
      "    \"### Architecture\\n\",\n",
      "    \"*   **Existing Components Leveraged**:\\n\",\n",
      "    \"    *   `yellhorn_mcp/llm_manager.py`: The core `LLMManager` class will be refactored to use LiteLLM as its underlying LLM interaction layer. Its existing logic for chunking (`ChunkingStrategy`), token counting (`TokenCounter`), and usage metadata aggregation (`UsageMetadata`) will be adapted to work with LiteLLM's outputs.\\n\",\n",
      "    \"    *   `yellhorn_mcp/token_counter.py`: Will continue to provide model-specific token limits and counting, potentially integrating with LiteLLM's internal token counting utilities if beneficial.\\n\",\n",
      "    \"    *   `yellhorn_mcp/models/metadata_models.py`: `UsageMetrics` and `CompletionMetadata` will continue to track LLM usage, adapted for LiteLLM's reporting format.\\n\",\n",
      "    \"    *   `yellhorn_mcp/utils/cost_tracker_utils.py`: `calculate_cost` and `format_metrics_section` will be updated to map LiteLLM's model identifiers to existing pricing.\\n\",\n",
      "    \"    *   `yellhorn_mcp/utils/search_grounding_utils.py`: The logic for configuring Google Search tools and adding citations will be adapted to pass through LiteLLM's API.\\n\",\n",
      "    \"    *   `yellhorn_mcp/processors/`: All processors (`context_processor.py`, `judgement_processor.py`, `workplan_processor.py`) will continue to use `LLMManager` as their LLM interface.\\n\",\n",
      "    \"*   **New Components Introduced**: No new top-level components will be introduced. The change is primarily an internal refactoring of the `LLMManager` to use LiteLLM.\\n\",\n",
      "    \"*   **Control-flow & Data-flow Diagram (ASCII)**:\\n\",\n",
      "    \"    ```\\n\",\n",
      "    \"    +-----------------+       +-------------------+       +-------------------+\\n\",\n",
      "    \"    | MCP Tools (CLI, |       | yellhorn_mcp/     |       | yellhorn_mcp/     |\\n\",\n",
      "    \"    | VSCode, Claude) |------>| server.py         |------>| llm_manager.py    |\\n\",\n",
      "    \"    +-----------------+       | (Tool Dispatcher) |       | (Unified LLM API) |\\n\",\n",
      "    \"                               +-------------------+       +-------------------+\\n\",\n",
      "    \"                                         ^                         |\\n\",\n",
      "    \"                                         |                         |\\n\",\n",
      "    \"                                         |                         |\\n\",\n",
      "    \"                                         |                         |\\n\",\n",
      "    \"                                         |                         v\\n\",\n",
      "    \"                                         |                   +-----------------+\\n\",\n",
      "    \"                                         |                   | LiteLLM Library |\\n\",\n",
      "    \"                                         |                   | (litellm.py)    |\\n\",\n",
      "    \"                                         |                   +-----------------+\\n\",\n",
      "    \"                                         |                         |\\n\",\n",
      "    \"                                         |                         |\\n\",\n",
      "    \"                                         |                         |\\n\",\n",
      "    \"                                         |                         v\\n\",\n",
      "    \"                                         |                   +-----------------+\\n\",\n",
      "    \"                                         |                   | OpenAI API      |\\n\",\n",
      "    \"                                         |                   | Gemini API      |\\n\",\n",
      "    \"                                         |                   | Other LLM APIs  |\\n\",\n",
      "    \"                                         |                   +-----------------+\\n\",\n",
      "    \"                                         |\\n\",\n",
      "    \"                                         +-------------------------------------+\\n\",\n",
      "    \"                                         (Usage Metadata, Responses, Errors)\\n\",\n",
      "    \"    ```\\n\",\n",
      "    \"*   **State-management, Retry/Fallback, and Error-handling Patterns**: LiteLLM offers robust built-in retry and fallback mechanisms.[2](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNO9XSCuQVloTCtVZqw58jsi_tTJFmx2aAJdBRQlPbj69lF2ZDEeGXdC4DOfLIXQ9wnw8vmYjsUreGa5rXUZogzcTaVIPrrHdIc-MoZp93nZIdnsLsynw=) The existing `api_retry` decorator in `llm_manager.py` will be removed, and `LLMManager` will rely on LiteLLM's internal retry logic. Error handling will be adapted to catch LiteLLM-specific exceptions and translate them into `YellhornMCPError` where appropriate.\\n\",\n",
      "    \"\\n\",\n",
      "    \"### Completion Criteria & Metrics\\n\",\n",
      "    \"*   **Engineering Metrics**:\\n\",\n",
      "    \"    *   All `pytest` unit and integration tests pass with 100% success rate.\\n\",\n",
      "    \"    *   Test coverage for `yellhorn_mcp` remains at or above 70% line coverage.\\n\",\n",
      "    \"    *   `black` and `isort` formatting checks pass.\\n\",\n",
      "    \"    *   `pyright` type-checking runs clean with no errors.\\n\",\n",
      "    \"    *   `LLMManager` successfully makes calls to both OpenAI and Gemini models via LiteLLM.\\n\",\n",
      "    \"    *   Cost tracking and usage metadata reported in GitHub comments remain accurate and consistent with previous versions.\\n\",\n",
      "    \"*   **Business Metrics**:\\n\",\n",
      "    \"    *   Reduced time-to-integrate new LLM providers by leveraging LiteLLM's unified interface.\\n\",\n",
      "    \"    *   Improved system reliability due to LiteLLM's robust retry and fallback mechanisms.\\n\",\n",
      "    \"*   **Code-state Definition of Done**:\\n\",\n",
      "    \"    *   All CI jobs (linting, formatting, testing) are green.\\n\",\n",
      "    \"    *   `pyproject.toml` is updated with the new `litellm` dependency.\\n\",\n",
      "    \"    *   `LLMManagerREADME.md` and `docs/USAGE.md` are updated to reflect the LiteLLM integration.\\n\",\n",
      "    \"\\n\",\n",
      "    \"### References\\n\",\n",
      "    \"*   `yellhorn_mcp/llm_manager.py`\\n\",\n",
      "    \"*   `yellhorn_mcp/server.py`\\n\",\n",
      "    \"*   `yellhorn_mcp/cli.py`\\n\",\n",
      "    \"*   `yellhorn_mcp/token_counter.py`\\n\",\n",
      "    \"*   `yellhorn_mcp/integrations/gemini_integration.py`\\n\",\n",
      "    \"*   `yellhorn_mcp/utils/cost_tracker_utils.py`\\n\",\n",
      "    \"*   `yellhorn_mcp/utils/search_grounding_utils.py`\\n\",\n",
      "    \"*   `pyproject.toml`\\n\",\n",
      "    \"*   LiteLLM GitHub Repository: `https://github.com/BerriAI/litellm`[4](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHJqrTQMqVpo30aziDctGWaQBTciLRLQ7cxR_-2L18AA-WDtAeW9Tc15Iulaz3EocAd6-xx6fScsj_J_o8JGGnbw4Umaj6-fP6WKin0aBn91jDU9DRzRDn6aHscPA==)\\n\",\n",
      "    \"*   LiteLLM Documentation: `https://docs.litellm.ai/`[5](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE7TAfZPbbVvO9V1PNxONobzzviteU9UhFpNZDcEBmmds4AFNHD3ve1SKZzeXa0X9_gmmsP5LkeBeFliaiF_qbJbV6oETAKOuLHrSY5jSvChIQC6H3FfQG08-jmKCzRQv0=), [2](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNO9XSCuQVloTCtVZqw58jsi_tTJFmx2aAJdBRQlPbj69lF2ZDEeGXdC4DOfLIXQ9wnw8vmYjsUreGa5rXUZogzcTaVIPrrHdIc-MoZp93nZIdnsLsynw=), [6](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG7LN0z9TdU0oWVk8IIjcNM2V5AwitnYaZ-qI9rf39ZkfkJY2RKVT_n-_7ZGXVmSnpQUZhy7xYafWgPWoH-uLFo1hTUD_FR4gh70OMHcD0r3Xnyj2xp1C1MS7iM3jgMnBsDCjkduQDfz5uk_xOGbD0ptegACqLFiQsWs1bcvY6MW1DJ6yc52PuDzj2LAvvO8tin), [7](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGqLP_Q1VgUi5mp07zBgowNwY4Jc-LmtXBEAwG-NmrcwdEvaEjV4dMKVwWxOPCN4gyhmyl-Ccj5RiSuhOzyq-f-Ym9flHxUOiNu344Rxvqvk8wJ)\\n\",\n",
      "    \"*   LiteLLM PyPI: `https://pypi.org/project/litellm/`\\n\",\n",
      "    \"\\n\",\n",
      "    \"### Implementation Steps\\n\",\n",
      "    \"### - [ ] Step 1: Add LiteLLM Dependency and Remove Direct API Clients\\n\",\n",
      "    \"**Description**: Introduce `litellm` as a core dependency in `pyproject.toml`. Remove direct `google-genai` and `openai` client dependencies, as LiteLLM will abstract these. Remove `tenacity` as LiteLLM provides its own retry logic.\\n\",\n",
      "    \"**Files**:\\n\",\n",
      "    \"*   `pyproject.toml`\\n\",\n",
      "    \"*   `yellhorn_mcp/server.py`\\n\",\n",
      "    \"**Reference(s)**:\\n\",\n",
      "    \"*   `pyproject.toml`\\n\",\n",
      "    \"*   `yellhorn_mcp/server.py`\\n\",\n",
      "    \"*   LiteLLM PyPI: `https://pypi.org/project/litellm/`\\n\",\n",
      "    \"**Test(s)**:\\n\",\n",
      "    \"*   Run `poetry install` to verify dependency resolution.\\n\",\n",
      "    \"*   Ensure `yellhorn-mcp` CLI still starts without errors: `python -m yellhorn_mcp.cli --help`.\\n\",\n",
      "    \"\\n\",\n",
      "    \"### - [ ] Step 2: Refactor `LLMManager` Initialization and Core Call Logic\\n\",\n",
      "    \"**Description**: Modify `LLMManager.__init__` to accept a `litellm` client (or initialize it internally) instead of separate OpenAI and Gemini clients. Update `_single_call` to use `litellm.acompletion()` for all LLM interactions. Remove the `api_retry` decorator from `_call_openai` and `_call_gemini` (or remove these methods entirely if `_single_call` becomes the sole entry point).\\n\",\n",
      "    \"**Files**:\\n\",\n",
      "    \"*   `yellhorn_mcp/llm_manager.py`\\n\",\n",
      "    \"*   `yellhorn_mcp/server.py`\\n\",\n",
      "    \"**Reference(s)**:\\n\",\n",
      "    \"*   `yellhorn_mcp/llm_manager.py`\\n\",\n",
      "    \"*   `yellhorn_mcp/server.py`\\n\",\n",
      "    \"*   LiteLLM Documentation: `https://docs.litellm.ai/docs/completion`\\n\",\n",
      "    \"**Test(s)**:\\n\",\n",
      "    \"*   Unit tests for `LLMManager`'s `_single_call` method, ensuring it correctly routes to LiteLLM.\\n\",\n",
      "    \"*   Mock LiteLLM's `acompletion` to verify call parameters.\\n\",\n",
      "    \"\\n\",\n",
      "    \"### - [ ] Step 3: Adapt `UsageMetadata` and `TokenCounter` for LiteLLM\\n\",\n",
      "    \"**Description**: Update `UsageMetadata` to correctly parse LiteLLM's unified usage format. Investigate if `TokenCounter` can leverage LiteLLM's `token_counter` utility for more accurate or consistent token estimation across models.\\n\",\n",
      "    \"**Files**:\\n\",\n",
      "    \"*   `yellhorn_mcp/llm_manager.py`\\n\",\n",
      "    \"*   `yellhorn_mcp/token_counter.py`\\n\",\n",
      "    \"*   `yellhorn_mcp/models/metadata_models.py`\\n\",\n",
      "    \"**Reference(s)**:\\n\",\n",
      "    \"*   `yellhorn_mcp/llm_manager.py`\\n\",\n",
      "    \"*   `yellhorn_mcp/token_counter.py`\\n\",\n",
      "    \"*   `yellhorn_mcp/models/metadata_models.py`\\n\",\n",
      "    \"*   LiteLLM API: `https://docs.litellm.ai/docs/token_counter`[8](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGO7UZuLkO5CgKE-QOmpelvPzFXNHEcEyR-rll1KfKNLtvPxjmI1sxV0MFp3BiMU0CQxaoQb1h7uJBwtC2MOBdhw2fNZtrmaBRIrDy3pp3Xrb1448DCO1X_TBcT7TY=)\\n\",\n",
      "    \"**Test(s)**:\\n\",\n",
      "    \"*   Unit tests for `UsageMetadata` to ensure correct parsing of LiteLLM's usage data.\\n\",\n",
      "    \"*   Unit tests for `TokenCounter` to verify token counts using LiteLLM's method (if adopted) or ensuring compatibility.\\n\",\n",
      "    \"\\n\",\n",
      "    \"### - [ ] Step 4: Integrate Search Grounding and Deep Research Model Logic with LiteLLM\\n\",\n",
      "    \"**Description**: Adapt the logic for Google Search Grounding (`_get_gemini_search_tools`, `add_citations_from_metadata`) and Deep Research model tool activation (`_is_deep_research_model` in `LLMManager`) to work seamlessly with LiteLLM's unified API. LiteLLM supports passing `tools` parameters.\\n\",\n",
      "    \"**Files**:\\n\",\n",
      "    \"*   `yellhorn_mcp/llm_manager.py`\\n\",\n",
      "    \"*   `yellhorn_mcp/utils/search_grounding_utils.py`\\n\",\n",
      "    \"*   `yellhorn_mcp/integrations/gemini_integration.py` (will be refactored or removed)\\n\",\n",
      "    \"**Reference(s)**:\\n\",\n",
      "    \"*   `yellhorn_mcp/llm_manager.py`\\n\",\n",
      "    \"*   `yellhorn_mcp/utils/search_grounding_utils.py`\\n\",\n",
      "    \"*   LiteLLM API: `https://docs.litellm.ai/docs/completion` (for `tools` parameter)\\n\",\n",
      "    \"*   LiteLLM API: `https://docs.litellm.ai/docs/providers` (for model support)[5](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE7TAfZPbbVvO9V1PNxONobzzviteU9UhFpNZDcEBmmds4AFNHD3ve1SKZzeXa0X9_gmmsP5LkeBeFliaiF_qbJbV6oETAKOuLHrSY5jSvChIQC6H3FfQG08-jmKCzRQv0=)\\n\",\n",
      "    \"**Test(s)**:\\n\",\n",
      "    \"*   Integration tests for `call_llm_with_citations` to ensure search grounding works via LiteLLM.\\n\",\n",
      "    \"*   Unit tests for `LLMManager` to verify deep research model tool activation.\\n\",\n",
      "    \"\\n\",\n",
      "    \"### - [ ] Step 5: Update `server.py` and `cli.py` for LiteLLM Integration\\n\",\n",
      "    \"**Description**: Modify `app_lifespan` in `server.py` to initialize LiteLLM instead of separate OpenAI and Gemini clients. Adjust `cli.py` to handle API keys and model selection in a LiteLLM-compatible manner, potentially simplifying environment variable requirements. Remove `yellhorn_mcp/integrations/gemini_integration.py` as its functionality will be absorbed by `llm_manager.py` and LiteLLM.\\n\",\n",
      "    \"**Files**:\\n\",\n",
      "    \"*   `yellhorn_mcp/server.py`\\n\",\n",
      "    \"*   `yellhorn_mcp/cli.py`\\n\",\n",
      "    \"*   `yellhorn_mcp/integrations/gemini_integration.py` (delete)\\n\",\n",
      "    \"**Reference(s)**:\\n\",\n",
      "    \"*   `yellhorn_mcp/server.py`\\n\",\n",
      "    \"*   `yellhorn_mcp/cli.py`\\n\",\n",
      "    \"*   LiteLLM Getting Started: `https://docs.litellm.ai/docs/getting_started`[2](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHNO9XSCuQVloTCtVZqw58jsi_tTJFmx2aAJdBRQlPbj69lF2ZDEeGXdC4DOfLIXQ9wnw8vmYjsUreGa5rXUZogzcTaVIPrrHdIc-MoZp93nZIdnsLsynw=)\\n\",\n",
      "    \"**Test(s)**:\\n\",\n",
      "    \"*   End-to-end integration tests for `create_workplan`, `judge_workplan`, and `curate_context` via the MCP server to ensure full functionality.\\n\",\n",
      "    \"*   Verify CLI commands still function correctly.\\n\",\n",
      "    \"\\n\",\n",
      "    \"### - [ ] Step 6: Update Cost Tracking and Documentation\\n\",\n",
      "    \"**Description**: Review and update `yellhorn_mcp/utils/cost_tracker_utils.py` to ensure accurate cost calculation with LiteLLM's model naming conventions and potentially leverage LiteLLM's cost tracking features if they offer more granularity. Update `LLMManagerREADME.md` and `docs/USAGE.md` to reflect the new LiteLLM-based architecture and usage.\\n\",\n",
      "    \"**Files**:\\n\",\n",
      "    \"*   `yellhorn_mcp/utils/cost_tracker_utils.py`\\n\",\n",
      "    \"*   `LLMManagerREADME.md`\\n\",\n",
      "    \"*   `docs/USAGE.md`\\n\",\n",
      "    \"**Reference(s)**:\\n\",\n",
      "    \"*   `yellhorn_mcp/utils/cost_tracker_utils.py`\\n\",\n",
      "    \"*   `LLMManagerREADME.md`\\n\",\n",
      "    \"*   `docs/USAGE.md`\\n\",\n",
      "    \"*   LiteLLM Model Cost Map: `https://docs.litellm.ai/docs/proxy/model_cost_map`[8](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGO7UZuLkO5CgKE-QOmpelvPzFXNHEcEyR-rll1KfKNLtvPxjmI1sxV0MFp3BiMU0CQxaoQb1h7uJBwtC2MOBdhw2fNZtrmaBRIrDy3pp3Xrb1448DCO1X_TBcT7TY=)\\n\",\n",
      "    \"**Test(s)**:\\n\",\n",
      "    \"*   Verify cost calculations in generated GitHub comments are correct.\\n\",\n",
      "    \"*   Review documentation for clarity and accuracy.\\n\",\n",
      "    \"\\n\",\n",
      "    \"### Global Test Strategy\\n\",\n",
      "    \"*   **Unit Tests**: All modified functions and classes will have comprehensive unit tests using `pytest` and `pytest-asyncio`. Mocking will be used for external API calls (LiteLLM).\\n\",\n",
      "    \"*   **Integration Tests**: The `examples/mock_context.py` will be updated and used to run integration tests for `create_workplan`, `judge_workplan`, and `curate_context` tools, simulating GitHub interactions and verifying LLM calls through the LiteLLM integration.\\n\",\n",
      "    \"*   **End-to-End Tests**: Manual verification of the `yellhorn-mcp` CLI and its interaction with GitHub issues.\\n\",\n",
      "    \"*   **Test Coverage**: Maintain minimum 70% test coverage for all new and modified code, enforced by `pytest-cov`.\\n\",\n",
      "    \"*   **Local Execution**: Tests can be run locally using `poetry run pytest`.\\n\",\n",
      "    \"*   **Environment Variables/Secrets**: API keys for LiteLLM (and underlying providers if needed) will be managed via environment variables (`LITELLM_API_KEY` or provider-specific keys like `OPENAI_API_KEY`, `GEMINI_API_KEY` as LiteLLM supports them).\\n\",\n",
      "    \"*   **Async Helpers/Fixtures**: Existing `pytest-asyncio` fixtures will be leveraged.\\n\",\n",
      "    \"\\n\",\n",
      "    \"### Files to Modify / New Files to Create\\n\",\n",
      "    \"*   **Files to Modify**:\\n\",\n",
      "    \"    *   `pyproject.toml`: Add `litellm` dependency, remove `google-genai`, `openai`, `tenacity`, `google-api-core`.\\n\",\n",
      "    \"    *   `yellhorn_mcp/llm_manager.py`: Refactor `LLMManager` to use LiteLLM, update `UsageMetadata`, remove `api_retry` decorator.\\n\",\n",
      "    \"    *   `yellhorn_mcp/server.py`: Update `app_lifespan` to initialize LiteLLM, remove direct client initializations.\\n\",\n",
      "    \"    *   `yellhorn_mcp/cli.py`: Adjust API key validation and model selection logic.\\n\",\n",
      "    \"    *   `yellhorn_mcp/token_counter.py`: Potentially integrate LiteLLM's token counting.\\n\",\n",
      "    \"    *   `yellhorn_mcp/utils/cost_tracker_utils.py`: Update `MODEL_PRICING` and `calculate_cost` for LiteLLM models.\\n\",\n",
      "    \"    *   `yellhorn_mcp/utils/search_grounding_utils.py`: Ensure compatibility with LiteLLM's `tools` parameter.\\n\",\n",
      "    \"    *   `LLMManagerREADME.md`: Update LLM Manager usage and architecture.\\n\",\n",
      "    \"    *   `docs/USAGE.md`: Update installation, configuration, and tool usage sections.\\n\",\n",
      "    \"*   **New Files to Create**:\\n\",\n",
      "    \"    *   None, aiming for refactoring existing files.\\n\",\n",
      "    \"[MOCK GitHub CLI] In directory: /Users/sravanj/project_work/yellhorn-mcp\\n\",\n",
      "    \"[MOCK GitHub CLI] Updated issue 2760\\n\",\n",
      "    \"[INFO] Successfully updated GitHub issue #2760 with generated workplan and metrics\\n\",\n",
      "    \"[MOCK GitHub CLI] Running: gh issue comment 2760 --body ## ✅ Workplan generated successfully\\n\",\n",
      "    \"\\n\",\n",
      "    \"### Generation Details\\n\",\n",
      "    \"**Time**: 46.1 seconds  \\n\",\n",
      "    \"**Completed**: 2025-08-10 23:09:33 UTC  \\n\",\n",
      "    \"**Model Used**: `gemini-2.5-flash`  \\n\",\n",
      "    \"\\n\",\n",
      "    \"### Token Usage\\n\",\n",
      "    \"**Input Tokens**: 113,934  \\n\",\n",
      "    \"**Output Tokens**: 4,109  \\n\",\n",
      "    \"**Total Tokens**: 124,466  \\n\",\n",
      "    \"**Estimated Cost**: $0.0445  \\n\",\n",
      "    \"\\n\",\n",
      "    \"**Search Results Used**: 1  \\n\",\n",
      "    \"**Context Size**: 398,011 characters \\\"\\\"\\\"\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 27,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"name\": \"stderr\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"2025-08-10 16:16:17,826 INFO AFC is enabled with max remote calls: 10.\\n\"\n",
      "     ]\n",
      "    },\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"[INFO] Getting codebase snapshot in mode: full\\n\",\n",
      "      \"[INFO] Found .gitignore with 86 patterns\\n\",\n",
      "      \"[INFO] Attempting to enable search grounding for model gemini-2.5-flash\\n\",\n",
      "      \"[INFO] Search grounding enabled for model gemini-2.5-flash\\n\",\n",
      "      \"[INFO] Found .yellhornignore with 1 patterns\\n\",\n",
      "      \"[INFO] Found .yellhorncontext with 17 whitelist, 0 blacklist, and 0 negation patterns\\n\",\n",
      "      \"[INFO] File categorization results out of 73 files:\\n\",\n",
      "      \"[INFO]   - 6 always ignored (images, binaries, configs, etc.)\\n\",\n",
      "      \"[INFO]   - 23 in yellhorncontext whitelist (included)\\n\",\n",
      "      \"[INFO]   - 0 in yellhorncontext blacklist (excluded)\\n\",\n",
      "      \"[INFO]   - 0 in yellhornignore whitelist (included)\\n\",\n",
      "      \"[INFO]   - 0 in yellhornignore blacklist (excluded)\\n\",\n",
      "      \"[INFO]   - 0 other files (excluded - .yellhorncontext exists)\\n\",\n",
      "      \"[INFO] Total included: 23 files (excluded 6 always-ignored files)\\n\",\n",
      "      \"[INFO] Read contents of 23 files\\n\",\n",
      "      \"[MOCK GitHub CLI] Running: gh issue edit 123 --body # Workplan Revision\\n\",\n",
      "      \"\\n\",\n",
      "      \"Here's the revised workplan, focusing on detailed testing steps, including error handling requirements, and emphasizing the creation of dedicated test notebooks for each major change.\\n\",\n",
      "      \"\\n\",\n",
      "      \"# Revised Workplan\\n\",\n",
      "      \"\\n\",\n",
      "      \"## Summary\\n\",\n",
      "      \"The current `yellhorn-mcp` codebase utilizes direct API calls to OpenAI and Google Gemini models, leading to fragmented LLM interaction logic and increased maintenance overhead for supporting new providers. This work plan proposes to replace and consolidate all LLM model calls through a unified model calling service, LiteLLM. This integration will simplify LLM API interactions, streamline the addition of new models and providers, and centralize advanced features like retry logic and cost tracking, thereby enhancing the system's flexibility, maintainability, and scalability. The primary subsystems affected will be `yellhorn_mcp/llm_manager.py`, `yellhorn_mcp/server.py`, and related utility and configuration files.\\n\",\n",
      "      \"\\n\",\n",
      "      \"## Technical Details\\n\",\n",
      "      \"*   **Languages & Frameworks**: Python 3.10+\\n\",\n",
      "      \"*   **External Dependencies**:\\n\",\n",
      "      \"    *   `litellm~=1.33.0`: This new dependency will be added to `pyproject.toml` under `[project] dependencies`. LiteLLM provides a unified interface to over 100 LLM APIs, including OpenAI and Gemini, simplifying API calls and offering built-in features like retry logic and cost tracking.,,\\n\",\n",
      "      \"*   **Dependency Management & Pinning Strategy**: `poetry` (as indicated by `pyproject.toml`). The new `litellm` dependency will be pinned using the `~=` operator for compatible releases.\\n\",\n",
      "      \"*   **Build, Lint, Formatting, Type-checking Commands**:\\n\",\n",
      "      \"    *   `python -m black yellhorn_mcp tests`\\n\",\n",
      "      \"    *   `python -m isort yellhorn_mcp tests`\\n\",\n",
      "      \"    *   `python -m pytest`\\n\",\n",
      "      \"    *   Type-checking will be performed via `pyright` (configured in `pyrightconfig.json`).\\n\",\n",
      "      \"*   **Logging & Observability**: Existing `logging` module will be leveraged. Ensure LiteLLM's internal logging integrates seamlessly or is configured to use the existing logging setup.\\n\",\n",
      "      \"*   **Analytics/KPIs**: Existing cost tracking and usage metadata (`yellhorn_mcp/utils/cost_tracker_utils.py`, `yellhorn_mcp/models/metadata_models.py`) will be adapted to consume LiteLLM's unified usage reports.\\n\",\n",
      "      \"*   **Testing Frameworks & Helpers**: `pytest`, `pytest-asyncio`, `httpx`, `pytest-cov` will continue to be used. Existing mock testing infrastructure (`examples/mock_context.py`) will be adapted to mock LiteLLM calls.\\n\",\n",
      "      \"\\n\",\n",
      "      \"## Architecture\\n\",\n",
      "      \"*   **Existing Components Leveraged**:\\n\",\n",
      "      \"    *   `yellhorn_mcp/llm_manager.py`: The core `LLMManager` class will be refactored to use LiteLLM as its underlying LLM interaction layer. Its existing logic for chunking (`ChunkingStrategy`), token counting (`TokenCounter`), and usage metadata aggregation (`UsageMetadata`) will be adapted to work with LiteLLM's outputs.\\n\",\n",
      "      \"    *   `yellhorn_mcp/token_counter.py`: Will continue to provide model-specific token limits and counting, potentially integrating with LiteLLM's internal token counting utilities if beneficial.\\n\",\n",
      "      \"    *   `yellhorn_mcp/models/metadata_models.py`: `UsageMetrics` and `CompletionMetadata` will continue to track LLM usage, adapted for LiteLLM's reporting format.\\n\",\n",
      "      \"    *   `yellhorn_mcp/utils/cost_tracker_utils.py`: `calculate_cost` and `format_metrics_section` will be updated to map LiteLLM's model identifiers to existing pricing.\\n\",\n",
      "      \"    *   `yellhorn_mcp/utils/search_grounding_utils.py`: The logic for configuring Google Search tools and adding citations will be adapted to pass through LiteLLM's API.\\n\",\n",
      "      \"    *   `yellhorn_mcp/processors/`: All processors (`context_processor.py`, `judgement_processor.py`, `workplan_processor.py`) will continue to use `LLMManager` as their LLM interface.\\n\",\n",
      "      \"*   **New Components Introduced**: No new top-level components will be introduced. The change is primarily an internal refactoring of the `LLMManager` to use LiteLLM.\\n\",\n",
      "      \"*   **Control-flow & Data-flow Diagram (ASCII)**:\\n\",\n",
      "      \"    ```\\n\",\n",
      "      \"    +-----------------+       +-------------------+       +-------------------+\\n\",\n",
      "      \"    | MCP Tools (CLI, |       | yellhorn_mcp/     |       | yellhorn_mcp/     |\\n\",\n",
      "      \"    | VSCode, Claude) |------>| server.py         |------>| llm_manager.py    |\\n\",\n",
      "      \"    +-----------------+       | (Tool Dispatcher) |       | (Unified LLM API) |\\n\",\n",
      "      \"                               +-------------------+       +-------------------+\\\\\\n\",\n",
      "      \"                                         ^                         |\\\\\\n\",\n",
      "      \"                                         |                         |\\\\\\n\",\n",
      "      \"                                         |                         |\\\\\\n\",\n",
      "      \"                                         |                         v\\\\\\n\",\n",
      "      \"                                         |                   +-----------------+\\\\\\n\",\n",
      "      \"                                         |                   | LiteLLM Library |\\\\\\n\",\n",
      "      \"                                         |                   | (litellm.py)    |\\\\\\n\",\n",
      "      \"                                         |                   +-----------------+\\\\\\n\",\n",
      "      \"                                         |                         |\\\\\\n\",\n",
      "      \"                                         |                         |\\\\\\n\",\n",
      "      \"                                         |                         |\\\\\\n\",\n",
      "      \"                                         |                         v\\\\\\n\",\n",
      "      \"                                         |                   +-----------------+\\\\\\n\",\n",
      "      \"                                         |                   | OpenAI API      |\\\\\\n\",\n",
      "      \"                                         |                   | Gemini API      |\\\\\\n\",\n",
      "      \"                                         |                   | Other LLM APIs  |\\\\\\n\",\n",
      "      \"                                         |                   +-----------------+\\\\\\n\",\n",
      "      \"                                         |\\\\\\n\",\n",
      "      \"                                         +-------------------------------------+\\\\\\n\",\n",
      "      \"                                         (Usage Metadata, Responses, Errors)\\n\",\n",
      "      \"    ```\\n\",\n",
      "      \"*   **State-management, Retry/Fallback, and Error-handling Patterns**: LiteLLM offers robust built-in retry and fallback mechanisms. The existing `api_retry` decorator in `llm_manager.py` will be removed, and `LLMManager` will rely on LiteLLM's internal retry logic. Error handling will be adapted to catch LiteLLM-specific exceptions and translate them into `YellhornMCPError` where appropriate.\\n\",\n",
      "      \"\\n\",\n",
      "      \"## Completion Criteria & Metrics\\n\",\n",
      "      \"*   **Engineering Metrics**:\\n\",\n",
      "      \"    *   All `pytest` unit and integration tests pass with 100% success rate.\\n\",\n",
      "      \"    *   Test coverage for `yellhorn_mcp` remains at or above 70% line coverage.\\n\",\n",
      "      \"    *   `black` and `isort` formatting checks pass.\\n\",\n",
      "      \"    *   `pyright` type-checking runs clean with no errors.\\n\",\n",
      "      \"    *   `LLMManager` successfully makes calls to both OpenAI and Gemini models via LiteLLM.\\n\",\n",
      "      \"    *   Cost tracking and usage metadata reported in GitHub comments remain accurate and consistent with previous versions.\\n\",\n",
      "      \"*   **Business Metrics**:\\n\",\n",
      "      \"    *   Reduced time-to-integrate new LLM providers by leveraging LiteLLM's unified interface.\\n\",\n",
      "      \"    *   Improved system reliability due to LiteLLM's robust retry and fallback mechanisms.\\n\",\n",
      "      \"*   **Code-state Definition of Done**:\\n\",\n",
      "      \"    *   All CI jobs (linting, formatting, testing) are green.\\n\",\n",
      "      \"    *   `pyproject.toml` is updated with the new `litellm` dependency.\\n\",\n",
      "      \"    *   `LLMManagerREADME.md` and `docs/USAGE.md` are updated to reflect the LiteLLM integration.\\n\",\n",
      "      \"\\n\",\n",
      "      \"## References\\n\",\n",
      "      \"*   `yellhorn_mcp/llm_manager.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/server.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/cli.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/token_counter.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/integrations/gemini_integration.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/utils/cost_tracker_utils.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/utils/search_grounding_utils.py`\\n\",\n",
      "      \"*   `pyproject.toml`\\n\",\n",
      "      \"*   LiteLLM GitHub Repository: `https://github.com/BerriAI/litellm`\\n\",\n",
      "      \"*   LiteLLM Documentation: `https://docs.litellm.ai/`,,,\\n\",\n",
      "      \"*   LiteLLM PyPI: `https://pypi.org/project/litellm/`\\n\",\n",
      "      \"\\n\",\n",
      "      \"## Implementation Steps\\n\",\n",
      "      \"### - [ ] Step 1: Add LiteLLM Dependency and Remove Direct API Clients\\n\",\n",
      "      \"**Description**: Introduce `litellm` as a core dependency in `pyproject.toml`. Remove direct `google-genai` and `openai` client dependencies, as LiteLLM will abstract these. Remove `tenacity` as LiteLLM provides its own retry logic.\\n\",\n",
      "      \"**Files**:\\n\",\n",
      "      \"*   `pyproject.toml`\\n\",\n",
      "      \"*   `yellhorn_mcp/server.py`\\n\",\n",
      "      \"**Reference(s)**:\\n\",\n",
      "      \"*   `pyproject.toml`\\n\",\n",
      "      \"*   `yellhorn_mcp/server.py`\\n\",\n",
      "      \"*   LiteLLM PyPI: `https://pypi.org/project/litellm/`\\n\",\n",
      "      \"**Test(s)**:\\n\",\n",
      "      \"*   Create a new test notebook `notebooks/test_litellm_dependency.ipynb`.\\n\",\n",
      "      \"*   **Detailed Testing Steps**:\\n\",\n",
      "      \"    *   Verify `poetry install` completes successfully without dependency conflicts.\\n\",\n",
      "      \"    *   Run `python -m yellhorn_mcp.cli --help` and assert that the command executes without `ImportError` or `ModuleNotFoundError` related to `google-genai`, `openai`, or `tenacity`.\\n\",\n",
      "      \"    *   Attempt to import `google.genai`, `openai`, and `tenacity` directly in a Python script and assert that they are no longer directly importable (or raise appropriate errors if they are now transitive dependencies).\\n\",\n",
      "      \"*   **Error Handling Requirements**:\\n\",\n",
      "      \"    *   Ensure `poetry install` provides clear error messages for dependency conflicts.\\n\",\n",
      "      \"    *   Verify that the CLI exits gracefully with an informative error if `litellm` itself cannot be found or initialized.\\n\",\n",
      "      \"\\n\",\n",
      "      \"### - [ ] Step 2: Refactor `LLMManager` Initialization and Core Call Logic\\n\",\n",
      "      \"**Description**: Modify `LLMManager.__init__` to accept a `litellm` client (or initialize it internally) instead of separate OpenAI and Gemini clients. Update `_single_call` to use `litellm.acompletion()` for all LLM interactions. Remove the `api_retry` decorator from `_call_openai` and `_call_gemini` (or remove these methods entirely if `_single_call` becomes the sole entry point).\\n\",\n",
      "      \"**Files**:\\n\",\n",
      "      \"*   `yellhorn_mcp/llm_manager.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/server.py`\\n\",\n",
      "      \"**Reference(s)**:\\n\",\n",
      "      \"*   `yellhorn_mcp/llm_manager.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/server.py`\\n\",\n",
      "      \"*   LiteLLM Documentation: `https://docs.litellm.ai/docs/completion`\\n\",\n",
      "      \"**Test(s)**:\\n\",\n",
      "      \"*   Create a new test notebook `notebooks/test_llm_manager_litellm.ipynb`.\\n\",\n",
      "      \"*   **Detailed Testing Steps**:\\n\",\n",
      "      \"    *   Initialize `LLMManager` with a mocked `litellm` client.\\n\",\n",
      "      \"    *   Call `llm_manager.call_llm` with various prompts, models (OpenAI and Gemini), temperatures, and `response_format` (text and JSON).\\n\",\n",
      "      \"    *   Assert that `litellm.acompletion()` is called with the correct parameters (e.g., `model`, `messages`, `temperature`, `response_format`).\\n\",\n",
      "      \"    *   Verify that the `api_retry` decorator is removed from `_call_openai` and `_call_gemini` (or that these methods are removed entirely).\\n\",\n",
      "      \"    *   Test both simple and chunked calls to ensure the chunking logic still functions correctly with LiteLLM.\\n\",\n",
      "      \"    *   Verify that the content returned from `call_llm` matches the mocked LiteLLM response.\\n\",\n",
      "      \"*   **Error Handling Requirements**:\\n\",\n",
      "      \"    *   Mock LiteLLM to simulate `RateLimitError` and `APIError` (e.g., 500 errors). Assert that LiteLLM's internal retry mechanism handles these errors as expected (e.g., exponential backoff, eventual success or failure after max retries).\\n\",\n",
      "      \"    *   Verify that `LLMManager` propagates unrecoverable LiteLLM errors as `YellhornMCPError` or a similar custom exception.\\n\",\n",
      "      \"    *   Test cases where LiteLLM returns an empty or malformed response.\\n\",\n",
      "      \"\\n\",\n",
      "      \"### - [ ] Step 3: Adapt `UsageMetadata` and `TokenCounter` for LiteLLM\\n\",\n",
      "      \"**Description**: Update `UsageMetadata` to correctly parse LiteLLM's unified usage format. Investigate if `TokenCounter` can leverage LiteLLM's `token_counter` utility for more accurate or consistent token estimation across models.\\n\",\n",
      "      \"**Files**:\\n\",\n",
      "      \"*   `yellhorn_mcp/llm_manager.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/token_counter.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/models/metadata_models.py`\\n\",\n",
      "      \"**Reference(s)**:\\n\",\n",
      "      \"*   `yellhorn_mcp/llm_manager.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/token_counter.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/models/metadata_models.py`\\n\",\n",
      "      \"*   LiteLLM API: `https://docs.litellm.ai/docs/token_counter`\\n\",\n",
      "      \"**Test(s)**:\\n\",\n",
      "      \"*   Extend `notebooks/test_llm_manager_litellm.ipynb` or create `notebooks/test_token_usage_litellm.ipynb`.\\n\",\n",
      "      \"*   **Detailed Testing Steps**:\\n\",\n",
      "      \"    *   Create mock LiteLLM responses with various `usage` structures (e.g., `prompt_tokens`, `completion_tokens`, `total_tokens`).\\n\",\n",
      "      \"    *   Initialize `UsageMetadata` with these mock responses and assert that `prompt_tokens`, `completion_tokens`, and `total_tokens` are correctly extracted.\\n\",\n",
      "      \"    *   Test `TokenCounter`'s `count_tokens` and `get_model_limit` methods with LiteLLM's model naming conventions.\\n\",\n",
      "      \"    *   If LiteLLM's internal token counter is adopted, verify its accuracy against known token counts for sample texts.\\n\",\n",
      "      \"    *   Test edge cases for `UsageMetadata` (e.g., missing fields, `None` values).\\n\",\n",
      "      \"*   **Error Handling Requirements**:\\n\",\n",
      "      \"    *   Ensure `UsageMetadata` gracefully handles unexpected or incomplete usage data from LiteLLM without crashing.\\n\",\n",
      "      \"    *   Verify `TokenCounter` provides a sensible fallback (e.g., default limit) for unknown LiteLLM models.\\n\",\n",
      "      \"\\n\",\n",
      "      \"### - [ ] Step 4: Integrate Search Grounding and Deep Research Model Logic with LiteLLM\\n\",\n",
      "      \"**Description**: Adapt the logic for Google Search Grounding (`_get_gemini_search_tools`, `add_citations_from_metadata`) and Deep Research model tool activation (`_is_deep_research_model` in `LLMManager`) to work seamlessly with LiteLLM's unified API. LiteLLM supports passing `tools` parameters.\\n\",\n",
      "      \"**Files**:\\n\",\n",
      "      \"*   `yellhorn_mcp/llm_manager.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/utils/search_grounding_utils.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/integrations/gemini_integration.py` (will be refactored or removed)\\n\",\n",
      "      \"**Reference(s)**:\\n\",\n",
      "      \"*   `yellhorn_mcp/llm_manager.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/utils/search_grounding_utils.py`\\n\",\n",
      "      \"*   LiteLLM API: `https://docs.litellm.ai/docs/completion` (for `tools` parameter)\\n\",\n",
      "      \"*   LiteLLM API: `https://docs.litellm.ai/docs/providers` (for model support)\\n\",\n",
      "      \"**Test(s)**:\\n\",\n",
      "      \"*   Extend `notebooks/test_llm_manager_litellm.ipynb`.\\n\",\n",
      "      \"*   **Detailed Testing Steps**:\\n\",\n",
      "      \"    *   Mock LiteLLM calls to simulate responses with `tools` parameters for Gemini models (e.g., `web_search_preview`).\\n\",\n",
      "      \"    *   Call `llm_manager.call_llm_with_citations` with a Gemini model and `tools` enabled. Assert that the `grounding_metadata` is correctly extracted and passed through.\\n\",\n",
      "      \"    *   Verify that `add_citations_from_metadata` correctly formats citations from LiteLLM's grounding metadata.\\n\",\n",
      "      \"    *   Test `_is_deep_research_model` in `LLMManager` to ensure it correctly identifies deep research models and that the appropriate `tools` (web search, code interpreter) are included in the LiteLLM call parameters.\\n\",\n",
      "      \"    *   Test cases where search grounding is disabled (`disable_search_grounding=True`) to ensure no tools are passed.\\n\",\n",
      "      \"*   **Error Handling Requirements**:\\n\",\n",
      "      \"    *   Verify that if LiteLLM fails to process `tools` parameters, an appropriate error is logged or raised.\\n\",\n",
      "      \"    *   Ensure `add_citations_from_metadata` gracefully handles missing or malformed grounding metadata.\\n\",\n",
      "      \"\\n\",\n",
      "      \"### - [ ] Step 5: Update `server.py` and `cli.py` for LiteLLM Integration\\n\",\n",
      "      \"**Description**: Modify `app_lifespan` in `server.py` to initialize LiteLLM instead of separate OpenAI and Gemini clients. Adjust `cli.py` to handle API keys and model selection in a LiteLLM-compatible manner, potentially simplifying environment variable requirements. Remove `yellhorn_mcp/integrations/gemini_integration.py` as its functionality will be absorbed by `llm_manager.py` and LiteLLM.\\n\",\n",
      "      \"**Files**:\\n\",\n",
      "      \"*   `yellhorn_mcp/server.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/cli.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/integrations/gemini_integration.py` (delete)\\n\",\n",
      "      \"**Reference(s)**:\\n\",\n",
      "      \"*   `yellhorn_mcp/server.py`\\n\",\n",
      "      \"*   `yellhorn_mcp/cli.py`\\n\",\n",
      "      \"*   LiteLLM Getting Started: `https://docs.litellm.ai/docs/getting_started`\\n\",\n",
      "      \"**Test(s)**:\\n\",\n",
      "      \"*   Create a new test notebook `notebooks/test_server_cli_integration.ipynb`.\\n\",\n",
      "      \"*   **Detailed Testing Steps**:\\n\",\n",
      "      \"    *   Run the `yellhorn-mcp` CLI with various model arguments (OpenAI, Gemini, deep research) and assert that `LLMManager` is initialized correctly with LiteLLM.\\n\",\n",
      "      \"    *   Verify that `app_lifespan` in `server.py` no longer initializes `google.genai.Client` or `openai.AsyncOpenAI` directly.\\n\",\n",
      "      \"    *   Perform end-to-end tests for `create_workplan`, `judge_workplan`, and `curate_context` using the `run_create_workplan`, `run_judge_workplan`, and `run_curate_context` helpers from `examples/mock_context.py`.\\n\",\n",
      "      \"    *   Assert that these tools successfully interact with the mocked LiteLLM (via `LLMManager`) and produce expected outputs (e.g., GitHub issue updates, context files).\\n\",\n",
      "      \"    *   Verify that API key validation in `cli.py` is adapted for LiteLLM's requirements (e.g., `LITELLM_API_KEY` or provider-specific keys if LiteLLM still uses them).\\n\",\n",
      "      \"*   **Error Handling Requirements**:\\n\",\n",
      "      \"    *   Test CLI startup with missing API keys and assert that it exits with a clear error message.\\n\",\n",
      "      \"    *   Simulate LiteLLM API errors during end-to-end tool execution and verify that appropriate error comments are posted to GitHub issues.\\n\",\n",
      "      \"    *   Ensure `app_lifespan` handles `ValueError` for missing environment variables gracefully.\\n\",\n",
      "      \"\\n\",\n",
      "      \"### - [ ] Step 6: Update Cost Tracking and Documentation\\n\",\n",
      "      \"**Description**: Review and update `yellhorn_mcp/utils/cost_tracker_utils.py` to ensure accurate cost calculation with LiteLLM's model naming conventions and potentially leverage LiteLLM's cost tracking features if they offer more granularity. Update `LLMManagerREADME.md` and `docs/USAGE.md` to reflect the new LiteLLM-based architecture and usage.\\n\",\n",
      "      \"**Files**:\\n\",\n",
      "      \"*   `yellhorn_mcp/utils/cost_tracker_utils.py`\\n\",\n",
      "      \"*   `LLMManagerREADME.md`\\n\",\n",
      "      \"*   `docs/USAGE.md`\\n\",\n",
      "      \"**Reference(s)**:\\n\",\n",
      "      \"*   `yellhorn_mcp/utils/cost_tracker_utils.py`\\n\",\n",
      "      \"*   `LLMManagerREADME.md`\\n\",\n",
      "      \"*   `docs/USAGE.md`\\n\",\n",
      "      \"*   LiteLLM Model Cost Map: `https://docs.litellm.ai/docs/proxy/model_cost_map`\\n\",\n",
      "      \"**Test(s)**:\\n\",\n",
      "      \"*   Extend `notebooks/test_llm_manager_litellm.ipynb` or create `notebooks/test_cost_tracking.ipynb`.\\n\",\n",
      "      \"*   **Detailed Testing Steps**:\\n\",\n",
      "      \"    *   Update `MODEL_PRICING` in `cost_tracker_utils.py` to reflect LiteLLM's model naming conventions and pricing (if different).\\n\",\n",
      "      \"    *   Create unit tests for `calculate_cost` with various LiteLLM models and token counts, asserting correct cost calculation.\\n\",\n",
      "      \"    *   Verify that `format_metrics_section` correctly displays usage and cost information based on LiteLLM's output.\\n\",\n",
      "      \"    *   Manually review `LLMManagerREADME.md` and `docs/USAGE.md` to ensure all references to direct OpenAI/Gemini clients are replaced with LiteLLM, and new usage instructions are clear.\\n\",\n",
      "      \"*   **Error Handling Requirements**:\\n\",\n",
      "      \"    *   Ensure `calculate_cost` gracefully handles unknown LiteLLM models (e.g., returns `None` or a default value).\\n\",\n",
      "      \"    *   Verify that documentation clearly outlines any new environment variables or configuration required for LiteLLM.\\n\",\n",
      "      \"\\n\",\n",
      "      \"## Global Test Strategy\\n\",\n",
      "      \"*   **Unit Tests**: All modified functions and classes will have comprehensive unit tests using `pytest` and `pytest-asyncio`. Mocking will be used for external API calls (LiteLLM). Focus on isolated component behavior and edge cases.\\n\",\n",
      "      \"*   **Test Notebooks**: Dedicated Jupyter notebooks (e.g., `notebooks/test_litellm_dependency.ipynb`, `notebooks/test_llm_manager_litellm.ipynb`, `notebooks/test_server_cli_integration.ipynb`, `notebooks/test_cost_tracking.ipynb`) will be created for each major change. These notebooks will serve as executable integration tests and living documentation for testing the new LiteLLM integration.\\n\",\n",
      "      \"*   **Integration Tests**: The `examples/mock_context.py` will be updated and used within the test notebooks to run integration tests for `create_workplan`, `judge_workplan`, and `curate_context` tools. These tests will simulate GitHub interactions and verify LLM calls through the LiteLLM integration, including chunking, search grounding, and deep research model activation.\\n\",\n",
      "      \"*   **Error Handling Tests**: Comprehensive tests will be added to verify robust error handling and retry logic. This includes:\\n\",\n",
      "      \"    *   Simulating API rate limits and transient network errors to ensure LiteLLM's built-in retry mechanism functions correctly.\\n\",\n",
      "      \"    *   Testing cases where LiteLLM returns malformed or empty responses.\\n\",\n",
      "      \"    *   Verifying that `YellhornMCPError` or other appropriate custom exceptions are raised for unrecoverable errors, and that informative error messages are logged and posted to GitHub issues.\\n\",\n",
      "      \"    *   Testing CLI startup with missing or invalid API keys.\\n\",\n",
      "      \"*   **End-to-End Tests**: Manual verification of the `yellhorn-mcp` CLI and its interaction with GitHub issues will be performed to confirm full functionality in a real environment.\\n\",\n",
      "      \"*   **Test Coverage**: Maintain minimum 70% test coverage for all new and modified code, enforced by `pytest-cov`.\\n\",\n",
      "      \"*   **Local Execution**: Tests can be run locally using `poetry run pytest` and by executing the new test notebooks.\\n\",\n",
      "      \"*   **Environment Variables/Secrets**: API keys for LiteLLM (and underlying providers if needed) will be managed via environment variables (`LITELLM_API_KEY` or provider-specific keys like `OPENAI_API_KEY`, `GEMINI_API_KEY` as LiteLLM supports them).\\n\",\n",
      "      \"*   **Async Helpers/Fixtures**: Existing `pytest-asyncio` fixtures will be leveraged.\\n\",\n",
      "      \"\\n\",\n",
      "      \"## Files to Modify / New Files to Create\\n\",\n",
      "      \"*   **Files to Modify**:\\n\",\n",
      "      \"    *   `pyproject.toml`: Add `litellm` dependency, remove `google-genai`, `openai`, `tenacity`, `google-api-core`.\\n\",\n",
      "      \"    *   `yellhorn_mcp/llm_manager.py`: Refactor `LLMManager` to use LiteLLM, update `UsageMetadata`, remove `api_retry` decorator.\\n\",\n",
      "      \"    *   `yellhorn_mcp/server.py`: Update `app_lifespan` to initialize LiteLLM, remove direct client initializations.\\n\",\n",
      "      \"    *   `yellhorn_mcp/cli.py`: Adjust API key validation and model selection logic.\\n\",\n",
      "      \"    *   `yellhorn_mcp/token_counter.py`: Potentially integrate LiteLLM's token counting.\\n\",\n",
      "      \"    *   `yellhorn_mcp/utils/cost_tracker_utils.py`: Update `MODEL_PRICING` and `calculate_cost` for LiteLLM models.\\n\",\n",
      "      \"    *   `yellhorn_mcp/utils/search_grounding_utils.py`: Ensure compatibility with LiteLLM's `tools` parameter.\\n\",\n",
      "      \"    *   `LLMManagerREADME.md`: Update LLM Manager usage and architecture.\\n\",\n",
      "      \"    *   `docs/USAGE.md`: Update installation, configuration, and tool usage sections.\\n\",\n",
      "      \"*   **New Files to Create**:\\n\",\n",
      "      \"    *   `notebooks/test_litellm_dependency.ipynb`: Test notebook for dependency changes and CLI startup.\\n\",\n",
      "      \"    *   `notebooks/test_llm_manager_litellm.ipynb`: Test notebook for `LLMManager` core logic, chunking, search grounding, deep research models, and usage/token counting.\\n\",\n",
      "      \"    *   `notebooks/test_server_cli_integration.ipynb`: Test notebook for end-to-end server and CLI tool integration.\\n\",\n",
      "      \"    *   `notebooks/test_cost_tracking.ipynb`: (Optional, can be merged with `test_llm_manager_litellm.ipynb`) Test notebook for cost calculation accuracy.\\n\",\n",
      "      \"    *   `yellhorn_mcp/integrations/gemini_integration.py` (delete) - This file will be removed as its functionality is absorbed.\\n\",\n",
      "      \"[MOCK GitHub CLI] In directory: /Users/sravanj/project_work/yellhorn-mcp\\n\",\n",
      "      \"[MOCK GitHub CLI] Updated issue 123\\n\",\n",
      "      \"[INFO] Successfully updated GitHub issue #123 with generated workplan and metrics\\n\",\n",
      "      \"[MOCK GitHub CLI] Running: gh issue comment 123 --body ## ✅ Workplan generated successfully\\n\",\n",
      "      \"\\n\",\n",
      "      \"### Generation Details\\n\",\n",
      "      \"**Time**: 47.8 seconds  \\n\",\n",
      "      \"**Completed**: 2025-08-10 23:17:05 UTC  \\n\",\n",
      "      \"**Model Used**: `gemini-2.5-flash`  \\n\",\n",
      "      \"\\n\",\n",
      "      \"### Token Usage\\n\",\n",
      "      \"**Input Tokens**: 120,233  \\n\",\n",
      "      \"**Output Tokens**: 5,950  \\n\",\n",
      "      \"**Total Tokens**: 130,829  \\n\",\n",
      "      \"**Estimated Cost**: $0.0509  \\n\",\n",
      "      \"\\n\",\n",
      "      \"**Search Results Used**: 0  \\n\",\n",
      "      \"**Context Size**: 415,846 characters  \\n\",\n",
      "      \"[MOCK GitHub CLI] In directory: /Users/sravanj/project_work/yellhorn-mcp\\n\",\n",
      "      \"[MOCK GitHub CLI] Added comment to issue 123\\n\",\n",
      "      \"[INFO] Waiting for 0 background tasks...\\n\",\n",
      "      \"[INFO] All background tasks completed\\n\",\n",
      "      \"Workplan Revised:\\n\",\n",
      "      \"--------------------------------------------------\\n\",\n",
      "      \"Issue URL: https://github.com/mock/repo/issues/123\\n\",\n",
      "      \"Issue Number: 123\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"# Call process_revision_async with our mock context\\n\",\n",
      "    \"revision_result = await run_revise_workplan(\\n\",\n",
      "    \"    issue_number=\\\"123\\\", \\n\",\n",
      "    \"    original_workplan=\\\"Original workplan content here...\\\", \\n\",\n",
      "    \"    revision_instructions=\\\"Focus on building test notebooks for each of the major changes. Add more detailed testing steps and include error handling requirements\\\",\\n\",\n",
      "    \"    repo_path=REPO_PATH,\\n\",\n",
      "    \"    llm_manager=llm_manager,\\n\",\n",
      "    \"    model=MODEL,\\n\",\n",
      "    \"    codebase_reasoning=\\\"full\\\",\\n\",\n",
      "    \"    debug=False,\\n\",\n",
      "    \"    disable_search_grounding=False,\\n\",\n",
      "    \"    github_command_func=mock_github_command,\\n\",\n",
      "    \"    git_command_func=run_git_command_with_set_cwd(REPO_PATH),\\n\",\n",
      "    \"    log_callback=log_callback,\\n\",\n",
      "    \"    wait_for_background_tasks=True,\\n\",\n",
      "    \"    background_task_timeout=180\\n\",\n",
      "    \")\\n\",\n",
      "    \"\\n\",\n",
      "    \"print(\\\"Workplan Revised:\\\")\\n\",\n",
      "    \"print(\\\"-\\\" * 50)\\n\",\n",
      "    \"print(f\\\"Issue URL: {revision_result['issue_url']}\\\")\\n\",\n",
      "    \"print(f\\\"Issue Number: {revision_result['issue_number']}\\\")\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"## 6. Using Judge Workplan\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 28,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": [\n",
      "    \"from examples.mock_context import run_judge_workplan, mock_github_command\\n\",\n",
      "    \"from yellhorn_mcp.server import process_judgement_async\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 29,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": [\n",
      "    \"# Example workplan content (you would typically get this from a GitHub issue)\\n\",\n",
      "    \"WORKPLAN_CONTENT = \\\"\\\"\\\"\\n\",\n",
      "    \"# Example Workplan: Add Token Counter Feature\\n\",\n",
      "    \"\\n\",\n",
      "    \"## Summary\\n\",\n",
      "    \"Implement a token counting system using tiktoken to prevent token overflow in LLM calls.\\n\",\n",
      "    \"\\n\",\n",
      "    \"## Implementation Steps\\n\",\n",
      "    \"1. Create TokenCounter class with tiktoken integration\\n\",\n",
      "    \"2. Add token counting to LLMManager\\n\",\n",
      "    \"3. Implement automatic prompt chunking when limits are exceeded\\n\",\n",
      "    \"4. Update all LLM call sites to use the new system\\n\",\n",
      "    \"\\n\",\n",
      "    \"## Files to Modify\\n\",\n",
      "    \"- `yellhorn_mcp/llm_manager.py`: Add token counting integration\\n\",\n",
      "    \"- `yellhorn_mcp/server.py`: Update LLM call sites\\n\",\n",
      "    \"\\n\",\n",
      "    \"## New Files to Create\\n\",\n",
      "    \"- `yellhorn_mcp/token_counter.py`: Core token counting functionality\\n\",\n",
      "    \"\\\"\\\"\\\"\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Example diff content (you would typically get this from git diff)\\n\",\n",
      "    \"DIFF_CONTENT = \\\"\\\"\\\"\\n\",\n",
      "    \"diff --git a/yellhorn_mcp/token_counter.py b/yellhorn_mcp/token_counter.py\\n\",\n",
      "    \"new file mode 100644\\n\",\n",
      "    \"index 0000000..1234567\\n\",\n",
      "    \"--- /dev/null\\n\",\n",
      "    \"+++ b/yellhorn_mcp/token_counter.py\\n\",\n",
      "    \"@@ -0,0 +1,50 @@\\n\",\n",
      "    \"+# Token counting utilities using tiktoken.\\n\",\n",
      "    \"+\\n\",\n",
      "    \"+import tiktoken\\n\",\n",
      "    \"+from typing import Optional\\n\",\n",
      "    \"+\\n\",\n",
      "    \"+class TokenCounter:\\n\",\n",
      "    \"+    \\\\\\\"\\\\\\\"\\\\\\\"Token counter using tiktoken for accurate token counting.\\\\\\\"\\\\\\\"\\\\\\\"\\n\",\n",
      "    \"+    \\n\",\n",
      "    \"+    def __init__(self, model: str):\\n\",\n",
      "    \"+        self.model = model\\n\",\n",
      "    \"+        self.encoding = tiktoken.encoding_for_model(model)\\n\",\n",
      "    \"+    \\n\",\n",
      "    \"+    def count_tokens(self, text: str) -> int:\\n\",\n",
      "    \"+        \\\\\\\"\\\\\\\"\\\\\\\"Count tokens in the given text.\\\\\\\"\\\\\\\"\\\\\\\"\\n\",\n",
      "    \"+        return len(self.encoding.encode(text))\\n\",\n",
      "    \"\\n\",\n",
      "    \"diff --git a/yellhorn_mcp/llm_manager.py b/yellhorn_mcp/llm_manager.py\\n\",\n",
      "    \"index abcd123..efgh456 100644\\n\",\n",
      "    \"--- a/yellhorn_mcp/llm_manager.py\\n\",\n",
      "    \"+++ b/yellhorn_mcp/llm_manager.py\\n\",\n",
      "    \"@@ -10,6 +10,7 @@ from typing import Dict, List, Optional, Any, Union\\n\",\n",
      "    \" from openai import AsyncOpenAI\\n\",\n",
      "    \" import google.genai as genai\\n\",\n",
      "    \" from .usage_metadata import UsageMetadata\\n\",\n",
      "    \"+from .token_counter import TokenCounter\\n\",\n",
      "    \" \\n\",\n",
      "    \" class LLMManager:\\n\",\n",
      "    \"     \\\\\\\"\\\\\\\"\\\\\\\"Unified LLM manager with token counting and chunking support.\\\\\\\"\\\\\\\"\\\\\\\"\\n\",\n",
      "    \"\\\"\\\"\\\"\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Judge workplan parameters\\n\",\n",
      "    \"base_ref = \\\"main\\\"\\n\",\n",
      "    \"head_ref = \\\"feature/token-counter\\\"\\n\",\n",
      "    \"subissue_to_update = \\\"456\\\"  # GitHub issue number for the sub-issue to update\\n\",\n",
      "    \"parent_workplan_issue_number = \\\"123\\\"  # Original workplan issue number\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": null,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": [\n",
      "    \"# Call judge_workplan with our mock context\\n\",\n",
      "    \"await run_judge_workplan(\\n\",\n",
      "    \"    workplan_content=WORKPLAN_CONTENT,\\n\",\n",
      "    \"    diff_content=DIFF_CONTENT,\\n\",\n",
      "    \"    base_ref=base_ref,\\n\",\n",
      "    \"    head_ref=head_ref,\\n\",\n",
      "    \"    subissue_to_update=subissue_to_update,\\n\",\n",
      "    \"    parent_workplan_issue_number=parent_workplan_issue_number,\\n\",\n",
      "    \"    repo_path=REPO_PATH,\\n\",\n",
      "    \"    gemini_client=gemini_client,\\n\",\n",
      "    \"    openai_client=openai_client,\\n\",\n",
      "    \"    llm_manager=llm_manager,\\n\",\n",
      "    \"    model=MODEL,\\n\",\n",
      "    \"    base_commit_hash=\\\"abc123\\\",  # Optional: actual commit hash\\n\",\n",
      "    \"    head_commit_hash=\\\"def456\\\",  # Optional: actual commit hash\\n\",\n",
      "    \"    debug=True,  # Set to True to see the full prompt used\\n\",\n",
      "    \"    codebase_reasoning=\\\"full\\\",  # Options: \\\"full\\\", \\\"lsp\\\", \\\"file_structure\\\", \\\"none\\\"\\n\",\n",
      "    \"    disable_search_grounding=False,\\n\",\n",
      "    \"    github_command_func=mock_github_command,\\n\",\n",
      "    \"    log_callback=log_callback,\\n\",\n",
      "    \"    wait_for_background_tasks=True,\\n\",\n",
      "    \"    background_task_timeout=180\\n\",\n",
      "    \")\\n\",\n",
      "    \"\\n\",\n",
      "    \"print(\\\"\\\\nJudgement completed successfully!\\\")\\n\",\n",
      "    \"print(f\\\"Check GitHub sub-issue #{subissue_to_update} for the detailed judgement results.\\\")\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": null,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": []\n",
      "  }\n",
      " ],\n",
      " \"metadata\": {\n",
      "  \"kernelspec\": {\n",
      "   \"display_name\": \"sravan-yellhorn\",\n",
      "   \"language\": \"python\",\n",
      "   \"name\": \"python3\"\n",
      "  },\n",
      "  \"language_info\": {\n",
      "   \"codemirror_mode\": {\n",
      "    \"name\": \"ipython\",\n",
      "    \"version\": 3\n",
      "   },\n",
      "   \"file_extension\": \".py\",\n",
      "   \"mimetype\": \"text/x-python\",\n",
      "   \"name\": \"python\",\n",
      "   \"nbconvert_exporter\": \"python\",\n",
      "   \"pygments_lexer\": \"ipython3\",\n",
      "   \"version\": \"3.11.11\"\n",
      "  }\n",
      " },\n",
      " \"nbformat\": 4,\n",
      " \"nbformat_minor\": 4\n",
      "}\n",
      "\n",
      "--- File: notebooks/test_curate_context_v2.ipynb ---\n",
      "{\n",
      " \"cells\": [\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"# Test Context Curation with Real Repository and LLM\\n\",\n",
      "    \"\\n\",\n",
      "    \"This notebook tests the context curation functions with an actual repository and real LLM calls.\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"## Setup and Configuration\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 1,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [],\n",
      "   \"source\": [\n",
      "    \"%load_ext autoreload\\n\",\n",
      "    \"%autoreload 2\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 2,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"✅ Imports successful\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"import sys\\n\",\n",
      "    \"import os\\n\",\n",
      "    \"from pathlib import Path\\n\",\n",
      "    \"import asyncio\\n\",\n",
      "    \"import json\\n\",\n",
      "    \"from datetime import datetime\\n\",\n",
      "    \"from unittest.mock import AsyncMock, MagicMock\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Add the parent directory to path to import yellhorn_mcp\\n\",\n",
      "    \"sys.path.insert(0, os.path.dirname(os.getcwd()))\\n\",\n",
      "    \"\\n\",\n",
      "    \"from yellhorn_mcp.processors.context_processor import (\\n\",\n",
      "    \"    build_codebase_context,\\n\",\n",
      "    \"    analyze_with_llm,\\n\",\n",
      "    \"    parse_llm_directories,\\n\",\n",
      "    \"    save_context_file,\\n\",\n",
      "    \"    process_context_curation_async\\n\",\n",
      "    \")\\n\",\n",
      "    \"from yellhorn_mcp.llm_manager import LLMManager\\n\",\n",
      "    \"from yellhorn_mcp.utils.git_utils import YellhornMCPError, run_git_command_with_set_cwd, run_github_command_with_set_cwd\\n\",\n",
      "    \"\\n\",\n",
      "    \"print(\\\"✅ Imports successful\\\")\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"## Configure LLM Manager\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": null,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"🤖 Using OpenAI model: gpt-4o-mini\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"# Initialize LLM Manager with API clients\\n\",\n",
      "    \"# You need to have your API keys set as environment variables:\\n\",\n",
      "    \"GEMINI_API_KEY = \\\"\\\"\\n\",\n",
      "    \"OPENAI_API_KEY = \\\"\\\"\\n\",\n",
      "    \"\\n\",\n",
      "    \"from openai import AsyncOpenAI\\n\",\n",
      "    \"from google import genai\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Initialize clients\\n\",\n",
      "    \"openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\\n\",\n",
      "    \"gemini_client = genai.Client(api_key=GEMINI_API_KEY)\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Create LLM Manager\\n\",\n",
      "    \"llm_manager = LLMManager(\\n\",\n",
      "    \"    openai_client=openai_client,\\n\",\n",
      "    \"    gemini_client=gemini_client,\\n\",\n",
      "    \"    config={\\n\",\n",
      "    \"        \\\"safety_margin_tokens\\\": 1000,\\n\",\n",
      "    \"        \\\"overlap_ratio\\\": 0.1,\\n\",\n",
      "    \"        \\\"chunk_strategy\\\": \\\"sentences\\\"\\n\",\n",
      "    \"    }\\n\",\n",
      "    \")\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Determine which model to use\\n\",\n",
      "    \"if openai_client:\\n\",\n",
      "    \"    DEFAULT_MODEL = \\\"gpt-4o-mini\\\"  # Using a cheaper model for testing\\n\",\n",
      "    \"    print(f\\\"🤖 Using OpenAI model: {DEFAULT_MODEL}\\\")\\n\",\n",
      "    \"elif gemini_client:\\n\",\n",
      "    \"    DEFAULT_MODEL = \\\"gemini-1.5-flash\\\"  # Fast and cheap Gemini model\\n\",\n",
      "    \"    print(f\\\"🤖 Using Gemini model: {DEFAULT_MODEL}\\\")\\n\",\n",
      "    \"else:\\n\",\n",
      "    \"    print(\\\"❌ No LLM client available. Please set OPENAI_API_KEY or GOOGLE_API_KEY\\\")\\n\",\n",
      "    \"    DEFAULT_MODEL = None\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"## Set Repository Path\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 4,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"✅ Repository path exists: /Users/sravanj/project_work/yellhorn-mcp\\n\",\n",
      "      \"📁 Repository contents (sample):\\n\",\n",
      "      \"  📄 .yellhornignore\\n\",\n",
      "      \"  📂 yellhorn_mcp/\\n\",\n",
      "      \"  📄 .DS_Store\\n\",\n",
      "      \"  📄 pyrightconfig.json\\n\",\n",
      "      \"  📂 .pytest_cache/\\n\",\n",
      "      \"  📄 CHANGELOG.md\\n\",\n",
      "      \"  📄 .coverage\\n\",\n",
      "      \"  📄 LLMManagerREADME.md\\n\",\n",
      "      \"  📄 .mcp.json\\n\",\n",
      "      \"  📄 pyproject.toml\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"# Use the actual yellhorn-mcp repository path\\n\",\n",
      "    \"REPO_PATH = Path(\\\"/Users/sravanj/project_work/yellhorn-mcp\\\")\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Verify the path exists\\n\",\n",
      "    \"if REPO_PATH.exists():\\n\",\n",
      "    \"    print(f\\\"✅ Repository path exists: {REPO_PATH}\\\")\\n\",\n",
      "    \"    print(f\\\"📁 Repository contents (sample):\\\")\\n\",\n",
      "    \"    for item in list(REPO_PATH.iterdir())[:10]:\\n\",\n",
      "    \"        if item.is_dir():\\n\",\n",
      "    \"            print(f\\\"  📂 {item.name}/\\\")\\n\",\n",
      "    \"        else:\\n\",\n",
      "    \"            print(f\\\"  📄 {item.name}\\\")\\n\",\n",
      "    \"else:\\n\",\n",
      "    \"    print(f\\\"❌ Repository path does not exist: {REPO_PATH}\\\")\\n\",\n",
      "    \"    print(\\\"Please update REPO_PATH to point to your repository\\\")\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"## Create Git Command Function and Mock Context Helpers\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 5,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"✅ Git command function and mock context helpers created\\n\",\n",
      "      \"   - git_command_func: Real git command wrapper\\n\",\n",
      "      \"   - create_mock_context(): Creates mock context with real git commands\\n\",\n",
      "      \"   - print_log_messages(): Displays logged messages from mock context\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"def create_mock_context():\\n\",\n",
      "    \"    \\\"\\\"\\\"Create a mock context with logging capabilities.\\\"\\\"\\\"\\n\",\n",
      "    \"    mock_ctx = MagicMock()\\n\",\n",
      "    \"    mock_ctx.log = AsyncMock()\\n\",\n",
      "    \"    mock_ctx.request_context.lifespan_context = {\\n\",\n",
      "    \"        \\\"git_command_func\\\": run_git_command_with_set_cwd(REPO_PATH),\\n\",\n",
      "    \"        \\\"codebase_reasoning\\\": \\\"file_structure\\\"\\n\",\n",
      "    \"    }\\n\",\n",
      "    \"    return mock_ctx\\n\",\n",
      "    \"\\n\",\n",
      "    \"def print_log_messages(mock_ctx, limit=100):\\n\",\n",
      "    \"    \\\"\\\"\\\"Print the log messages from mock context.\\\"\\\"\\\"\\n\",\n",
      "    \"    if mock_ctx.log.called:\\n\",\n",
      "    \"        log_messages = [(call[1].get('level', 'info'), call[1]['message']) \\n\",\n",
      "    \"                       for call in mock_ctx.log.call_args_list]\\n\",\n",
      "    \"        print(f\\\"\\\\n📝 Log messages (showing first {limit}):\\\")\\n\",\n",
      "    \"        for level, msg in log_messages[:limit]:\\n\",\n",
      "    \"            emoji = \\\"🔵\\\" if level == \\\"info\\\" else \\\"🟡\\\" if level == \\\"warning\\\" else \\\"🔴\\\"\\n\",\n",
      "    \"            print(f\\\"  {emoji} [{level}] {msg[:5000]}...\\\" if len(msg) > 5000 else f\\\"  {emoji} [{level}] {msg}\\\")\\n\",\n",
      "    \"        if len(log_messages) > limit:\\n\",\n",
      "    \"            print(f\\\"  ... and {len(log_messages) - limit} more messages\\\")\\n\",\n",
      "    \"    else:\\n\",\n",
      "    \"        print(\\\"\\\\n📝 No log messages recorded\\\")\\n\",\n",
      "    \"\\n\",\n",
      "    \"print(\\\"✅ Git command function and mock context helpers created\\\")\\n\",\n",
      "    \"print(\\\"   - git_command_func: Real git command wrapper\\\")\\n\",\n",
      "    \"print(\\\"   - create_mock_context(): Creates mock context with real git commands\\\")\\n\",\n",
      "    \"print(\\\"   - print_log_messages(): Displays logged messages from mock context\\\")\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"## Test 1: Build Codebase Context with Real Repository\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 6,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"\\n\",\n",
      "      \"🧪 Testing build_codebase_context with real repository...\\n\",\n",
      "      \"\\n\",\n",
      "      \"⏱️ Time taken: 0.17 seconds\\n\",\n",
      "      \"\\n\",\n",
      "      \"📊 Statistics:\\n\",\n",
      "      \"  - Total files found: 37\\n\",\n",
      "      \"  - Total directories: 11\\n\",\n",
      "      \"  - Context size: 1128 characters\\n\",\n",
      "      \"\\n\",\n",
      "      \"📁 Top-level directories:\\n\",\n",
      "      \"  - .\\n\",\n",
      "      \"  - .github\\n\",\n",
      "      \"  - docs\\n\",\n",
      "      \"  - notebooks\\n\",\n",
      "      \"  - yellhorn_mcp\\n\",\n",
      "      \"\\n\",\n",
      "      \"📄 Sample files (first 10):\\n\",\n",
      "      \"  - .github/workflows/publish.yml\\n\",\n",
      "      \"  - .github/workflows/tests.yml\\n\",\n",
      "      \"  - .mcp.json\\n\",\n",
      "      \"  - .python-version\\n\",\n",
      "      \"  - CHANGELOG.md\\n\",\n",
      "      \"  - CLAUDE.md\\n\",\n",
      "      \"  - LLMManagerREADME.md\\n\",\n",
      "      \"  - README.md\\n\",\n",
      "      \"  - coverage_stats.txt\\n\",\n",
      "      \"  - docs/USAGE.md\\n\",\n",
      "      \"\\n\",\n",
      "      \"📝 Context preview (first 1000 chars):\\n\",\n",
      "      \"<codebase_tree>\\n\",\n",
      "      \".\\n\",\n",
      "      \"├── .mcp.json\\n\",\n",
      "      \"├── .python-version\\n\",\n",
      "      \"├── CHANGELOG.md\\n\",\n",
      "      \"├── CLAUDE.md\\n\",\n",
      "      \"├── LLMManagerREADME.md\\n\",\n",
      "      \"├── README.md\\n\",\n",
      "      \"├── coverage_stats.txt\\n\",\n",
      "      \"├── pyproject.toml\\n\",\n",
      "      \"├── pyrightconfig.json\\n\",\n",
      "      \"│   ├── workflows/\\n\",\n",
      "      \"│   │   ├── publish.yml\\n\",\n",
      "      \"│   │   └── tests.yml\\n\",\n",
      "      \"├── docs/\\n\",\n",
      "      \"│   ├── USAGE.md\\n\",\n",
      "      \"│   └── coverage_baseline.md\\n\",\n",
      "      \"├── notebooks/\\n\",\n",
      "      \"│   ├── file_structure.ipynb\\n\",\n",
      "      \"│   └── llm_manager.ipynb\\n\",\n",
      "      \"├── yellhorn_mcp/\\n\",\n",
      "      \"│   ├── __init__.py\\n\",\n",
      "      \"│   ├── cli.py\\n\",\n",
      "      \"│   ├── llm_manager.py\\n\",\n",
      "      \"│   ├── metadata_models.py\\n\",\n",
      "      \"│   ├── server.py\\n\",\n",
      "      \"│   └── token_counter.py\\n\",\n",
      "      \"│   ├── formatters/\\n\",\n",
      "      \"│   │   ├── __init__.py\\n\",\n",
      "      \"│   │   ├── codebase_snapshot.py\\n\",\n",
      "      \"│   │   ├── context_fetcher.py\\n\",\n",
      "      \"│   │   └── prompt_formatter.py\\n\",\n",
      "      \"│   ├── integrations/\\n\",\n",
      "      \"│   │   ├── gemini_integration.py\\n\",\n",
      "      \"│   │   └── github_integration.py\\n\",\n",
      "      \"│   ├── models/\\n\",\n",
      "      \"│   │   └── metadata_models.py\\n\",\n",
      "      \"│   ├── processors/\\n\",\n",
      "      \"│   │   ├── __init__.py\\n\",\n",
      "      \"│   │   ├── context_processor.py\\n\",\n",
      "      \"│   │   ├── judgement_processor.py\\n\",\n",
      "      \"│   │   └── workplan_processor.py\\n\",\n",
      "      \"│   ├── utils/\\n\",\n",
      "      \"│   │   ├── comment_utils.py\\n\",\n",
      "      \"│   │   ├─\\n\",\n",
      "      \"\\n\",\n",
      "      \"📝 Log messages (showing first 100):\\n\",\n",
      "      \"  🔵 [info] Getting codebase context using file_structure mode\\n\",\n",
      "      \"  🔵 [info] Getting codebase snapshot in mode: paths\\n\",\n",
      "      \"  🔵 [info] Found .gitignore with 86 patterns\\n\",\n",
      "      \"  🔵 [info] Found .yellhornignore with 1 patterns\\n\",\n",
      "      \"  🔵 [info] File categorization results out of 70 files:\\n\",\n",
      "      \"  🔵 [info]   - 5 always ignored (images, binaries, configs, etc.)\\n\",\n",
      "      \"  🔵 [info]   - 0 in yellhorncontext whitelist (included)\\n\",\n",
      "      \"  🔵 [info]   - 0 in yellhorncontext blacklist (excluded)\\n\",\n",
      "      \"  🔵 [info]   - 0 in yellhornignore whitelist (included)\\n\",\n",
      "      \"  🔵 [info]   - 28 in yellhornignore blacklist (excluded)\\n\",\n",
      "      \"  🔵 [info]   - 37 other files (included - no .yellhorncontext)\\n\",\n",
      "      \"  🔵 [info] Total included: 37 files (excluded 5 always-ignored files)\\n\",\n",
      "      \"  🔵 [info] Codebase context metrics: 49 lines, 432 tokens (gpt-4o-mini)\\n\",\n",
      "      \"  🔵 [info] Extracted 11 directories from 37 filtered files\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"async def test_real_build_context():\\n\",\n",
      "    \"    \\\"\\\"\\\"Test building context from real repository.\\\"\\\"\\\"\\n\",\n",
      "    \"    print(\\\"\\\\n🧪 Testing build_codebase_context with real repository...\\\")\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    # Create mock context\\n\",\n",
      "    \"    mock_ctx = create_mock_context()\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    start_time = datetime.now()\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    # Build context for the repository with mock context\\n\",\n",
      "    \"    directory_context, file_paths, all_dirs = await build_codebase_context(\\n\",\n",
      "    \"        repo_path=REPO_PATH,\\n\",\n",
      "    \"        codebase_reasoning_mode=\\\"file_structure\\\",\\n\",\n",
      "    \"        model=DEFAULT_MODEL,\\n\",\n",
      "    \"        ctx=mock_ctx,\\n\",\n",
      "    \"        git_command_func=mock_ctx.request_context.lifespan_context['git_command_func']\\n\",\n",
      "    \"    )\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    elapsed = (datetime.now() - start_time).total_seconds()\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    print(f\\\"\\\\n⏱️ Time taken: {elapsed:.2f} seconds\\\")\\n\",\n",
      "    \"    print(f\\\"\\\\n📊 Statistics:\\\")\\n\",\n",
      "    \"    print(f\\\"  - Total files found: {len(file_paths)}\\\")\\n\",\n",
      "    \"    print(f\\\"  - Total directories: {len(all_dirs)}\\\")\\n\",\n",
      "    \"    print(f\\\"  - Context size: {len(directory_context)} characters\\\")\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    print(f\\\"\\\\n📁 Top-level directories:\\\")\\n\",\n",
      "    \"    top_dirs = [d for d in sorted(all_dirs) if '/' not in d]\\n\",\n",
      "    \"    for dir_name in top_dirs[:10]:\\n\",\n",
      "    \"        print(f\\\"  - {dir_name}\\\")\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    print(f\\\"\\\\n📄 Sample files (first 10):\\\")\\n\",\n",
      "    \"    for file_path in file_paths[:10]:\\n\",\n",
      "    \"        print(f\\\"  - {file_path}\\\")\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    print(f\\\"\\\\n📝 Context preview (first 1000 chars):\\\")\\n\",\n",
      "    \"    print(directory_context[:1000])\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    # Print log messages\\n\",\n",
      "    \"    print_log_messages(mock_ctx)\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    return directory_context, file_paths, all_dirs, mock_ctx\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Run the test\\n\",\n",
      "    \"if REPO_PATH.exists():\\n\",\n",
      "    \"    context_data = await test_real_build_context()\\n\",\n",
      "    \"else:\\n\",\n",
      "    \"    print(\\\"⚠️ Skipping test - repository path not found\\\")\\n\",\n",
      "    \"    context_data = None\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"## Test 2: Analyze with Real LLM\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 7,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"\\n\",\n",
      "      \"🧪 Testing analyze_with_llm with real LLM...\\n\",\n",
      "      \"🤖 Using model: gpt-4o-mini\\n\",\n",
      "      \"\\n\",\n",
      "      \"📋 Task: Improve the context curation system to better identify important directories for AI workplan generation. Include .python-version\\n\"\n",
      "     ]\n",
      "    },\n",
      "    {\n",
      "     \"name\": \"stderr\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"2025-08-10 15:27:47,958 INFO HTTP Request: POST https://api.openai.com/v1/responses \\\"HTTP/1.1 200 OK\\\"\\n\"\n",
      "     ]\n",
      "    },\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"\\n\",\n",
      "      \"✅ LLM analysis complete!\\n\",\n",
      "      \"⏱️ Time taken: 2.77 seconds\\n\",\n",
      "      \"\\n\",\n",
      "      \"🤖 LLM Response:\\n\",\n",
      "      \"------------------------------------------------------------\\n\",\n",
      "      \"```context\\n\",\n",
      "      \".python-version\\n\",\n",
      "      \"yellhorn_mcp/\\n\",\n",
      "      \"yellhorn_mcp/cli.py\\n\",\n",
      "      \"yellhorn_mcp/llm_manager.py\\n\",\n",
      "      \"yellhorn_mcp/metadata_models.py\\n\",\n",
      "      \"yellhorn_mcp/server.py\\n\",\n",
      "      \"yellhorn_mcp/token_counter.py\\n\",\n",
      "      \"yellhorn_mcp/formatters/\\n\",\n",
      "      \"yellhorn_mcp/integrations/\\n\",\n",
      "      \"yellhorn_mcp/models/\\n\",\n",
      "      \"yellhorn_mcp/processors/\\n\",\n",
      "      \"yellhorn_mcp/utils/\\n\",\n",
      "      \"```\\n\",\n",
      "      \"------------------------------------------------------------\\n\",\n",
      "      \"\\n\",\n",
      "      \"📝 Log messages (showing first 100):\\n\",\n",
      "      \"  🔵 [info] Analyzing directory structure with gpt-4o-mini\\n\",\n",
      "      \"  🔵 [info] [DEBUG] System message: You are an expert software developer tasked with analyzing a codebase structure to identify important directories for building and executing a workplan.\\n\",\n",
      "      \"\\n\",\n",
      "      \"Your goal is to identify the most important directories that should be included for the user's task.\\n\",\n",
      "      \"\\n\",\n",
      "      \"Analyze the directories and identify the ones that:\\n\",\n",
      "      \"1. Contain core application code relevant to the user's task\\n\",\n",
      "      \"2. Likely contain important business logic\\n\",\n",
      "      \"3. Would be essential for understanding the codebase architecture\\n\",\n",
      "      \"4. Are needed to implement the requested task\\n\",\n",
      "      \"5. Contain SDKs or libraries relevant to the user's task\\n\",\n",
      "      \"\\n\",\n",
      "      \"Ignore directories that:\\n\",\n",
      "      \"1. Contain only build artifacts or generated code\\n\",\n",
      "      \"2. Store dependencies or vendor code\\n\",\n",
      "      \"3. Contain temporary or cache files\\n\",\n",
      "      \"4. Probably aren't relevant to the user's specific task\\n\",\n",
      "      \"\\n\",\n",
      "      \"User Task: Improve the context curation system to better identify important directories for AI workplan generation. Include .python-version\\n\",\n",
      "      \"\\n\",\n",
      "      \"Return your analysis as a list of important directories, one per line, without any additional text or formatting as below:\\n\",\n",
      "      \"\\n\",\n",
      "      \"```context\\n\",\n",
      "      \"dir1/subdir1/\\n\",\n",
      "      \"dir2/\\n\",\n",
      "      \"dir3/subdir3/file3.filetype\\n\",\n",
      "      \"```\\n\",\n",
      "      \"\\n\",\n",
      "      \"Prefer to include directories, and not just file paths but include just file paths when appropriate.\\n\",\n",
      "      \"Don't include explanations for your choices, just return the list in the specified format.\\n\",\n",
      "      \"  🔵 [info] [DEBUG] User prompt (1128 chars): <codebase_tree>\\n\",\n",
      "      \".\\n\",\n",
      "      \"├── .mcp.json\\n\",\n",
      "      \"├── .python-version\\n\",\n",
      "      \"├── CHANGELOG.md\\n\",\n",
      "      \"├── CLAUDE.md\\n\",\n",
      "      \"├── LLMManagerREADME.md\\n\",\n",
      "      \"├── README.md\\n\",\n",
      "      \"├── coverage_stats.txt\\n\",\n",
      "      \"├── pyproject.toml\\n\",\n",
      "      \"├── pyrightconfig.json\\n\",\n",
      "      \"│   ├── workflows/\\n\",\n",
      "      \"│   │   ├── publish.yml\\n\",\n",
      "      \"│   │   └── tests.yml\\n\",\n",
      "      \"├── docs/\\n\",\n",
      "      \"│   ├── USAGE.md\\n\",\n",
      "      \"│   └── coverage_baseline.md\\n\",\n",
      "      \"├── notebooks/\\n\",\n",
      "      \"│   ├── file_structure.ipynb\\n\",\n",
      "      \"│   └── llm_manager.ipynb\\n\",\n",
      "      \"├── yellhorn_mcp/\\n\",\n",
      "      \"│   ├── __init__.py\\n\",\n",
      "      \"│   ├── cli.py\\n\",\n",
      "      \"│   ├── llm_manager.py\\n\",\n",
      "      \"│   ├── metadata_models.py\\n\",\n",
      "      \"│   ├── server.py\\n\",\n",
      "      \"│   └── token_counter.py\\n\",\n",
      "      \"│   ├── formatters/\\n\",\n",
      "      \"│   │   ├── __init__.py\\n\",\n",
      "      \"│   │   ├── codebase_snapshot.py\\n\",\n",
      "      \"│   │   ├── context_fetcher.py\\n\",\n",
      "      \"│   │   └── prompt_formatter.py\\n\",\n",
      "      \"│   ├── integrations/\\n\",\n",
      "      \"│   │   ├── gemini_integration.py\\n\",\n",
      "      \"│   │   └── github_integration.py\\n\",\n",
      "      \"│   ├── models/\\n\",\n",
      "      \"│   │   └── metadata_models.py\\n\",\n",
      "      \"│   ├── processors/\\n\",\n",
      "      \"│   │   ├── __init__.py\\n\",\n",
      "      \"│   │   ├── context_processor.py\\n\",\n",
      "      \"│   │   ├── judgement_processor.py\\n\",\n",
      "      \"│   │   └── workplan_processor.py\\n\",\n",
      "      \"│   ├── utils/\\n\",\n",
      "      \"│   │   ├── comment_utils.py\\n\",\n",
      "      \"│   │   ├── cost_tracker_utils.py\\n\",\n",
      "      \"│   │   ├── git_utils.py\\n\",\n",
      "      \"│   │   ├── lsp_utils.py\\n\",\n",
      "      \"│   │   └── search_grounding_utils.py\\n\",\n",
      "      \"</codebase_tree>...\\n\",\n",
      "      \"\\n\",\n",
      "      \"📊 Token Usage:\\n\",\n",
      "      \"  - Prompt tokens: 702\\n\",\n",
      "      \"  - Completion tokens: 101\\n\",\n",
      "      \"  - Total tokens: 803\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"async def test_real_llm_analysis():\\n\",\n",
      "    \"    \\\"\\\"\\\"Test analyzing codebase with real LLM.\\\"\\\"\\\"\\n\",\n",
      "    \"    if not DEFAULT_MODEL or not context_data:\\n\",\n",
      "    \"        print(\\\"⚠️ Skipping test - LLM or context not available\\\")\\n\",\n",
      "    \"        return None\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    print(\\\"\\\\n🧪 Testing analyze_with_llm with real LLM...\\\")\\n\",\n",
      "    \"    print(f\\\"🤖 Using model: {DEFAULT_MODEL}\\\")\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    # Create new mock context for this test\\n\",\n",
      "    \"    mock_ctx = create_mock_context()\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    directory_context = context_data[0]\\n\",\n",
      "    \"    user_task = \\\"Improve the context curation system to better identify important directories for AI workplan generation. Include .python-version\\\"\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    print(f\\\"\\\\n📋 Task: {user_task}\\\")    \\n\",\n",
      "    \"    start_time = datetime.now()\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    try:\\n\",\n",
      "    \"        # Call real LLM with mock context\\n\",\n",
      "    \"        llm_result = await analyze_with_llm(\\n\",\n",
      "    \"            llm_manager=llm_manager,\\n\",\n",
      "    \"            model=DEFAULT_MODEL,\\n\",\n",
      "    \"            directory_context=directory_context,\\n\",\n",
      "    \"            user_task=user_task,\\n\",\n",
      "    \"            debug=True,  # Set to True to see debug logs\\n\",\n",
      "    \"            ctx=mock_ctx\\n\",\n",
      "    \"        )\\n\",\n",
      "    \"        \\n\",\n",
      "    \"        elapsed = (datetime.now() - start_time).total_seconds()\\n\",\n",
      "    \"        \\n\",\n",
      "    \"        print(f\\\"\\\\n✅ LLM analysis complete!\\\")\\n\",\n",
      "    \"        print(f\\\"⏱️ Time taken: {elapsed:.2f} seconds\\\")\\n\",\n",
      "    \"        print(f\\\"\\\\n🤖 LLM Response:\\\")\\n\",\n",
      "    \"        print(\\\"-\\\" * 60)\\n\",\n",
      "    \"        print(llm_result)\\n\",\n",
      "    \"        print(\\\"-\\\" * 60)\\n\",\n",
      "    \"        \\n\",\n",
      "    \"        # Print log messages\\n\",\n",
      "    \"        print_log_messages(mock_ctx)\\n\",\n",
      "    \"        \\n\",\n",
      "    \"        # Check for usage metadata\\n\",\n",
      "    \"        usage = llm_manager.get_last_usage_metadata()\\n\",\n",
      "    \"        if usage:\\n\",\n",
      "    \"            print(f\\\"\\\\n📊 Token Usage:\\\")\\n\",\n",
      "    \"            print(f\\\"  - Prompt tokens: {usage.prompt_tokens}\\\")\\n\",\n",
      "    \"            print(f\\\"  - Completion tokens: {usage.completion_tokens}\\\")\\n\",\n",
      "    \"            print(f\\\"  - Total tokens: {usage.total_tokens}\\\")\\n\",\n",
      "    \"        \\n\",\n",
      "    \"        return llm_result, mock_ctx\\n\",\n",
      "    \"        \\n\",\n",
      "    \"    except Exception as e:\\n\",\n",
      "    \"        print(f\\\"\\\\n❌ Error calling LLM: {e}\\\")\\n\",\n",
      "    \"        print_log_messages(mock_ctx)\\n\",\n",
      "    \"        return None, mock_ctx\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Run the test\\n\",\n",
      "    \"llm_data = await test_real_llm_analysis()\\n\",\n",
      "    \"if llm_data:\\n\",\n",
      "    \"    llm_result, llm_ctx = llm_data\\n\",\n",
      "    \"else:\\n\",\n",
      "    \"    llm_result = None\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"## Test 3: Parse LLM Output\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 12,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"\\n\",\n",
      "      \"🧪 Testing parse_llm_directories with real output...\\n\",\n",
      "      \"\\n\",\n",
      "      \"📊 Parsing Results:\\n\",\n",
      "      \"  - Total directories available: 11\\n\",\n",
      "      \"  - Important directories identified: 12\\n\",\n",
      "      \"  - [SHOULD HAPPEN] Reduction: -9.1%\\n\",\n",
      "      \"\\n\",\n",
      "      \"📁 Important directories identified:\\n\",\n",
      "      \"  - .python-version\\n\",\n",
      "      \"  - yellhorn_mcp\\n\",\n",
      "      \"  - yellhorn_mcp/cli.py\\n\",\n",
      "      \"  - yellhorn_mcp/formatters\\n\",\n",
      "      \"  - yellhorn_mcp/integrations\\n\",\n",
      "      \"  - yellhorn_mcp/llm_manager.py\\n\",\n",
      "      \"  - yellhorn_mcp/metadata_models.py\\n\",\n",
      "      \"  - yellhorn_mcp/models\\n\",\n",
      "      \"  - yellhorn_mcp/processors\\n\",\n",
      "      \"  - yellhorn_mcp/server.py\\n\",\n",
      "      \"  - yellhorn_mcp/token_counter.py\\n\",\n",
      "      \"  - yellhorn_mcp/utils\\n\",\n",
      "      \"\\n\",\n",
      "      \"🔍 Selection Analysis:\\n\",\n",
      "      \"  ✅ yellhorn_mcp - included\\n\",\n",
      "      \"  ✅ yellhorn_mcp/processors - included\\n\",\n",
      "      \"  ✅ yellhorn_mcp/formatters - included\\n\",\n",
      "      \"  ⚠️ tests - not found in repository\\n\",\n",
      "      \"\\n\",\n",
      "      \"📝 Log messages (showing first 100):\\n\",\n",
      "      \"  🔵 [info] Matched '.python-version' to directories: .python-version\\n\",\n",
      "      \"  🔵 [info] Matched 'yellhorn_mcp' to directories: yellhorn_mcp\\n\",\n",
      "      \"  🔵 [info] Matched 'yellhorn_mcp/cli.py' to directories: yellhorn_mcp/cli.py\\n\",\n",
      "      \"  🔵 [info] Matched 'yellhorn_mcp/llm_manager.py' to directories: yellhorn_mcp/llm_manager.py\\n\",\n",
      "      \"  🔵 [info] Matched 'yellhorn_mcp/metadata_models.py' to directories: yellhorn_mcp/metadata_models.py\\n\",\n",
      "      \"  🔵 [info] Matched 'yellhorn_mcp/server.py' to directories: yellhorn_mcp/server.py\\n\",\n",
      "      \"  🔵 [info] Matched 'yellhorn_mcp/token_counter.py' to directories: yellhorn_mcp/token_counter.py\\n\",\n",
      "      \"  🔵 [info] Matched 'yellhorn_mcp/formatters' to directories: yellhorn_mcp/formatters\\n\",\n",
      "      \"  🔵 [info] Matched 'yellhorn_mcp/integrations' to directories: yellhorn_mcp/integrations\\n\",\n",
      "      \"  🔵 [info] Matched 'yellhorn_mcp/models' to directories: yellhorn_mcp/models\\n\",\n",
      "      \"  🔵 [info] Matched 'yellhorn_mcp/processors' to directories: yellhorn_mcp/processors\\n\",\n",
      "      \"  🔵 [info] Matched 'yellhorn_mcp/utils' to directories: yellhorn_mcp/utils\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"async def test_parse_real_output():\\n\",\n",
      "    \"    \\\"\\\"\\\"Test parsing real LLM output.\\\"\\\"\\\"\\n\",\n",
      "    \"    if not llm_result or not context_data:\\n\",\n",
      "    \"        print(\\\"⚠️ Skipping test - LLM result not available\\\")\\n\",\n",
      "    \"        return None\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    print(\\\"\\\\n🧪 Testing parse_llm_directories with real output...\\\")\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    # Create new mock context for this test\\n\",\n",
      "    \"    mock_ctx = create_mock_context()\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    all_dirs = context_data[2]\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    # Parse the LLM output with mock context\\n\",\n",
      "    \"    important_dirs = await parse_llm_directories(\\n\",\n",
      "    \"        llm_result=llm_result,\\n\",\n",
      "    \"        all_dirs=all_dirs,\\n\",\n",
      "    \"        ctx=mock_ctx\\n\",\n",
      "    \"    )\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    print(f\\\"\\\\n📊 Parsing Results:\\\")\\n\",\n",
      "    \"    print(f\\\"  - Total directories available: {len(all_dirs)}\\\")\\n\",\n",
      "    \"    print(f\\\"  - Important directories identified: {len(important_dirs)}\\\")\\n\",\n",
      "    \"    print(f\\\"  - [SHOULD HAPPEN] Reduction: {100 * (1 - len(important_dirs)/len(all_dirs)):.1f}%\\\")\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    print(f\\\"\\\\n📁 Important directories identified:\\\")\\n\",\n",
      "    \"    for dir_name in sorted(important_dirs)[:20]:\\n\",\n",
      "    \"        print(f\\\"  - {dir_name}\\\")\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    if len(important_dirs) > 20:\\n\",\n",
      "    \"        print(f\\\"  ... and {len(important_dirs) - 20} more\\\")\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    # Analyze the selection\\n\",\n",
      "    \"    print(f\\\"\\\\n🔍 Selection Analysis:\\\")\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    # Check if key directories were selected\\n\",\n",
      "    \"    key_dirs = [\\\"yellhorn_mcp\\\", \\\"yellhorn_mcp/processors\\\", \\\"yellhorn_mcp/formatters\\\", \\\"tests\\\"]\\n\",\n",
      "    \"    for key_dir in key_dirs:\\n\",\n",
      "    \"        if key_dir in important_dirs:\\n\",\n",
      "    \"            print(f\\\"  ✅ {key_dir} - included\\\")\\n\",\n",
      "    \"        elif key_dir in all_dirs:\\n\",\n",
      "    \"            print(f\\\"  ❌ {key_dir} - excluded\\\")\\n\",\n",
      "    \"        else:\\n\",\n",
      "    \"            print(f\\\"  ⚠️ {key_dir} - not found in repository\\\")\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    # Print log messages\\n\",\n",
      "    \"    print_log_messages(mock_ctx)\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    return important_dirs, mock_ctx\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Run the test\\n\",\n",
      "    \"parse_data = await test_parse_real_output()\\n\",\n",
      "    \"if parse_data:\\n\",\n",
      "    \"    parsed_dirs, parse_ctx = parse_data\\n\",\n",
      "    \"else:\\n\",\n",
      "    \"    parsed_dirs = None\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 13,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"data\": {\n",
      "      \"text/plain\": [\n",
      "       \"{'.',\\n\",\n",
      "       \" '.github',\\n\",\n",
      "       \" '.github/workflows',\\n\",\n",
      "       \" 'docs',\\n\",\n",
      "       \" 'notebooks',\\n\",\n",
      "       \" 'yellhorn_mcp',\\n\",\n",
      "       \" 'yellhorn_mcp/formatters',\\n\",\n",
      "       \" 'yellhorn_mcp/integrations',\\n\",\n",
      "       \" 'yellhorn_mcp/models',\\n\",\n",
      "       \" 'yellhorn_mcp/processors',\\n\",\n",
      "       \" 'yellhorn_mcp/utils'}\"\n",
      "      ]\n",
      "     },\n",
      "     \"execution_count\": 13,\n",
      "     \"metadata\": {},\n",
      "     \"output_type\": \"execute_result\"\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"context_data[2]\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 14,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"```context\\n\",\n",
      "      \".python-version\\n\",\n",
      "      \"yellhorn_mcp/\\n\",\n",
      "      \"yellhorn_mcp/cli.py\\n\",\n",
      "      \"yellhorn_mcp/llm_manager.py\\n\",\n",
      "      \"yellhorn_mcp/metadata_models.py\\n\",\n",
      "      \"yellhorn_mcp/server.py\\n\",\n",
      "      \"yellhorn_mcp/token_counter.py\\n\",\n",
      "      \"yellhorn_mcp/formatters/\\n\",\n",
      "      \"yellhorn_mcp/integrations/\\n\",\n",
      "      \"yellhorn_mcp/models/\\n\",\n",
      "      \"yellhorn_mcp/processors/\\n\",\n",
      "      \"yellhorn_mcp/utils/\\n\",\n",
      "      \"```\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"print(llm_result)\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 15,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"{'.python-version', 'yellhorn_mcp', 'yellhorn_mcp/cli.py', 'yellhorn_mcp/formatters', 'yellhorn_mcp/integrations', 'yellhorn_mcp/models', 'yellhorn_mcp/utils', 'yellhorn_mcp/llm_manager.py', 'yellhorn_mcp/processors', 'yellhorn_mcp/metadata_models.py', 'yellhorn_mcp/server.py', 'yellhorn_mcp/token_counter.py'}\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"print(parsed_dirs)\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"## Test 4: End-to-End Context Curation\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 17,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"\\n\",\n",
      "      \"🧪 Testing end-to-end context curation...\\n\",\n",
      "      \"📁 Repository: /Users/sravanj/project_work/yellhorn-mcp\\n\",\n",
      "      \"🤖 Model: gpt-4o-mini\\n\",\n",
      "      \"\\n\",\n",
      "      \"📋 Task: Refactor the context processor module to improve modularity and add better error handling\\n\",\n",
      "      \"📄 Output file: .yellhorncontext.test\\n\"\n",
      "     ]\n",
      "    },\n",
      "    {\n",
      "     \"name\": \"stderr\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"2025-08-10 15:35:37,264 INFO HTTP Request: POST https://api.openai.com/v1/responses \\\"HTTP/1.1 200 OK\\\"\\n\"\n",
      "     ]\n",
      "    },\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"\\n\",\n",
      "      \"✅ Context curation complete!\\n\",\n",
      "      \"⏱️ Time taken: 1.42 seconds\\n\",\n",
      "      \"\\n\",\n",
      "      \"📊 Result: Successfully created .yellhorncontext file at /Users/sravanj/project_work/yellhorn-mcp/.yellhorncontext.test with 3 important directories.\\n\",\n",
      "      \"\\n\",\n",
      "      \"📄 Generated context file content:\\n\",\n",
      "      \"============================================================\\n\",\n",
      "      \"# Yellhorn Context File - AI context optimization\\n\",\n",
      "      \"# Generated by yellhorn-mcp curate_context tool\\n\",\n",
      "      \"# Based on task: Refactor the context processor module to improve modularity and add better error\\n\",\n",
      "      \"\\n\",\n",
      "      \"# Important directories to specifically include\\n\",\n",
      "      \"yellhorn_mcp/\\n\",\n",
      "      \"yellhorn_mcp/processors/\\n\",\n",
      "      \"yellhorn_mcp/utils/\\n\",\n",
      "      \"\\n\",\n",
      "      \"============================================================\\n\",\n",
      "      \"\\n\",\n",
      "      \"📊 File Analysis:\\n\",\n",
      "      \"  - Total lines: 9\\n\",\n",
      "      \"  - Comment lines: 6\\n\",\n",
      "      \"  - Directory patterns: 3\\n\",\n",
      "      \"\\n\",\n",
      "      \"📝 Log messages (showing first 100):\\n\",\n",
      "      \"  🔵 [info] Starting context curation process\\n\",\n",
      "      \"  🔵 [info] Getting codebase context using file_structure mode\\n\",\n",
      "      \"  🔵 [info] Getting codebase snapshot in mode: paths\\n\",\n",
      "      \"  🔵 [info] Found .gitignore with 86 patterns\\n\",\n",
      "      \"  🔵 [info] Found .yellhornignore with 1 patterns\\n\",\n",
      "      \"  🔵 [info] File categorization results out of 70 files:\\n\",\n",
      "      \"  🔵 [info]   - 5 always ignored (images, binaries, configs, etc.)\\n\",\n",
      "      \"  🔵 [info]   - 0 in yellhorncontext whitelist (included)\\n\",\n",
      "      \"  🔵 [info]   - 0 in yellhorncontext blacklist (excluded)\\n\",\n",
      "      \"  🔵 [info]   - 0 in yellhornignore whitelist (included)\\n\",\n",
      "      \"  🔵 [info]   - 28 in yellhornignore blacklist (excluded)\\n\",\n",
      "      \"  🔵 [info]   - 37 other files (included - no .yellhorncontext)\\n\",\n",
      "      \"  🔵 [info] Total included: 37 files (excluded 5 always-ignored files)\\n\",\n",
      "      \"  🔵 [info] Codebase context metrics: 49 lines, 432 tokens (gpt-4o-mini)\\n\",\n",
      "      \"  🔵 [info] Extracted 11 directories from 37 filtered files\\n\",\n",
      "      \"  🔵 [info] Directory context:\\n\",\n",
      "      \"<codebase_tree>\\n\",\n",
      "      \".\\n\",\n",
      "      \"├── .mcp.json\\n\",\n",
      "      \"├── .python-version\\n\",\n",
      "      \"├── CHANGELOG.md\\n\",\n",
      "      \"├── CLAUDE.md\\n\",\n",
      "      \"├── LLMManagerREADME.md\\n\",\n",
      "      \"├── README.md\\n\",\n",
      "      \"├── coverage_stats.txt\\n\",\n",
      "      \"├── pyproject.toml\\n\",\n",
      "      \"├── pyrightconfig.json\\n\",\n",
      "      \"│   ├── workflows/\\n\",\n",
      "      \"│   │   ├── publish.yml\\n\",\n",
      "      \"│   │   └── tests.yml\\n\",\n",
      "      \"├── docs/\\n\",\n",
      "      \"│   ├── USAGE.md\\n\",\n",
      "      \"│   └── coverage_baseline.md\\n\",\n",
      "      \"├── notebooks/\\n\",\n",
      "      \"│   ├── file_structure.ipynb\\n\",\n",
      "      \"│   └── llm_manager.ipynb\\n\",\n",
      "      \"├── yellhorn_mcp/\\n\",\n",
      "      \"│   ├── __init__.py\\n\",\n",
      "      \"│   ├── cli.py\\n\",\n",
      "      \"│   ├── llm_manager.py\\n\",\n",
      "      \"│   ├── metadata_models.py\\n\",\n",
      "      \"│   ├── server.py\\n\",\n",
      "      \"│ ...\\n\",\n",
      "      \"  🔵 [info] Analyzing directory structure with gpt-4o-mini\\n\",\n",
      "      \"  🔵 [info] Matched 'yellhorn_mcp/processors' to directories: yellhorn_mcp/processors\\n\",\n",
      "      \"  🔵 [info] Matched 'yellhorn_mcp' to directories: yellhorn_mcp\\n\",\n",
      "      \"  🔵 [info] Matched 'yellhorn_mcp/utils' to directories: yellhorn_mcp/utils\\n\",\n",
      "      \"  🔵 [info] Analysis complete, found 3 important directories: yellhorn_mcp, yellhorn_mcp/processors, yellhorn_mcp/utils\\n\",\n",
      "      \"  🔵 [info] Processing complete, identified 3 important directories\\n\",\n",
      "      \"  🔵 [info] Successfully wrote .yellhorncontext file to /Users/sravanj/project_work/yellhorn-mcp/.yellhorncontext.test\\n\",\n",
      "      \"\\n\",\n",
      "      \"🧹 Cleaning up test file...\\n\",\n",
      "      \"✅ Test file removed\\n\",\n",
      "      \"\\n\",\n",
      "      \"📊 Total Token Usage:\\n\",\n",
      "      \"  - Prompt tokens: 696\\n\",\n",
      "      \"  - Completion tokens: 26\\n\",\n",
      "      \"  - Total tokens: 722\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"async def test_end_to_end_real():\\n\",\n",
      "    \"    \\\"\\\"\\\"Test complete context curation with real repository and LLM.\\\"\\\"\\\"\\n\",\n",
      "    \"    if not DEFAULT_MODEL or not REPO_PATH.exists():\\n\",\n",
      "    \"        print(\\\"⚠️ Skipping test - LLM or repository not available\\\")\\n\",\n",
      "    \"        return None\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    print(\\\"\\\\n🧪 Testing end-to-end context curation...\\\")\\n\",\n",
      "    \"    print(f\\\"📁 Repository: {REPO_PATH}\\\")\\n\",\n",
      "    \"    print(f\\\"🤖 Model: {DEFAULT_MODEL}\\\")\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    # Create mock context for this test\\n\",\n",
      "    \"    mock_ctx = create_mock_context()\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    user_task = \\\"Refactor the context processor module to improve modularity and add better error handling\\\"\\n\",\n",
      "    \"    output_path = \\\".yellhorncontext.test\\\"\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    print(f\\\"\\\\n📋 Task: {user_task}\\\")\\n\",\n",
      "    \"    print(f\\\"📄 Output file: {output_path}\\\")\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    start_time = datetime.now()\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    try:\\n\",\n",
      "    \"        # Run the complete process with mock context\\n\",\n",
      "    \"        result = await process_context_curation_async(\\n\",\n",
      "    \"            repo_path=REPO_PATH,\\n\",\n",
      "    \"            llm_manager=llm_manager,\\n\",\n",
      "    \"            model=DEFAULT_MODEL,\\n\",\n",
      "    \"            user_task=user_task,\\n\",\n",
      "    \"            output_path=output_path,\\n\",\n",
      "    \"            codebase_reasoning=\\\"file_structure\\\",\\n\",\n",
      "    \"            disable_search_grounding=False,\\n\",\n",
      "    \"            debug=False,\\n\",\n",
      "    \"            ctx=mock_ctx\\n\",\n",
      "    \"        )\\n\",\n",
      "    \"        \\n\",\n",
      "    \"        elapsed = (datetime.now() - start_time).total_seconds()\\n\",\n",
      "    \"        \\n\",\n",
      "    \"        print(f\\\"\\\\n✅ Context curation complete!\\\")\\n\",\n",
      "    \"        print(f\\\"⏱️ Time taken: {elapsed:.2f} seconds\\\")\\n\",\n",
      "    \"        print(f\\\"\\\\n📊 Result: {result}\\\")\\n\",\n",
      "    \"        \\n\",\n",
      "    \"        # Read and display the generated file\\n\",\n",
      "    \"        context_file = REPO_PATH / output_path\\n\",\n",
      "    \"        if context_file.exists():\\n\",\n",
      "    \"            content = context_file.read_text()\\n\",\n",
      "    \"            print(f\\\"\\\\n📄 Generated context file content:\\\")\\n\",\n",
      "    \"            print(\\\"=\\\" * 60)\\n\",\n",
      "    \"            print(content)\\n\",\n",
      "    \"            print(\\\"=\\\" * 60)\\n\",\n",
      "    \"            \\n\",\n",
      "    \"            # Analyze the content\\n\",\n",
      "    \"            lines = content.split('\\\\n')\\n\",\n",
      "    \"            non_comment_lines = [l for l in lines if l.strip() and not l.strip().startswith('#')]\\n\",\n",
      "    \"            \\n\",\n",
      "    \"            print(f\\\"\\\\n📊 File Analysis:\\\")\\n\",\n",
      "    \"            print(f\\\"  - Total lines: {len(lines)}\\\")\\n\",\n",
      "    \"            print(f\\\"  - Comment lines: {len(lines) - len(non_comment_lines)}\\\")\\n\",\n",
      "    \"            print(f\\\"  - Directory patterns: {len(non_comment_lines)}\\\")\\n\",\n",
      "    \"            \\n\",\n",
      "    \"            # Print log messages\\n\",\n",
      "    \"            print_log_messages(mock_ctx)\\n\",\n",
      "    \"            \\n\",\n",
      "    \"            # Clean up test file\\n\",\n",
      "    \"            print(f\\\"\\\\n🧹 Cleaning up test file...\\\")\\n\",\n",
      "    \"            context_file.unlink()\\n\",\n",
      "    \"            print(f\\\"✅ Test file removed\\\")\\n\",\n",
      "    \"        else:\\n\",\n",
      "    \"            print(f\\\"\\\\n❌ Context file was not created\\\")\\n\",\n",
      "    \"            print_log_messages(mock_ctx)\\n\",\n",
      "    \"        \\n\",\n",
      "    \"        # Check usage\\n\",\n",
      "    \"        usage = llm_manager.get_last_usage_metadata()\\n\",\n",
      "    \"        if usage:\\n\",\n",
      "    \"            print(f\\\"\\\\n📊 Total Token Usage:\\\")\\n\",\n",
      "    \"            print(f\\\"  - Prompt tokens: {usage.prompt_tokens}\\\")\\n\",\n",
      "    \"            print(f\\\"  - Completion tokens: {usage.completion_tokens}\\\")\\n\",\n",
      "    \"            print(f\\\"  - Total tokens: {usage.total_tokens}\\\")\\n\",\n",
      "    \"        \\n\",\n",
      "    \"        return result, mock_ctx\\n\",\n",
      "    \"        \\n\",\n",
      "    \"    except Exception as e:\\n\",\n",
      "    \"        print(f\\\"\\\\n❌ Error during context curation: {e}\\\")\\n\",\n",
      "    \"        import traceback\\n\",\n",
      "    \"        traceback.print_exc()\\n\",\n",
      "    \"        print_log_messages(mock_ctx)\\n\",\n",
      "    \"        return None, mock_ctx\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Run the test\\n\",\n",
      "    \"end_to_end_data = await test_end_to_end_real()\\n\",\n",
      "    \"if end_to_end_data:\\n\",\n",
      "    \"    end_to_end_result, end_to_end_ctx = end_to_end_data\\n\",\n",
      "    \"else:\\n\",\n",
      "    \"    end_to_end_result = None\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"## Test Different Reasoning Modes\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 19,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"\\n\",\n",
      "      \"🧪 Testing different reasoning modes...\\n\",\n",
      "      \"\\n\",\n",
      "      \"📋 Testing mode: file_structure\\n\",\n",
      "      \"  ✅ Completed in 0.03s\\n\",\n",
      "      \"  📊 Context size: 1128 chars\\n\",\n",
      "      \"  📝 Log messages: 14\\n\",\n",
      "      \"  Sample logs:\\n\",\n",
      "      \"    - [info] Getting codebase context using file_structure mode...\\n\",\n",
      "      \"    - [info] Getting codebase snapshot in mode: paths...\\n\",\n",
      "      \"    - [info] Found .gitignore with 86 patterns...\\n\",\n",
      "      \"\\n\",\n",
      "      \"📋 Testing mode: lsp\\n\",\n",
      "      \"  ✅ Completed in 0.04s\\n\",\n",
      "      \"  📊 Context size: 17126 chars\\n\",\n",
      "      \"  📝 Log messages: 14\\n\",\n",
      "      \"  Sample logs:\\n\",\n",
      "      \"    - [info] Getting codebase context using lsp mode...\\n\",\n",
      "      \"    - [info] Getting codebase snapshot in mode: paths...\\n\",\n",
      "      \"    - [info] Found .gitignore with 86 patterns...\\n\",\n",
      "      \"\\n\",\n",
      "      \"📋 Testing mode: full\\n\",\n",
      "      \"  ✅ Completed in 0.08s\\n\",\n",
      "      \"  📊 Context size: 407588 chars\\n\",\n",
      "      \"  📝 Log messages: 15\\n\",\n",
      "      \"  Sample logs:\\n\",\n",
      "      \"    - [info] Getting codebase context using full mode...\\n\",\n",
      "      \"    - [info] Getting codebase snapshot in mode: full...\\n\",\n",
      "      \"    - [info] Found .gitignore with 86 patterns...\\n\",\n",
      "      \"\\n\",\n",
      "      \"📊 Comparison of reasoning modes:\\n\",\n",
      "      \"Mode            Time (s)   Context Size    Files      Dirs       Logs      \\n\",\n",
      "      \"----------------------------------------------------------------------\\n\",\n",
      "      \"file_structure  0.03       1,128           37         11         14        \\n\",\n",
      "      \"lsp             0.04       17,126          37         11         14        \\n\",\n",
      "      \"full            0.08       407,588         37         11         15        \\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"async def test_reasoning_modes():\\n\",\n",
      "    \"    \\\"\\\"\\\"Test different codebase reasoning modes.\\\"\\\"\\\"\\n\",\n",
      "    \"    if not REPO_PATH.exists():\\n\",\n",
      "    \"        print(\\\"⚠️ Skipping test - repository not available\\\")\\n\",\n",
      "    \"        return\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    print(\\\"\\\\n🧪 Testing different reasoning modes...\\\")\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    modes = [\\\"file_structure\\\", \\\"lsp\\\", \\\"full\\\"]\\n\",\n",
      "    \"    results = {}\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    for mode in modes:\\n\",\n",
      "    \"        print(f\\\"\\\\n📋 Testing mode: {mode}\\\")\\n\",\n",
      "    \"        \\n\",\n",
      "    \"        # Create new mock context for each mode\\n\",\n",
      "    \"        mock_ctx = create_mock_context()\\n\",\n",
      "    \"        mock_ctx.request_context.lifespan_context[\\\"codebase_reasoning\\\"] = mode\\n\",\n",
      "    \"        \\n\",\n",
      "    \"        start_time = datetime.now()\\n\",\n",
      "    \"        \\n\",\n",
      "    \"        try:\\n\",\n",
      "    \"            directory_context, file_paths, all_dirs = await build_codebase_context(\\n\",\n",
      "    \"                repo_path=REPO_PATH,\\n\",\n",
      "    \"                codebase_reasoning_mode=mode,\\n\",\n",
      "    \"                model=DEFAULT_MODEL,\\n\",\n",
      "    \"                ctx=mock_ctx,\\n\",\n",
      "    \"                git_command_func=mock_ctx.request_context.lifespan_context['git_command_func']\\n\",\n",
      "    \"            )\\n\",\n",
      "    \"            \\n\",\n",
      "    \"            elapsed = (datetime.now() - start_time).total_seconds()\\n\",\n",
      "    \"            \\n\",\n",
      "    \"            results[mode] = {\\n\",\n",
      "    \"                \\\"time\\\": elapsed,\\n\",\n",
      "    \"                \\\"context_size\\\": len(directory_context),\\n\",\n",
      "    \"                \\\"files\\\": len(file_paths),\\n\",\n",
      "    \"                \\\"dirs\\\": len(all_dirs),\\n\",\n",
      "    \"                \\\"logs\\\": mock_ctx.log.call_count\\n\",\n",
      "    \"            }\\n\",\n",
      "    \"            \\n\",\n",
      "    \"            print(f\\\"  ✅ Completed in {elapsed:.2f}s\\\")\\n\",\n",
      "    \"            print(f\\\"  📊 Context size: {len(directory_context)} chars\\\")\\n\",\n",
      "    \"            print(f\\\"  📝 Log messages: {mock_ctx.log.call_count}\\\")\\n\",\n",
      "    \"            \\n\",\n",
      "    \"            # Show sample log messages\\n\",\n",
      "    \"            if mock_ctx.log.called:\\n\",\n",
      "    \"                print(\\\"  Sample logs:\\\")\\n\",\n",
      "    \"                for i, call in enumerate(mock_ctx.log.call_args_list[:3]):\\n\",\n",
      "    \"                    level = call[1].get('level', 'info')\\n\",\n",
      "    \"                    msg = call[1]['message'][:80]\\n\",\n",
      "    \"                    print(f\\\"    - [{level}] {msg}...\\\")\\n\",\n",
      "    \"            \\n\",\n",
      "    \"        except Exception as e:\\n\",\n",
      "    \"            print(f\\\"  ❌ Error: {e}\\\")\\n\",\n",
      "    \"            results[mode] = {\\\"error\\\": str(e)}\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    # Compare results\\n\",\n",
      "    \"    print(\\\"\\\\n📊 Comparison of reasoning modes:\\\")\\n\",\n",
      "    \"    print(f\\\"{'Mode':<15} {'Time (s)':<10} {'Context Size':<15} {'Files':<10} {'Dirs':<10} {'Logs':<10}\\\")\\n\",\n",
      "    \"    print(\\\"-\\\" * 70)\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    for mode, data in results.items():\\n\",\n",
      "    \"        if \\\"error\\\" not in data:\\n\",\n",
      "    \"            print(f\\\"{mode:<15} {data['time']:<10.2f} {data['context_size']:<15,} {data['files']:<10} {data['dirs']:<10} {data['logs']:<10}\\\")\\n\",\n",
      "    \"        else:\\n\",\n",
      "    \"            print(f\\\"{mode:<15} Error: {data['error'][:40]}\\\")\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    return results\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Run the test\\n\",\n",
      "    \"reasoning_results = await test_reasoning_modes()\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"## Summary and Recommendations\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"## Test with Debug Mode Enabled\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"code\",\n",
      "   \"execution_count\": 21,\n",
      "   \"metadata\": {},\n",
      "   \"outputs\": [\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"\\n\",\n",
      "      \"🧪 Testing with debug mode enabled...\\n\",\n",
      "      \"📁 Repository: /Users/sravanj/project_work/yellhorn-mcp\\n\",\n",
      "      \"🤖 Model: gpt-4o-mini\\n\",\n",
      "      \"\\n\",\n",
      "      \"📋 Task: Test debug logging functionality\\n\",\n",
      "      \"\\n\",\n",
      "      \"1️⃣ Building codebase context...\\n\",\n",
      "      \"   ✅ Context built: 1128 chars, 37 files\\n\",\n",
      "      \"\\n\",\n",
      "      \"2️⃣ Analyzing with LLM (debug=True)...\\n\"\n",
      "     ]\n",
      "    },\n",
      "    {\n",
      "     \"name\": \"stderr\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"2025-08-10 15:37:51,726 INFO HTTP Request: POST https://api.openai.com/v1/responses \\\"HTTP/1.1 200 OK\\\"\\n\"\n",
      "     ]\n",
      "    },\n",
      "    {\n",
      "     \"name\": \"stdout\",\n",
      "     \"output_type\": \"stream\",\n",
      "     \"text\": [\n",
      "      \"   ✅ LLM analysis complete\\n\",\n",
      "      \"\\n\",\n",
      "      \"3️⃣ Parsing LLM output...\\n\",\n",
      "      \"   ✅ Parsed 8 important directories\\n\",\n",
      "      \"{'yellhorn_mcp/utils', 'yellhorn_mcp', 'yellhorn_mcp/llm_manager.py', 'yellhorn_mcp/processors', 'yellhorn_mcp/cli.py', 'yellhorn_mcp/formatters', 'yellhorn_mcp/integrations', 'yellhorn_mcp/server.py'}\\n\",\n",
      "      \"\\n\",\n",
      "      \"📝 Complete Log Messages:\\n\",\n",
      "      \"============================================================\\n\",\n",
      "      \"  1. 🔵 [info   ] Getting codebase context using file_structure mode\\n\",\n",
      "      \"  2. 🔵 [info   ] Getting codebase snapshot in mode: paths\\n\",\n",
      "      \"  3. 🔵 [info   ] Found .gitignore with 86 patterns\\n\",\n",
      "      \"  4. 🔵 [info   ] Found .yellhornignore with 1 patterns\\n\",\n",
      "      \"  5. 🔵 [info   ] File categorization results out of 70 files:\\n\",\n",
      "      \"  6. 🔵 [info   ]   - 5 always ignored (images, binaries, configs, etc.)\\n\",\n",
      "      \"  7. 🔵 [info   ]   - 0 in yellhorncontext whitelist (included)\\n\",\n",
      "      \"  8. 🔵 [info   ]   - 0 in yellhorncontext blacklist (excluded)\\n\",\n",
      "      \"  9. 🔵 [info   ]   - 0 in yellhornignore whitelist (included)\\n\",\n",
      "      \" 10. 🔵 [info   ]   - 28 in yellhornignore blacklist (excluded)\\n\",\n",
      "      \" 11. 🔵 [info   ]   - 37 other files (included - no .yellhorncontext)\\n\",\n",
      "      \" 12. 🔵 [info   ] Total included: 37 files (excluded 5 always-ignored files)\\n\",\n",
      "      \" 13. 🔵 [info   ] Codebase context metrics: 49 lines, 432 tokens (gpt-4o-mini)\\n\",\n",
      "      \" 14. 🔵 [info   ] Extracted 11 directories from 37 filtered files\\n\",\n",
      "      \" 15. 🔵 [info   ] Analyzing directory structure with gpt-4o-mini\\n\",\n",
      "      \" 16. 🔵 [info   ] [DEBUG] System message: You are an expert software developer tasked with analyzing a codebase structure to identify important directories for building and executing a workplan.\\n\",\n",
      "      \"\\n\",\n",
      "      \"Your goal is to identify the most important directories that should be included for the user's task.\\n\",\n",
      "      \"\\n\",\n",
      "      \"Analyze the directories and identify the ones that:\\n\",\n",
      "      \"1. Contain core application code relevant to the user's task\\n\",\n",
      "      \"2. Likely contain important business logic\\n\",\n",
      "      \"3. Would be essential for understanding the codebase architecture\\n\",\n",
      "      \"4. Are needed to implement the requested task\\n\",\n",
      "      \"5. Contain SDKs or libraries relevant to the user's task\\n\",\n",
      "      \"\\n\",\n",
      "      \"Ignore directories that:\\n\",\n",
      "      \"1. Contain only build artifacts or generated code\\n\",\n",
      "      \"2. Store dependencies or vendor code\\n\",\n",
      "      \"3. Contain temporary or cache files\\n\",\n",
      "      \"4. Probably aren't relevant to the user's specific task\\n\",\n",
      "      \"\\n\",\n",
      "      \"User Task: Test debug logging functionality\\n\",\n",
      "      \"\\n\",\n",
      "      \"Return your analysis as a list of important directories, one per line, without any additional text or formatting as below:\\n\",\n",
      "      \"\\n\",\n",
      "      \"```context\\n\",\n",
      "      \"dir1/subdir1/\\n\",\n",
      "      \"dir2/\\n\",\n",
      "      \"dir3/subdir3/file3.filetype\\n\",\n",
      "      \"```\\n\",\n",
      "      \"\\n\",\n",
      "      \"Prefer to include directories, and not just file paths but include just file paths when appropriate.\\n\",\n",
      "      \"Don't include explanations for your choices, just return the list in the specified format.\\n\",\n",
      "      \" 17. 🔵 [info   ] [DEBUG] User prompt (1128 chars): <codebase_tree>\\n\",\n",
      "      \".\\n\",\n",
      "      \"├── .mcp.json\\n\",\n",
      "      \"├── .python-version\\n\",\n",
      "      \"├── CHANGELOG.md\\n\",\n",
      "      \"├── CLAUDE.md\\n\",\n",
      "      \"├── LLMManagerREADME.md\\n\",\n",
      "      \"├── README.md\\n\",\n",
      "      \"├── coverage_stats.txt\\n\",\n",
      "      \"├── pyproject.toml\\n\",\n",
      "      \"├── pyrightconfig.json\\n\",\n",
      "      \"│   ├── workflows/\\n\",\n",
      "      \"│   │   ├── publish.yml\\n\",\n",
      "      \"│   │   └── tests.yml\\n\",\n",
      "      \"├── docs/\\n\",\n",
      "      \"│   ├── USAGE.md\\n\",\n",
      "      \"│   └── coverage_baseline.md\\n\",\n",
      "      \"├── notebooks/\\n\",\n",
      "      \"│   ├── file_structure.ipynb\\n\",\n",
      "      \"│   └── llm_manager.ipynb\\n\",\n",
      "      \"├── yellhorn_mcp/\\n\",\n",
      "      \"│   ├── __init__.py\\n\",\n",
      "      \"│   ├── cli.py\\n\",\n",
      "      \"│   ├── llm_manager.py\\n\",\n",
      "      \"│   ├── metadata_models.py\\n\",\n",
      "      \"│   ├── server.py\\n\",\n",
      "      \"│   └── token_counter.py\\n\",\n",
      "      \"│   ├── formatters/\\n\",\n",
      "      \"│   │   ├── __init__.py\\n\",\n",
      "      \"│   │   ├── codebase_snapshot.py\\n\",\n",
      "      \"│   │   ├── context_fetcher.py\\n\",\n",
      "      \"│   │   └── prompt_formatter.py\\n\",\n",
      "      \"│   ├── integrations/\\n\",\n",
      "      \"│   │   ├── gemini_integration.py\\n\",\n",
      "      \"│   │   └── github_integration.py\\n\",\n",
      "      \"│   ├── models/\\n\",\n",
      "      \"│   │   └── metadata_models.py\\n\",\n",
      "      \"│   ├── processors/\\n\",\n",
      "      \"│   │   ├── __init__.py\\n\",\n",
      "      \"│   │   ├── context_processor.py\\n\",\n",
      "      \"│   │   ├── judgement_processor.py\\n\",\n",
      "      \"│   │   └── workplan_processor.py\\n\",\n",
      "      \"│   ├── utils/\\n\",\n",
      "      \"│   │   ├── comment_utils.py\\n\",\n",
      "      \"│   │   ├── cost_tracker_utils.py\\n\",\n",
      "      \"│   │   ├── git_utils.py\\n\",\n",
      "      \"│   │   ├── lsp_utils.py\\n\",\n",
      "      \"│   │   └── search_grounding_utils.py\\n\",\n",
      "      \"</codebase_tree>...\\n\",\n",
      "      \"============================================================\\n\",\n",
      "      \"\\n\",\n",
      "      \"📊 Summary:\\n\",\n",
      "      \"  - Total log messages: 17\\n\",\n",
      "      \"  - Info messages: 17\\n\",\n",
      "      \"  - Warning messages: 0\\n\",\n",
      "      \"  - Error messages: 0\\n\"\n",
      "     ]\n",
      "    }\n",
      "   ],\n",
      "   \"source\": [\n",
      "    \"async def test_with_debug_mode():\\n\",\n",
      "    \"    \\\"\\\"\\\"Test with debug logging enabled to see detailed information.\\\"\\\"\\\"\\n\",\n",
      "    \"    if not DEFAULT_MODEL or not REPO_PATH.exists():\\n\",\n",
      "    \"        print(\\\"⚠️ Skipping test - LLM or repository not available\\\")\\n\",\n",
      "    \"        return None\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    print(\\\"\\\\n🧪 Testing with debug mode enabled...\\\")\\n\",\n",
      "    \"    print(f\\\"📁 Repository: {REPO_PATH}\\\")\\n\",\n",
      "    \"    print(f\\\"🤖 Model: {DEFAULT_MODEL}\\\")\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    # Create mock context\\n\",\n",
      "    \"    mock_ctx = create_mock_context()\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    # Simple task for debugging\\n\",\n",
      "    \"    user_task = \\\"Test debug logging functionality\\\"\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    print(f\\\"\\\\n📋 Task: {user_task}\\\")\\n\",\n",
      "    \"    \\n\",\n",
      "    \"    try:\\n\",\n",
      "    \"        # Build context\\n\",\n",
      "    \"        print(\\\"\\\\n1️⃣ Building codebase context...\\\")\\n\",\n",
      "    \"        directory_context, file_paths, all_dirs = await build_codebase_context(\\n\",\n",
      "    \"            repo_path=REPO_PATH,\\n\",\n",
      "    \"            codebase_reasoning_mode=\\\"file_structure\\\",\\n\",\n",
      "    \"            model=DEFAULT_MODEL,\\n\",\n",
      "    \"            ctx=mock_ctx,\\n\",\n",
      "    \"            git_command_func=mock_ctx.request_context.lifespan_context['git_command_func']\\n\",\n",
      "    \"        )\\n\",\n",
      "    \"        print(f\\\"   ✅ Context built: {len(directory_context)} chars, {len(file_paths)} files\\\")\\n\",\n",
      "    \"        \\n\",\n",
      "    \"        # Analyze with LLM (with debug=True)\\n\",\n",
      "    \"        print(\\\"\\\\n2️⃣ Analyzing with LLM (debug=True)...\\\")\\n\",\n",
      "    \"        llm_result = await analyze_with_llm(\\n\",\n",
      "    \"            llm_manager=llm_manager,\\n\",\n",
      "    \"            model=DEFAULT_MODEL,\\n\",\n",
      "    \"            directory_context=directory_context[:5000],  # Use smaller context for debug\\n\",\n",
      "    \"            user_task=user_task,\\n\",\n",
      "    \"            debug=True,  # Enable debug logging\\n\",\n",
      "    \"            ctx=mock_ctx\\n\",\n",
      "    \"        )\\n\",\n",
      "    \"        print(f\\\"   ✅ LLM analysis complete\\\")\\n\",\n",
      "    \"        \\n\",\n",
      "    \"        # Parse directories\\n\",\n",
      "    \"        print(\\\"\\\\n3️⃣ Parsing LLM output...\\\")\\n\",\n",
      "    \"        important_dirs = await parse_llm_directories(\\n\",\n",
      "    \"            llm_result=llm_result,\\n\",\n",
      "    \"            all_dirs=all_dirs,\\n\",\n",
      "    \"            ctx=mock_ctx\\n\",\n",
      "    \"        )\\n\",\n",
      "    \"        print(f\\\"   ✅ Parsed {len(important_dirs)} important directories\\\")\\n\",\n",
      "    \"        print(important_dirs)\\n\",\n",
      "    \"        \\n\",\n",
      "    \"        # Print all log messages\\n\",\n",
      "    \"        print(\\\"\\\\n📝 Complete Log Messages:\\\")\\n\",\n",
      "    \"        print(\\\"=\\\" * 60)\\n\",\n",
      "    \"        if mock_ctx.log.called:\\n\",\n",
      "    \"            for i, call in enumerate(mock_ctx.log.call_args_list):\\n\",\n",
      "    \"                level = call[1].get('level', 'info')\\n\",\n",
      "    \"                msg = call[1]['message']\\n\",\n",
      "    \"                emoji = \\\"🔵\\\" if level == \\\"info\\\" else \\\"🟡\\\" if level == \\\"warning\\\" else \\\"🔴\\\"\\n\",\n",
      "    \"                print(f\\\"{i+1:3}. {emoji} [{level:7}] {msg}\\\")\\n\",\n",
      "    \"        else:\\n\",\n",
      "    \"            print(\\\"No log messages recorded\\\")\\n\",\n",
      "    \"        print(\\\"=\\\" * 60)\\n\",\n",
      "    \"        \\n\",\n",
      "    \"        print(f\\\"\\\\n📊 Summary:\\\")\\n\",\n",
      "    \"        print(f\\\"  - Total log messages: {mock_ctx.log.call_count}\\\")\\n\",\n",
      "    \"        print(f\\\"  - Info messages: {sum(1 for c in mock_ctx.log.call_args_list if c[1].get('level') == 'info')}\\\")\\n\",\n",
      "    \"        print(f\\\"  - Warning messages: {sum(1 for c in mock_ctx.log.call_args_list if c[1].get('level') == 'warning')}\\\")\\n\",\n",
      "    \"        print(f\\\"  - Error messages: {sum(1 for c in mock_ctx.log.call_args_list if c[1].get('level') == 'error')}\\\")\\n\",\n",
      "    \"        \\n\",\n",
      "    \"        return mock_ctx\\n\",\n",
      "    \"        \\n\",\n",
      "    \"    except Exception as e:\\n\",\n",
      "    \"        print(f\\\"\\\\n❌ Error during debug test: {e}\\\")\\n\",\n",
      "    \"        import traceback\\n\",\n",
      "    \"        traceback.print_exc()\\n\",\n",
      "    \"        print_log_messages(mock_ctx)\\n\",\n",
      "    \"        return mock_ctx\\n\",\n",
      "    \"\\n\",\n",
      "    \"# Run the debug test\\n\",\n",
      "    \"debug_ctx = await test_with_debug_mode()\"\n",
      "   ]\n",
      "  }\n",
      " ],\n",
      " \"metadata\": {\n",
      "  \"kernelspec\": {\n",
      "   \"display_name\": \"sravan-yellhorn\",\n",
      "   \"language\": \"python\",\n",
      "   \"name\": \"python3\"\n",
      "  },\n",
      "  \"language_info\": {\n",
      "   \"codemirror_mode\": {\n",
      "    \"name\": \"ipython\",\n",
      "    \"version\": 3\n",
      "   },\n",
      "   \"file_extension\": \".py\",\n",
      "   \"mimetype\": \"text/x-python\",\n",
      "   \"name\": \"python\",\n",
      "   \"nbconvert_exporter\": \"python\",\n",
      "   \"pygments_lexer\": \"ipython3\",\n",
      "   \"version\": \"3.11.11\"\n",
      "  }\n",
      " },\n",
      " \"nbformat\": 4,\n",
      " \"nbformat_minor\": 4\n",
      "}\n",
      "\n",
      "--- File: pyproject.toml ---\n",
      "[build-system]\n",
      "requires = [\"hatchling\"]\n",
      "build-backend = \"hatchling.build\"\n",
      "\n",
      "[project]\n",
      "name = \"yellhorn-mcp\"\n",
      "version = \"0.7.0\"\n",
      "authors = [{ name = \"Author\" }]\n",
      "description = \"Yellhorn offers MCP tools to generate detailed workplans with Gemini 2.5 Pro or OpenAI models and to review diffs against them using your entire codebase as context. Features unified LLM management, automatic chunking, and robust retry logic.\"\n",
      "readme = \"README.md\"\n",
      "requires-python = \">=3.10\"\n",
      "classifiers = [\n",
      "    \"Programming Language :: Python :: 3\",\n",
      "    \"Programming Language :: Python :: 3.10\",\n",
      "    \"License :: OSI Approved :: MIT License\",\n",
      "    \"Operating System :: OS Independent\",\n",
      "]\n",
      "dependencies = [\n",
      "    \"mcp[cli]~=1.10.1\",\n",
      "    \"google-genai~=1.26.0\",\n",
      "    \"aiohttp~=3.12.13\",\n",
      "    \"pydantic~=2.11.7\",\n",
      "    \"openai~=1.93.0\",\n",
      "    \"jedi~=0.19.2\",\n",
      "    \"tiktoken~=0.8.0\",\n",
      "    \"tenacity<9.0.0\",\n",
      "    \"google-api-core~=2.25.1\",\n",
      "]\n",
      "\n",
      "[project.optional-dependencies]\n",
      "dev = [\n",
      "    \"black\",\n",
      "    \"flake8\",\n",
      "    \"isort\",\n",
      "    \"pytest\",\n",
      "    \"pytest-asyncio\",\n",
      "    \"httpx\",\n",
      "    \"pytest-cov\",\n",
      "    \"jedi~=0.19\",\n",
      "]\n",
      "\n",
      "[project.scripts]\n",
      "yellhorn-mcp = \"yellhorn_mcp.cli:main\"\n",
      "\n",
      "[tool.black]\n",
      "line-length = 100\n",
      "target-version = [\"py310\"]\n",
      "\n",
      "[tool.isort]\n",
      "profile = \"black\"\n",
      "line_length = 100\n",
      "\n",
      "[tool.flake8]\n",
      "max-line-length = 100\n",
      "exclude = [\"venv\", \".git\", \"__pycache__\", \"build\", \"dist\"]\n",
      "\n",
      "[tool.pytest.ini_options]\n",
      "asyncio_default_fixture_loop_scope = \"function\"\n",
      "\n",
      "[tool.hatch.build.targets.wheel]\n",
      "packages = [\"yellhorn_mcp\"]\n",
      "\n",
      "--- File: yellhorn_mcp/cli.py ---\n",
      "\"\"\"\n",
      "Command-line interface for running the Yellhorn MCP server.\n",
      "\n",
      "This module provides a simple command to run the Yellhorn MCP server as a standalone\n",
      "application, making it easier to integrate with other programs or launch directly.\n",
      "\"\"\"\n",
      "\n",
      "import argparse\n",
      "import asyncio\n",
      "import logging\n",
      "import os\n",
      "import sys\n",
      "from pathlib import Path\n",
      "\n",
      "import uvicorn\n",
      "\n",
      "from yellhorn_mcp.server import is_git_repository, mcp\n",
      "\n",
      "logging.basicConfig(\n",
      "    stream=sys.stderr, level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\"\n",
      ")\n",
      "\n",
      "\n",
      "def main():\n",
      "    \"\"\"\n",
      "    Run the Yellhorn MCP server as a standalone command.\n",
      "\n",
      "    This function parses command-line arguments, validates environment variables,\n",
      "    and launches the MCP server.\n",
      "    \"\"\"\n",
      "    parser = argparse.ArgumentParser(description=\"Yellhorn MCP Server\")\n",
      "\n",
      "    parser.add_argument(\n",
      "        \"--repo-path\",\n",
      "        dest=\"repo_path\",\n",
      "        default=os.getenv(\"REPO_PATH\", os.getcwd()),\n",
      "        help=\"Path to the Git repository (default: current directory or REPO_PATH env var)\",\n",
      "    )\n",
      "\n",
      "    parser.add_argument(\n",
      "        \"--model\",\n",
      "        dest=\"model\",\n",
      "        default=os.getenv(\"YELLHORN_MCP_MODEL\", \"gemini-2.5-pro\"),\n",
      "        help=\"Model to use (e.g., gemini-2.5-pro, gemini-2.5-flash, \"\n",
      "        \"gpt-4o, gpt-4o-mini, o4-mini, o3, o3-deep-research, o4-mini-deep-research). \"\n",
      "        \"Default: gemini-2.5-pro or YELLHORN_MCP_MODEL env var.\",\n",
      "    )\n",
      "\n",
      "    parser.add_argument(\n",
      "        \"--codebase-reasoning\",\n",
      "        dest=\"codebase_reasoning\",\n",
      "        default=os.getenv(\"YELLHORN_MCP_REASONING\", \"full\"),\n",
      "        choices=[\"full\", \"lsp\", \"none\"],\n",
      "        help=\"Control codebase context for AI processing: \"\n",
      "        \"'full' (all code), 'lsp' (function signatures only), 'none' (no code). \"\n",
      "        \"Default: full or YELLHORN_MCP_REASONING env var.\",\n",
      "    )\n",
      "\n",
      "    parser.add_argument(\n",
      "        \"--no-search-grounding\",\n",
      "        dest=\"no_search_grounding\",\n",
      "        action=\"store_true\",\n",
      "        help=\"Disable Google Search Grounding for Gemini models. \"\n",
      "        \"By default, search grounding is enabled for all Gemini models. \"\n",
      "        \"This flag maps to YELLHORN_MCP_SEARCH=off environment variable.\",\n",
      "    )\n",
      "\n",
      "    parser.add_argument(\n",
      "        \"--host\",\n",
      "        dest=\"host\",\n",
      "        default=\"127.0.0.1\",\n",
      "        help=\"Host to bind the server to (default: 127.0.0.1)\",\n",
      "    )\n",
      "\n",
      "    parser.add_argument(\n",
      "        \"--port\",\n",
      "        dest=\"port\",\n",
      "        type=int,\n",
      "        default=8000,\n",
      "        help=\"Port to bind the server to (default: 8000)\",\n",
      "    )\n",
      "\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    # Validate API keys based on model\n",
      "    model = args.model\n",
      "    is_openai_model = model.startswith(\"gpt-\") or model.startswith(\"o\")\n",
      "\n",
      "    # For Gemini models\n",
      "    if not is_openai_model:\n",
      "        api_key = os.getenv(\"GEMINI_API_KEY\")\n",
      "        if not api_key:\n",
      "            logging.error(\"GEMINI_API_KEY environment variable is not set\")\n",
      "            logging.error(\n",
      "                \"Please set the GEMINI_API_KEY environment variable with your Gemini API key\"\n",
      "            )\n",
      "            sys.exit(1)\n",
      "    # For OpenAI models\n",
      "    else:\n",
      "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
      "        if not api_key:\n",
      "            logging.error(\"OPENAI_API_KEY environment variable is not set\")\n",
      "            logging.error(\n",
      "                \"Please set the OPENAI_API_KEY environment variable with your OpenAI API key\"\n",
      "            )\n",
      "            sys.exit(1)\n",
      "\n",
      "    # Set environment variables for the server\n",
      "    os.environ[\"REPO_PATH\"] = args.repo_path\n",
      "    os.environ[\"YELLHORN_MCP_MODEL\"] = args.model\n",
      "    os.environ[\"YELLHORN_MCP_REASONING\"] = args.codebase_reasoning\n",
      "\n",
      "    # Handle search grounding flag\n",
      "    if args.no_search_grounding:\n",
      "        os.environ[\"YELLHORN_MCP_SEARCH\"] = \"off\"\n",
      "\n",
      "    # Validate repository path\n",
      "    repo_path = Path(args.repo_path).resolve()\n",
      "    if not repo_path.exists():\n",
      "        logging.error(f\"Repository path {repo_path} does not exist\")\n",
      "        sys.exit(1)\n",
      "\n",
      "    # Check if the path is a Git repository (either standard or worktree)\n",
      "    if not is_git_repository(repo_path):\n",
      "        logging.error(f\"{repo_path} is not a Git repository\")\n",
      "        sys.exit(1)\n",
      "\n",
      "    logging.info(f\"Starting Yellhorn MCP server at http://{args.host}:{args.port}\")\n",
      "    logging.info(f\"Repository path: {repo_path}\")\n",
      "    logging.info(f\"Using model: {args.model}\")\n",
      "\n",
      "    # Show search grounding status if using Gemini model\n",
      "    is_openai_model = args.model.startswith(\"gpt-\") or args.model.startswith(\"o\")\n",
      "    if not is_openai_model:\n",
      "        search_status = \"disabled\" if args.no_search_grounding else \"enabled\"\n",
      "        logging.info(f\"Google Search Grounding: {search_status}\")\n",
      "\n",
      "    mcp.run(transport=\"stdio\")\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\n",
      "--- File: yellhorn_mcp/formatters/__init__.py ---\n",
      "\"\"\"Formatters package for codebase processing and formatting utilities.\"\"\"\n",
      "\n",
      "from .codebase_snapshot import get_codebase_snapshot\n",
      "from .prompt_formatter import format_codebase_for_prompt, build_file_structure_context\n",
      "from .context_fetcher import get_codebase_context\n",
      "\n",
      "__all__ = [\n",
      "    \"get_codebase_snapshot\",\n",
      "    \"build_file_structure_context\", \n",
      "    \"format_codebase_for_prompt\",\n",
      "    \"get_codebase_context\",\n",
      "]\n",
      "\n",
      "--- File: yellhorn_mcp/formatters/codebase_snapshot.py ---\n",
      "\"\"\"Codebase snapshot functionality for fetching and filtering repository files.\"\"\"\n",
      "\n",
      "from pathlib import Path\n",
      "from yellhorn_mcp.utils.git_utils import run_git_command\n",
      "import fnmatch\n",
      "\n",
      "# Global set of file patterns and extensions to always ignore\n",
      "# These are files that should never be included in AI context as they are:\n",
      "# - Binary/compiled artifacts\n",
      "# - Auto-generated files\n",
      "# - Environment-specific configurations\n",
      "# - Transient cache/logs\n",
      "ALWAYS_IGNORE_PATTERNS = {\n",
      "    # === Compiled binaries & libraries ===\n",
      "    # Machine-code artifacts with no source information\n",
      "    \"*.exe\",\n",
      "    \"*.dll\",\n",
      "    \"*.so\",\n",
      "    \"*.dylib\",\n",
      "    \"*.bin\",\n",
      "    \"*.pyc\",\n",
      "    \"*.pyo\",\n",
      "    \"*.pyd\",\n",
      "    \"*.class\",\n",
      "    \"*.o\",\n",
      "    \"*.a\",\n",
      "    \"*.lib\",\n",
      "    \"*.ko\",\n",
      "    \"*.elf\",\n",
      "    \"*.dSYM/\",\n",
      "    \"*.wasm\",\n",
      "    \n",
      "    # === Databases & model checkpoints ===\n",
      "    # Large binary blobs, ML checkpoints, experiment tracking\n",
      "    \"*.db\",\n",
      "    \"*.sqlite\",\n",
      "    \"*.sqlite3\",\n",
      "    \"*.sqlite-journal\",\n",
      "    \"*.sqlite-wal\",\n",
      "    \"milvus.db\",\n",
      "    \"milvus.db.lock\",\n",
      "    \"*models_ckpt_cache*\",\n",
      "    \"*/wandb/*\",\n",
      "    \"wandb/\",\n",
      "    \"mlruns/\",\n",
      "    \"mlartifacts/\",\n",
      "    \"*.ipynb_checkpoints\",\n",
      "    \".ipynb_checkpoints/\",\n",
      "    \"*.ckpt\",\n",
      "    \"*.h5\",\n",
      "    \"*.hdf5\",\n",
      "    \"*.pkl\",\n",
      "    \"*.pickle\",\n",
      "    \"*.joblib\",\n",
      "    \"*.npy\",\n",
      "    \"*.npz\",\n",
      "    \"*.parquet\",\n",
      "    \"*.feather\",\n",
      "    \"*.arrow\",\n",
      "    \n",
      "    # === Lock files & dependency manifests ===\n",
      "    # Auto-generated to pin dependency versions\n",
      "    \"*.lock\",\n",
      "    \"package-lock.json\",\n",
      "    \"yarn.lock\",\n",
      "    \"pnpm-lock.yaml\",\n",
      "    \"poetry.lock\",\n",
      "    \"Pipfile.lock\",\n",
      "    \"composer.lock\",\n",
      "    \"Gemfile.lock\",\n",
      "    \"go.sum\",\n",
      "    \"Cargo.lock\",\n",
      "    \"pubspec.lock\",\n",
      "    \"mix.lock\",\n",
      "    \n",
      "    # === Environment & configuration files ===\n",
      "    \".env\",\n",
      "    \".env.*\",\n",
      "    \".env.local\",\n",
      "    \".env.development\",\n",
      "    \".env.production\",\n",
      "    \".env.test\",\n",
      "    \"*.local\",\n",
      "    \"*local-only*\",\n",
      "    \n",
      "    # === Logs & run outputs ===\n",
      "    # Ephemeral run or CI output\n",
      "    \"*.log\",\n",
      "    \"*.log.*\",\n",
      "    \"logs/\",\n",
      "    \"log/\",\n",
      "    \"super-linter.log\",\n",
      "    \"events.out.tfevents*\",\n",
      "    \"*.tfevents\",\n",
      "    \"go/logs.jsonl\",\n",
      "    \"*.out\",\n",
      "    \"*.err\",\n",
      "    \"nohup.out\",\n",
      "    \"*.pid\",\n",
      "    \n",
      "    # === Test artifacts & outputs ===\n",
      "    # Generated when running tests\n",
      "    \"*.test\",\n",
      "    \".test\",\n",
      "    \"test_output/\",\n",
      "    \"testoutput/\",\n",
      "    \"test-results/\",\n",
      "    \"*/__test__/\",\n",
      "    \"*/__tests__/\",\n",
      "    \"*.snap\",\n",
      "    \"*.snapshot\",\n",
      "    \"coverage/\",\n",
      "    \"htmlcov/\",\n",
      "    \".coverage\",\n",
      "    \"coverage.xml\",\n",
      "    \"*.coverage\",\n",
      "    \"*.cover\",\n",
      "    \"*.gcov\",\n",
      "    \"*.gcda\",\n",
      "    \"*.gcno\",\n",
      "    \"junit.xml\",\n",
      "    \"test-report.xml\",\n",
      "    \n",
      "    # === Cache directories ===\n",
      "    # Speed up tooling, never manually edited\n",
      "    \"__pycache__/\",\n",
      "    \".mypy_cache/\",\n",
      "    \".pytest_cache/\",\n",
      "    \".ruff_cache/\",\n",
      "    \".hypothesis/\",\n",
      "    \".tox/\",\n",
      "    \".nox/\",\n",
      "    \".pyre/\",\n",
      "    \"go/tmp/\",\n",
      "    \"go/temp/\",\n",
      "    \"tmp/\",\n",
      "    \"temp/\",\n",
      "    \"cache/\",\n",
      "    \".cache/\",\n",
      "    \".sass-cache/\",\n",
      "    \".parcel-cache/\",\n",
      "    \".next/\",\n",
      "    \".nuxt/\",\n",
      "    \".vuepress/dist/\",\n",
      "    \".docusaurus/\",\n",
      "    \".serverless/\",\n",
      "    \".fusebox/\",\n",
      "    \".dynamodb/\",\n",
      "    \".yarn/cache/\",\n",
      "    \".yarn/install-state.gz\",\n",
      "    \n",
      "    # === Virtual environments ===\n",
      "    \"env/\",\n",
      "    \"venv/\",\n",
      "    \"virtualenv/\",\n",
      "    \".venv/\",\n",
      "    \"ENV/\",\n",
      "    \"env.bak/\",\n",
      "    \"venv.bak/\",\n",
      "    \n",
      "    # === IDE & editor files ===\n",
      "    \".idea/\",\n",
      "    \".vscode/\",\n",
      "    \"*.swp\",\n",
      "    \"*.swo\",\n",
      "    \"*.swn\",\n",
      "    \"*.bak\",\n",
      "    \"*~\",\n",
      "    \".project\",\n",
      "    \".classpath\",\n",
      "    \".settings/\",\n",
      "    \"*.sublime-project\",\n",
      "    \"*.sublime-workspace\",\n",
      "    \".kate-swp\",\n",
      "    \".ropeproject/\",\n",
      "    \n",
      "    # === Build artifacts & outputs ===\n",
      "    \"build/\",\n",
      "    \"dist/\",\n",
      "    \"out/\",\n",
      "    \"target/\",\n",
      "    \"bin/\",\n",
      "    \"obj/\",\n",
      "    \"*.egg-info/\",\n",
      "    \"*.egg\",\n",
      "    \"develop-eggs/\",\n",
      "    \"downloads/\",\n",
      "    \"eggs/\",\n",
      "    \".eggs/\",\n",
      "    \"lib/\",\n",
      "    \"lib64/\",\n",
      "    \"parts/\",\n",
      "    \"sdist/\",\n",
      "    \"var/\",\n",
      "    \"wheels/\",\n",
      "    \"*.whl\",\n",
      "    \"share/python-wheels/\",\n",
      "    \"MANIFEST\",\n",
      "    \n",
      "    # === Node.js specific ===\n",
      "    \"node_modules/\",\n",
      "    \"jspm_packages/\",\n",
      "    \"bower_components/\",\n",
      "    \".npm/\",\n",
      "    \"web_modules/\",\n",
      "    \".pnp.*\",\n",
      "    \n",
      "    # === Container & infrastructure files ===\n",
      "    \"**/Dockerfile*\",\n",
      "    \"docker-compose*.yml\",\n",
      "    \"docker-compose*.yaml\",\n",
      "    \"*.override.*\",\n",
      "    \"*/entrypoint.sh\",\n",
      "    \".dockerignore\",\n",
      "    \"go/run\",\n",
      "    \"go/runx.sh\",\n",
      "    \n",
      "    # === Terraform & IaC ===\n",
      "    \".terraform/\",\n",
      "    \".terraform.lock.hcl\",\n",
      "    \"*.tfstate\",\n",
      "    \"*.tfstate.*\",\n",
      "    \"*.tfplan\",\n",
      "    \"*.tfvars\",\n",
      "    \"terraform.tfvars\",\n",
      "    \n",
      "    # === Generated documentation ===\n",
      "    \"site/\",\n",
      "    \"docs/_build/\",\n",
      "    \"docs/.vuepress/dist/\",\n",
      "    \"_site/\",\n",
      "    \".jekyll-cache/\",\n",
      "    \".jekyll-metadata\",\n",
      "    \"public/\",\n",
      "    \"*.pdf\",\n",
      "    \n",
      "    # === Archive files ===\n",
      "    \"*.zip\",\n",
      "    \"*.tar\",\n",
      "    \"*.tar.gz\",\n",
      "    \"*.tgz\",\n",
      "    \"*.tar.bz2\",\n",
      "    \"*.tbz2\",\n",
      "    \"*.tar.xz\",\n",
      "    \"*.txz\",\n",
      "    \"*.rar\",\n",
      "    \"*.7z\",\n",
      "    \"*.gz\",\n",
      "    \"*.bz2\",\n",
      "    \"*.xz\",\n",
      "    \"*.Z\",\n",
      "    \"*.deb\",\n",
      "    \"*.rpm\",\n",
      "    \"*.dmg\",\n",
      "    \"*.iso\",\n",
      "    \"*.jar\",\n",
      "    \"*.war\",\n",
      "    \"*.ear\",\n",
      "    \n",
      "    # === Media files ===\n",
      "    # Images\n",
      "    \"*.png\",\n",
      "    \"*.jpg\",\n",
      "    \"*.jpeg\",\n",
      "    \"*.gif\",\n",
      "    \"*.bmp\",\n",
      "    \"*.svg\",\n",
      "    \"*.ico\",\n",
      "    \"*.webp\",\n",
      "    \"*.tiff\",\n",
      "    \"*.tif\",\n",
      "    \"*.psd\",\n",
      "    \"*.ai\",\n",
      "    \"*.eps\",\n",
      "    \"*.raw\",\n",
      "    # Video\n",
      "    \"*.mp4\",\n",
      "    \"*.avi\",\n",
      "    \"*.mov\",\n",
      "    \"*.wmv\",\n",
      "    \"*.flv\",\n",
      "    \"*.webm\",\n",
      "    \"*.mkv\",\n",
      "    \"*.m4v\",\n",
      "    \"*.mpg\",\n",
      "    \"*.mpeg\",\n",
      "    \"*.3gp\",\n",
      "    # Audio\n",
      "    \"*.mp3\",\n",
      "    \"*.wav\",\n",
      "    \"*.flac\",\n",
      "    \"*.aac\",\n",
      "    \"*.ogg\",\n",
      "    \"*.wma\",\n",
      "    \"*.m4a\",\n",
      "    \"*.opus\",\n",
      "    \"*.ape\",\n",
      "    \n",
      "    # === Fonts ===\n",
      "    \"*.ttf\",\n",
      "    \"*.otf\",\n",
      "    \"*.woff\",\n",
      "    \"*.woff2\",\n",
      "    \"*.eot\",\n",
      "    \n",
      "    # === OS specific files ===\n",
      "    \".DS_Store\",\n",
      "    \"Thumbs.db\",\n",
      "    \"desktop.ini\",\n",
      "    \"*.lnk\",\n",
      "    \"Icon\\r\",\n",
      "    \".Spotlight-V100/\",\n",
      "    \".Trashes/\",\n",
      "    \"ehthumbs.db\",\n",
      "    \"ehthumbs_vista.db\",\n",
      "    \"*.stackdump\",\n",
      "    \"[Dd]esktop.ini\",\n",
      "    \"$RECYCLE.BIN/\",\n",
      "    \n",
      "    # === Temporary files ===\n",
      "    \"*.tmp\",\n",
      "    \"*.temp\",\n",
      "    \"*.bak\",\n",
      "    \"*.backup\",\n",
      "    \"*.old\",\n",
      "    \"*.orig\",\n",
      "    \"*.rej\",\n",
      "    \"*.BACKUP.*\",\n",
      "    \"*.BASE.*\",\n",
      "    \"*.LOCAL.*\",\n",
      "    \"*.REMOTE.*\",\n",
      "    \n",
      "    # === Security & secrets ===\n",
      "    \"*.key\",\n",
      "    \"*.pem\",\n",
      "    \"*.p12\",\n",
      "    \"*.pfx\",\n",
      "    \"*.cert\",\n",
      "    \"*.crt\",\n",
      "    \"*.csr\",\n",
      "    \"*.jks\",\n",
      "    \"*.keystore\",\n",
      "    \"id_rsa\",\n",
      "    \"id_rsa.pub\",\n",
      "    \"id_dsa\",\n",
      "    \"id_dsa.pub\",\n",
      "    \"*.gpg\",\n",
      "    \n",
      "    # === Version control & tool config ===\n",
      "    \".git/\",\n",
      "    \".gitignore\",\n",
      "    \".gitattributes\",\n",
      "    \".gitmodules\",\n",
      "    \".hg/\",\n",
      "    \".hgignore\",\n",
      "    \".svn/\",\n",
      "    \".bzr/\",\n",
      "    \".yellhornignore\",\n",
      "    \".yellhorncontext\",\n",
      "    \".continueignore\",\n",
      "    \".cursorignore\",\n",
      "    \".sourcery.yaml\",\n",
      "    \".pre-commit-config.yaml\",\n",
      "    \".editorconfig\",\n",
      "    \".prettierrc*\",\n",
      "    \".eslintrc*\",\n",
      "    \".stylelintrc*\",\n",
      "    \".markdownlint*\",\n",
      "    \n",
      "    # === Example & fixture files ===\n",
      "    \"example-*.yml\",\n",
      "    \"example-*.yaml\",\n",
      "    \"example-*.json\",\n",
      "    \"fixture-*\",\n",
      "    \"fixtures/\",\n",
      "    \"examples/\",\n",
      "    \"samples/\",\n",
      "    \n",
      "    # === Minified files ===\n",
      "    \"*.min.js\",\n",
      "    \"*.min.css\",\n",
      "    \"*.min.map\",\n",
      "    \n",
      "    # === Source maps ===\n",
      "    \"*.map\",\n",
      "    \"*.js.map\",\n",
      "    \"*.css.map\",\n",
      "}\n",
      "\n",
      "def matches_pattern(path: str, pattern: str) -> bool:\n",
      "    if pattern.endswith(\"/\"):\n",
      "        # Directory pattern - check if file is within this directory\n",
      "        return path.startswith(pattern) or fnmatch.fnmatch(path + \"/\", pattern)\n",
      "    else:\n",
      "        # File pattern\n",
      "        return fnmatch.fnmatch(path, pattern)\n",
      "\n",
      "async def get_codebase_snapshot(\n",
      "    repo_path: Path, just_paths: bool = False, log_function=print, git_command_func=None\n",
      ") -> tuple[list[str], dict[str, str]]:\n",
      "    \"\"\"Get a snapshot of the codebase.\n",
      "\n",
      "    Args:\n",
      "        repo_path: Path to the repository.\n",
      "        just_paths: If True, return only file paths without contents.\n",
      "        log_function: Function to use for logging.\n",
      "        git_command_func: Optional Git command function (for mocking).\n",
      "\n",
      "    Returns:\n",
      "        Tuple of (file_paths, file_contents).\n",
      "    \"\"\"\n",
      "    mode_str = \"paths\" if just_paths else \"full\"\n",
      "    log_function(f\"Getting codebase snapshot in mode: {mode_str}\")\n",
      "\n",
      "    # Get the .gitignore patterns\n",
      "    gitignore_patterns = []\n",
      "    gitignore_path = repo_path / \".gitignore\"\n",
      "    if gitignore_path.exists():\n",
      "        gitignore_patterns = [\n",
      "            line.strip()\n",
      "            for line in gitignore_path.read_text().strip().split(\"\\n\")\n",
      "            if line.strip() and not line.strip().startswith(\"#\")\n",
      "        ]\n",
      "        log_function(f\"Found .gitignore with {len(gitignore_patterns)} patterns\")\n",
      "\n",
      "    # Get tracked files\n",
      "    tracked_files = await run_git_command(repo_path, [\"ls-files\"], git_command_func)\n",
      "    tracked_file_list = tracked_files.strip().split(\"\\n\") if tracked_files else []\n",
      "\n",
      "    # Get untracked files (not ignored by .gitignore)\n",
      "    untracked_files = await run_git_command(\n",
      "        repo_path, [\"ls-files\", \"--others\", \"--exclude-standard\"], git_command_func\n",
      "    )\n",
      "    untracked_file_list = untracked_files.strip().split(\"\\n\") if untracked_files else []\n",
      "\n",
      "    # Combine all files\n",
      "    all_files = set(tracked_file_list + untracked_file_list)\n",
      "\n",
      "    # Filter out empty strings\n",
      "    all_files = {f for f in all_files if f}\n",
      "\n",
      "    # Check for additional ignore files (.yellhornignore and .yellhorncontext)\n",
      "    yellhornignore_path = repo_path / \".yellhornignore\"\n",
      "    yellhornignore_patterns = []\n",
      "    if yellhornignore_path.exists():\n",
      "        yellhornignore_patterns = [\n",
      "            line.strip()\n",
      "            for line in yellhornignore_path.read_text().strip().split(\"\\n\")\n",
      "            if line.strip() and not line.strip().startswith(\"#\")\n",
      "        ]\n",
      "        log_function(f\"Found .yellhornignore with {len(yellhornignore_patterns)} patterns\")\n",
      "\n",
      "    # Parse .yellhorncontext patterns (supports blacklist, whitelist, and negation)\n",
      "    yellhorncontext_path = repo_path / \".yellhorncontext\"\n",
      "    context_blacklist_patterns = []\n",
      "    context_whitelist_patterns = []\n",
      "    context_negation_patterns = []\n",
      "\n",
      "    if yellhorncontext_path.exists():\n",
      "        lines = [\n",
      "            line.strip()\n",
      "            for line in yellhorncontext_path.read_text().strip().split(\"\\n\")\n",
      "            if line.strip() and not line.strip().startswith(\"#\")\n",
      "        ]\n",
      "\n",
      "        # Separate patterns by type\n",
      "        for line in lines:\n",
      "            if line.startswith(\"!\"):\n",
      "                # Blacklist pattern (exclude this directory/file)\n",
      "                context_blacklist_patterns.append(line[1:])  # Remove the '!' prefix\n",
      "            else:\n",
      "                # All other patterns are whitelist (directories/files to include)\n",
      "                context_whitelist_patterns.append(line)\n",
      "\n",
      "        log_function(\n",
      "            f\"Found .yellhorncontext with {len(context_whitelist_patterns)} whitelist, \"\n",
      "            f\"{len(context_blacklist_patterns)} blacklist, and {len(context_negation_patterns)} negation patterns\"\n",
      "        )\n",
      "\n",
      "\n",
      "    # Categorize files by priority\n",
      "    # Priority: yellhorncontext whitelist > yellhorncontext blacklist > yellhornignore whitelist > yellhornignore blacklist > gitignore blacklist\n",
      "    \n",
      "    # Categories for files\n",
      "    yellhorncontext_whitelist_files = []\n",
      "    yellhorncontext_blacklist_files = []\n",
      "    yellhornignore_whitelist_files = []\n",
      "    yellhornignore_blacklist_files = []\n",
      "    gitignore_blacklist_files = []\n",
      "    other_files = []\n",
      "    always_ignored_count = 0\n",
      "    \n",
      "    # Parse .yellhornignore patterns to separate whitelist and blacklist\n",
      "    yellhornignore_whitelist_patterns = []\n",
      "    yellhornignore_blacklist_patterns = []\n",
      "    \n",
      "    for pattern in yellhornignore_patterns:\n",
      "        if pattern.startswith(\"!\"):\n",
      "            # Whitelist pattern in yellhornignore (negation)\n",
      "            yellhornignore_whitelist_patterns.append(pattern[1:])\n",
      "        else:\n",
      "            # Blacklist pattern in yellhornignore\n",
      "            yellhornignore_blacklist_patterns.append(pattern)\n",
      "    \n",
      "    # Process each file and categorize it (excluding always-ignored files)\n",
      "    for file_path in sorted(all_files):\n",
      "        # Skip files matching always-ignore patterns\n",
      "        should_ignore = False\n",
      "        for pattern in ALWAYS_IGNORE_PATTERNS:\n",
      "            if matches_pattern(file_path, pattern):\n",
      "                should_ignore = True\n",
      "                break\n",
      "        \n",
      "        if should_ignore:\n",
      "            always_ignored_count += 1\n",
      "            continue\n",
      "        # Determine which category this file belongs to\n",
      "        is_context_whitelisted = any(matches_pattern(file_path, p) for p in context_whitelist_patterns)\n",
      "        is_context_blacklisted = any(matches_pattern(file_path, p) for p in context_blacklist_patterns)\n",
      "        is_ignore_whitelisted = any(matches_pattern(file_path, p) for p in yellhornignore_whitelist_patterns)\n",
      "        is_ignore_blacklisted = any(matches_pattern(file_path, p) for p in yellhornignore_blacklist_patterns)\n",
      "        \n",
      "        # If we have context whitelist patterns, only include files that match them\n",
      "        if context_whitelist_patterns and not is_context_whitelisted:\n",
      "            continue  # Skip files not in whitelist\n",
      "        \n",
      "        # Categorize based on priority\n",
      "        if is_context_whitelisted and not is_context_blacklisted:\n",
      "            yellhorncontext_whitelist_files.append(file_path)\n",
      "        elif is_context_whitelisted and is_context_blacklisted:\n",
      "            yellhorncontext_blacklist_files.append(file_path)\n",
      "        elif is_context_blacklisted:\n",
      "            yellhorncontext_blacklist_files.append(file_path)\n",
      "        elif is_ignore_whitelisted:\n",
      "            yellhornignore_whitelist_files.append(file_path)\n",
      "        elif is_ignore_blacklisted:\n",
      "            yellhornignore_blacklist_files.append(file_path)\n",
      "        else:\n",
      "            # File doesn't match any special patterns\n",
      "            other_files.append(file_path)\n",
      "    \n",
      "    # Files to include in priority order (excluding blacklisted files)\n",
      "    # Priority: yellhorncontext whitelist > yellhornignore whitelist > other files (only if no .yellhorncontext)\n",
      "    if yellhorncontext_path.exists():\n",
      "        # If .yellhorncontext exists, only include whitelisted files\n",
      "        files_to_include = (\n",
      "            yellhorncontext_whitelist_files\n",
      "        )\n",
      "    else:\n",
      "        # If no .yellhorncontext, include other files as well\n",
      "        files_to_include = (\n",
      "            yellhornignore_whitelist_files +\n",
      "            other_files\n",
      "        )\n",
      "    \n",
      "    # Log filtering results\n",
      "    total_files = len(all_files)\n",
      "    log_function(f\"File categorization results out of {total_files} files:\")\n",
      "    if always_ignored_count > 0:\n",
      "        log_function(f\"  - {always_ignored_count} always ignored (images, binaries, configs, etc.)\")\n",
      "    log_function(f\"  - {len(yellhorncontext_whitelist_files)} in yellhorncontext whitelist (included)\")\n",
      "    log_function(f\"  - {len(yellhorncontext_blacklist_files)} in yellhorncontext blacklist (excluded)\")\n",
      "    log_function(f\"  - {len(yellhornignore_whitelist_files)} in yellhornignore whitelist (included)\")\n",
      "    log_function(f\"  - {len(yellhornignore_blacklist_files)} in yellhornignore blacklist (excluded)\")\n",
      "    if yellhorncontext_path.exists():\n",
      "        log_function(f\"  - {len(other_files)} other files (excluded - .yellhorncontext exists)\")\n",
      "    else:\n",
      "        log_function(f\"  - {len(other_files)} other files (included - no .yellhorncontext)\")\n",
      "    log_function(f\"Total included: {len(files_to_include)} files (excluded {always_ignored_count} always-ignored files)\")\n",
      "    \n",
      "    # Use the prioritized list of files to include\n",
      "    file_paths = files_to_include\n",
      "\n",
      "    # If just_paths is True, return empty file contents\n",
      "    if just_paths:\n",
      "        return file_paths, {}\n",
      "\n",
      "    # Read file contents for full mode\n",
      "    file_contents = {}\n",
      "    MAX_FILE_SIZE = 1024 * 1024  # 1MB limit per file\n",
      "    skipped_large_files = 0\n",
      "\n",
      "    for file_path in file_paths:\n",
      "        full_path = repo_path / file_path\n",
      "        try:\n",
      "            # Check file size first\n",
      "            if full_path.stat().st_size > MAX_FILE_SIZE:\n",
      "                skipped_large_files += 1\n",
      "                continue\n",
      "\n",
      "            # Try to read as text\n",
      "            content = full_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
      "            file_contents[file_path] = content\n",
      "        except Exception:\n",
      "            # Skip files that can't be read\n",
      "            continue\n",
      "\n",
      "    if skipped_large_files > 0:\n",
      "        log_function(f\"Skipped {skipped_large_files} files larger than 1MB\")\n",
      "\n",
      "    log_function(f\"Read contents of {len(file_contents)} files\")\n",
      "\n",
      "    return file_paths, file_contents\n",
      "\n",
      "--- File: yellhorn_mcp/formatters/context_fetcher.py ---\n",
      "\"\"\"Context fetching orchestration for different codebase reasoning modes.\"\"\"\n",
      "\n",
      "from pathlib import Path\n",
      "from typing import Callable, Optional\n",
      "from .codebase_snapshot import get_codebase_snapshot\n",
      "from .prompt_formatter import format_codebase_for_prompt, build_file_structure_context\n",
      "from yellhorn_mcp.utils.lsp_utils import get_lsp_snapshot\n",
      "from yellhorn_mcp.token_counter import TokenCounter\n",
      "\n",
      "\n",
      "def apply_token_limit(\n",
      "    content: str, \n",
      "    token_limit: int, \n",
      "    model: str, \n",
      "    log_function,\n",
      "    file_paths: list[str] | None = None,\n",
      "    file_contents: dict[str, str] | None = None\n",
      ") -> tuple[str, list[str]]:\n",
      "    \"\"\"Apply token limit to content by truncating if necessary.\n",
      "    \n",
      "    Args:\n",
      "        content: The content to potentially truncate.\n",
      "        token_limit: Maximum number of tokens allowed.\n",
      "        model: Model name for token counting.\n",
      "        log_function: Function to use for logging.\n",
      "        file_paths: Optional list of file paths in the content.\n",
      "        file_contents: Optional dict mapping file paths to their contents.\n",
      "        \n",
      "    Returns:\n",
      "        Tuple of (content, file_paths) where content is possibly truncated \n",
      "        and file_paths contains the paths of files included in the final content.\n",
      "    \"\"\"\n",
      "    token_counter = TokenCounter()\n",
      "    current_tokens = token_counter.count_tokens(content, model)\n",
      "    \n",
      "    if current_tokens <= token_limit:\n",
      "        # Return all file paths if content fits within limit\n",
      "        return content, file_paths if file_paths else []\n",
      "        \n",
      "    log_function(f\"Context exceeds token limit ({current_tokens} > {token_limit}), truncating...\")\n",
      "    \n",
      "    # If we have file information, truncate by complete files\n",
      "    if file_paths and file_contents:\n",
      "        included_paths = []\n",
      "        accumulated_content = \"\"\n",
      "        accumulated_tokens = 0\n",
      "        \n",
      "        # Try to include files one by one until we hit the limit\n",
      "        for file_path in file_paths:\n",
      "            # Check if this file is in the content\n",
      "            file_header = f\"--- File: {file_path} ---\"\n",
      "            if file_header not in content:\n",
      "                # File might be in tree structure only\n",
      "                continue\n",
      "                \n",
      "            # Get the file content from the dict\n",
      "            if file_path not in file_contents:\n",
      "                continue\n",
      "                \n",
      "            file_content = file_contents[file_path]\n",
      "            # Construct the file section as it appears in the formatted content\n",
      "            file_section = f\"\\n{file_header}\\n{file_content}\"\n",
      "            if not file_content.endswith(\"\\n\"):\n",
      "                file_section += \"\\n\"\n",
      "            \n",
      "            # Check if adding this file would exceed the limit\n",
      "            test_content = accumulated_content + file_section\n",
      "            test_tokens = token_counter.count_tokens(test_content, model)\n",
      "            \n",
      "            if test_tokens > token_limit:\n",
      "                # Can't fit this file, stop here\n",
      "                break\n",
      "                \n",
      "            # Add this file\n",
      "            accumulated_content = test_content\n",
      "            accumulated_tokens = test_tokens\n",
      "            included_paths.append(file_path)\n",
      "        \n",
      "        if included_paths:\n",
      "            # Rebuild content with only included files\n",
      "            from .prompt_formatter import build_file_structure_context\n",
      "            \n",
      "            # Build tree with only included files\n",
      "            truncated_content = build_file_structure_context(included_paths)\n",
      "            \n",
      "            # Add file contents\n",
      "            if accumulated_content:\n",
      "                truncated_content += \"\\n\\n<file_contents>\" + accumulated_content + \"\\n</file_contents>\"\n",
      "            \n",
      "            # Add truncation notice\n",
      "            truncated_content += \"\\n\\n... [Content truncated due to token limit]\"\n",
      "            \n",
      "            final_tokens = token_counter.count_tokens(truncated_content, model)\n",
      "            log_function(f\"Context truncated from {current_tokens} to {final_tokens} tokens\")\n",
      "            log_function(f\"Included {len(included_paths)} of {len(file_paths)} files\")\n",
      "            \n",
      "            return truncated_content, included_paths\n",
      "    \n",
      "    # Fallback to character-based truncation if no file information\n",
      "    left, right = 0, len(content)\n",
      "    result_length = 0\n",
      "    \n",
      "    while left <= right:\n",
      "        mid = (left + right) // 2\n",
      "        truncated = content[:mid]\n",
      "        tokens = token_counter.count_tokens(truncated, model)\n",
      "        \n",
      "        if tokens <= token_limit:\n",
      "            result_length = mid\n",
      "            left = mid + 1\n",
      "        else:\n",
      "            right = mid - 1\n",
      "    \n",
      "    # Truncate at the last newline before the limit to avoid cutting mid-line\n",
      "    truncated_content = content[:result_length]\n",
      "    last_newline = truncated_content.rfind('\\n')\n",
      "    if last_newline > 0:\n",
      "        truncated_content = truncated_content[:last_newline]\n",
      "    \n",
      "    # Add truncation notice\n",
      "    truncated_content += \"\\n\\n... [Content truncated due to token limit]\"\n",
      "    \n",
      "    final_tokens = token_counter.count_tokens(truncated_content, model)\n",
      "    log_function(f\"Context truncated from {current_tokens} to {final_tokens} tokens\")\n",
      "    \n",
      "    # Try to determine which files made it into the truncated content\n",
      "    included_paths = []\n",
      "    if file_paths:\n",
      "        for file_path in file_paths:\n",
      "            file_header = f\"--- File: {file_path} ---\"\n",
      "            if file_header in truncated_content:\n",
      "                included_paths.append(file_path)\n",
      "    \n",
      "    return truncated_content, included_paths\n",
      "\n",
      "\n",
      "async def get_codebase_context(\n",
      "    repo_path: Path, \n",
      "    reasoning_mode: str, \n",
      "    log_function: Optional[Callable[[str], None]] =print,\n",
      "    token_limit: Optional[int] = None,\n",
      "    model: Optional[str] = None,\n",
      "    git_command_func: Optional[Callable] = None\n",
      ") -> tuple[str, list[str]]:\n",
      "    \"\"\"Fetches and formats the codebase context based on the reasoning mode.\n",
      "\n",
      "    Args:\n",
      "        repo_path: Path to the repository.\n",
      "        reasoning_mode: Mode for codebase analysis (\"full\", \"lsp\", \"file_structure\", \"none\").\n",
      "        log_function: Function to use for logging.\n",
      "        token_limit: Optional maximum number of tokens to include in the context.\n",
      "        model: Optional model name for token counting (required if token_limit is set).\n",
      "        git_command_func: Optional Git command function (for mocking).\n",
      "\n",
      "    Returns:\n",
      "        Tuple of (formatted codebase context string, list of file paths included).\n",
      "        Context may be truncated to fit token limit.\n",
      "    \"\"\"\n",
      "    file_paths, file_contents = await get_codebase_snapshot(\n",
      "        repo_path, just_paths=(reasoning_mode!=\"full\"), log_function=log_function, git_command_func=git_command_func\n",
      "    )\n",
      "    codebase_prompt_content = \"\"\n",
      "    included_paths = file_paths.copy()  # Default to all paths\n",
      "    \n",
      "    if reasoning_mode == \"lsp\":\n",
      "        file_paths, file_contents = await get_lsp_snapshot(repo_path, file_paths)\n",
      "        codebase_prompt_content = await format_codebase_for_prompt(file_paths, file_contents)\n",
      "    elif reasoning_mode == \"file_structure\":\n",
      "        codebase_prompt_content = build_file_structure_context(file_paths)\n",
      "    elif reasoning_mode == \"full\":\n",
      "        codebase_prompt_content = await format_codebase_for_prompt(file_paths, file_contents)\n",
      "    elif reasoning_mode == \"none\":\n",
      "        return \"\", []\n",
      "\n",
      "    # Apply token limit if specified\n",
      "    if token_limit and model:\n",
      "        codebase_prompt_content, included_paths = apply_token_limit(\n",
      "            codebase_prompt_content, \n",
      "            token_limit, \n",
      "            model, \n",
      "            log_function,\n",
      "            file_paths=file_paths,\n",
      "            file_contents=file_contents\n",
      "        )\n",
      "\n",
      "    return codebase_prompt_content, included_paths\n",
      "\n",
      "--- File: yellhorn_mcp/formatters/prompt_formatter.py ---\n",
      "\"\"\"Prompt formatting utilities for combining codebase structure and contents.\"\"\"\n",
      "\n",
      "\n",
      "from collections import defaultdict\n",
      "\n",
      "\n",
      "async def format_codebase_for_prompt(file_paths: list[str], file_contents: dict[str, str]) -> str:\n",
      "    \"\"\"Format the codebase information for inclusion in the prompt.\n",
      "\n",
      "    Args:\n",
      "        file_paths: List of file paths.\n",
      "        file_contents: Dictionary mapping file paths to their contents.\n",
      "\n",
      "    Returns:\n",
      "        Formatted string with codebase structure and contents.\n",
      "    \"\"\"\n",
      "    # Start with the file structure tree\n",
      "    codebase_info = build_file_structure_context(file_paths)\n",
      "\n",
      "    # Add file contents if available\n",
      "    if file_contents:\n",
      "        codebase_info += \"\\n\\n<file_contents>\\n\"\n",
      "        for file_path in sorted(file_contents.keys()):\n",
      "            content = file_contents[file_path]\n",
      "            # Skip empty files\n",
      "            if not content.strip():\n",
      "                continue\n",
      "\n",
      "            # Add file header and content\n",
      "            codebase_info += f\"\\n--- File: {file_path} ---\\n\"\n",
      "            codebase_info += content\n",
      "            if not content.endswith(\"\\n\"):\n",
      "                codebase_info += \"\\n\"\n",
      "\n",
      "        codebase_info += \"</file_contents>\"\n",
      "\n",
      "    return codebase_info\n",
      "\n",
      "\n",
      "\n",
      "def build_file_structure_context(file_paths: list[str]) -> str:\n",
      "    \"\"\"Build a codebase info string containing only the file structure.\n",
      "\n",
      "    Args:\n",
      "        file_paths: List of file paths to include.\n",
      "\n",
      "    Returns:\n",
      "        Formatted string with directory tree structure.\n",
      "    \"\"\"\n",
      "    # Group files by directory\n",
      "    dir_structure = defaultdict(list)\n",
      "    for path in file_paths:\n",
      "        parts = path.split(\"/\")\n",
      "        if len(parts) == 1:\n",
      "            # Root level file\n",
      "            dir_structure[\"\"].append(parts[0])\n",
      "        else:\n",
      "            # File in subdirectory\n",
      "            dir_path = \"/\".join(parts[:-1])\n",
      "            filename = parts[-1]\n",
      "            dir_structure[dir_path].append(filename)\n",
      "\n",
      "    # Build tree representation\n",
      "    lines = [\"<codebase_tree>\"]\n",
      "    lines.append(\".\")\n",
      "\n",
      "    # Sort directories for consistent output\n",
      "    sorted_dirs = sorted(dir_structure.keys())\n",
      "\n",
      "    for dir_path in sorted_dirs:\n",
      "        if dir_path:  # Skip root (already shown as \".\")\n",
      "            indent_level = dir_path.count(\"/\")\n",
      "            indent = \"│   \" * indent_level\n",
      "            dir_name = dir_path.split(\"/\")[-1]\n",
      "            lines.append(f\"{indent}├── {dir_name}/\")\n",
      "\n",
      "            # Add files in this directory\n",
      "            indent = \"│   \" * (indent_level + 1)\n",
      "            sorted_files = sorted(dir_structure[dir_path])\n",
      "            for i, filename in enumerate(sorted_files):\n",
      "                if i == len(sorted_files) - 1:\n",
      "                    lines.append(f\"{indent}└── {filename}\")\n",
      "                else:\n",
      "                    lines.append(f\"{indent}├── {filename}\")\n",
      "        else:\n",
      "            # Root level files\n",
      "            sorted_files = sorted(dir_structure[\"\"])\n",
      "            for filename in sorted_files:\n",
      "                lines.append(f\"├── {filename}\")\n",
      "\n",
      "    lines.append(\"</codebase_tree>\")\n",
      "    return \"\\n\".join(lines)\n",
      "\n",
      "--- File: yellhorn_mcp/integrations/gemini_integration.py ---\n",
      "\"\"\"Gemini API integration for Yellhorn MCP.\n",
      "\n",
      "This module handles all Gemini-specific model interactions including:\n",
      "- Gemini 2.5 Pro/Flash API calls\n",
      "- Search grounding configuration\n",
      "- Response parsing and usage tracking\n",
      "\"\"\"\n",
      "\n",
      "from typing import Any\n",
      "\n",
      "from google import genai\n",
      "from google.genai import types as genai_types\n",
      "\n",
      "from yellhorn_mcp.models.metadata_models import CompletionMetadata\n",
      "from yellhorn_mcp.utils.git_utils import YellhornMCPError\n",
      "from yellhorn_mcp.utils.search_grounding_utils import _get_gemini_search_tools, add_citations\n",
      "\n",
      "\n",
      "async def async_generate_content_with_config(\n",
      "    client: genai.Client, model_name: str, prompt: str, generation_config: Any = None\n",
      ") -> genai_types.GenerateContentResponse:\n",
      "    \"\"\"Helper function to call aio.models.generate_content with generation_config.\n",
      "\n",
      "    Args:\n",
      "        client: The Gemini client instance.\n",
      "        model_name: The model name string.\n",
      "        prompt: The prompt content.\n",
      "        generation_config: Optional GenerateContentConfig instance.\n",
      "\n",
      "    Returns:\n",
      "        The response from the Gemini API.\n",
      "\n",
      "    Raises:\n",
      "        YellhornMCPError: If the client doesn't support the required API.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        if generation_config is not None:\n",
      "            return await client.aio.models.generate_content(\n",
      "                model=model_name, contents=prompt, config=generation_config\n",
      "            )\n",
      "        else:\n",
      "            return await client.aio.models.generate_content(model=model_name, contents=prompt)\n",
      "    except AttributeError:\n",
      "        raise YellhornMCPError(\n",
      "            \"Client does not support aio.models.generate_content. \"\n",
      "            \"Please ensure you're using a valid Gemini client.\"\n",
      "        )\n",
      "\n",
      "--- File: yellhorn_mcp/llm_manager.py ---\n",
      "\"\"\"Unified LLM Manager with automatic chunking support and rate limit handling.\"\"\"\n",
      "\n",
      "import asyncio\n",
      "import json\n",
      "import logging\n",
      "import re\n",
      "import time\n",
      "from typing import Any, Dict, List, Optional, Union\n",
      "\n",
      "from google import genai\n",
      "from google.api_core import exceptions as google_exceptions\n",
      "from openai import AsyncOpenAI, RateLimitError\n",
      "from tenacity import (\n",
      "    RetryCallState,\n",
      "    before_sleep_log,\n",
      "    retry,\n",
      "    retry_if_exception,\n",
      "    retry_if_exception_type,\n",
      "    stop_after_attempt,\n",
      "    wait_exponential,\n",
      ")\n",
      "\n",
      "from .token_counter import TokenCounter\n",
      "\n",
      "# Configure logging\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "def log_retry_attempt(retry_state: RetryCallState) -> None:\n",
      "    \"\"\"Log the retry attempt with exponential backoff details.\"\"\"\n",
      "    if retry_state.outcome is None:\n",
      "        return\n",
      "\n",
      "    attempt = retry_state.attempt_number\n",
      "    wait_time = retry_state.outcome_timestamp - retry_state.start_time\n",
      "\n",
      "    logger.warning(\n",
      "        f\"Retrying {retry_state.fn.__name__} after {wait_time:.1f} seconds \"\n",
      "        f\"(attempt {attempt}): {str(retry_state.outcome.exception())}\"\n",
      "    )\n",
      "\n",
      "\n",
      "def is_retryable_error(exception: Exception) -> bool:\n",
      "    \"\"\"Check if the exception is retryable.\"\"\"\n",
      "    # Handle ClientError from google.generativeai which wraps the actual error\n",
      "    if hasattr(exception, \"message\") and hasattr(exception, \"code\"):\n",
      "        error_message = str(exception.message).lower()\n",
      "        error_code = getattr(exception, \"code\", None)\n",
      "\n",
      "        # Check for rate limiting or quota exceeded\n",
      "        if error_code == 429 or \"resource_exhausted\" in error_message or \"quota\" in error_message:\n",
      "            return True\n",
      "\n",
      "    # Check for standard retryable exceptions\n",
      "    if any(\n",
      "        isinstance(exception, exc_type)\n",
      "        for exc_type in [\n",
      "            RateLimitError,\n",
      "            google_exceptions.ResourceExhausted,\n",
      "            google_exceptions.TooManyRequests,\n",
      "            ConnectionError,\n",
      "            asyncio.TimeoutError,\n",
      "        ]\n",
      "    ):\n",
      "        return True\n",
      "\n",
      "    # Check for error messages in string representation\n",
      "    error_message = str(exception).lower()\n",
      "    if any(\n",
      "        term in error_message\n",
      "        for term in [\"resource_exhausted\", \"quota\", \"rate limit\", \"too many requests\"]\n",
      "    ):\n",
      "        return True\n",
      "\n",
      "    return False\n",
      "\n",
      "\n",
      "# Common retry decorator for API calls\n",
      "api_retry = retry(\n",
      "    retry=retry_if_exception(is_retryable_error),\n",
      "    wait=wait_exponential(multiplier=1, min=4, max=60, exp_base=2),\n",
      "    stop=stop_after_attempt(5),\n",
      "    before_sleep=log_retry_attempt,\n",
      "    reraise=True,\n",
      ")\n",
      "\n",
      "\n",
      "class UsageMetadata:\n",
      "    \"\"\"\n",
      "    Unified usage metadata class that handles both OpenAI and Gemini formats.\n",
      "\n",
      "    This class provides a consistent interface for accessing token usage information\n",
      "    regardless of the source (OpenAI API, Gemini API, or dictionary).\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, data: Any = None):\n",
      "        \"\"\"\n",
      "        Initialize UsageMetadata from various sources.\n",
      "\n",
      "        Args:\n",
      "            data: Can be:\n",
      "                - OpenAI CompletionUsage object\n",
      "                - Gemini GenerateContentResponseUsageMetadata object\n",
      "                - Dictionary with token counts\n",
      "                - None (defaults to 0 for all values)\n",
      "        \"\"\"\n",
      "        self.prompt_tokens: int = 0\n",
      "        self.completion_tokens: int = 0\n",
      "        self.total_tokens: int = 0\n",
      "        self.model: Optional[str] = None\n",
      "\n",
      "        if data is None:\n",
      "            return\n",
      "\n",
      "        if isinstance(data, dict):\n",
      "            # Handle dictionary format (our internal format)\n",
      "            self.prompt_tokens = data.get(\"prompt_tokens\", 0)\n",
      "            self.completion_tokens = data.get(\"completion_tokens\", 0)\n",
      "            self.total_tokens = data.get(\"total_tokens\", 0)\n",
      "            self.model = data.get(\"model\")\n",
      "        elif hasattr(data, \"input_tokens\"):\n",
      "            # Response format\n",
      "            self.prompt_tokens = getattr(data, \"input_tokens\", 0)\n",
      "            self.completion_tokens = getattr(data, \"output_tokens\", 0)\n",
      "            self.total_tokens = getattr(data, \"total_tokens\", 0)\n",
      "        elif hasattr(data, \"prompt_tokens\"):\n",
      "            # OpenAI CompletionUsage format\n",
      "            self.prompt_tokens = getattr(data, \"prompt_tokens\", 0)\n",
      "            self.completion_tokens = getattr(data, \"completion_tokens\", 0)\n",
      "            self.total_tokens = getattr(data, \"total_tokens\", 0)\n",
      "        elif hasattr(data, \"prompt_token_count\"):\n",
      "            # Gemini GenerateContentResponseUsageMetadata format\n",
      "            self.prompt_tokens = getattr(data, \"prompt_token_count\", 0)\n",
      "            self.completion_tokens = getattr(data, \"candidates_token_count\", 0)\n",
      "            self.total_tokens = getattr(data, \"total_token_count\", 0)\n",
      "\n",
      "    @property\n",
      "    def prompt_token_count(self) -> int:\n",
      "        \"\"\"Gemini-style property for compatibility.\"\"\"\n",
      "        return self.prompt_tokens\n",
      "\n",
      "    @property\n",
      "    def candidates_token_count(self) -> int:\n",
      "        \"\"\"Gemini-style property for compatibility.\"\"\"\n",
      "        return self.completion_tokens\n",
      "\n",
      "    @property\n",
      "    def total_token_count(self) -> int:\n",
      "        \"\"\"Gemini-style property for compatibility.\"\"\"\n",
      "        return self.total_tokens\n",
      "\n",
      "    def to_dict(self) -> Dict[str, Any]:\n",
      "        \"\"\"Convert to dictionary format.\"\"\"\n",
      "        result = {\n",
      "            \"prompt_tokens\": self.prompt_tokens,\n",
      "            \"completion_tokens\": self.completion_tokens,\n",
      "            \"total_tokens\": self.total_tokens,\n",
      "        }\n",
      "        if self.model:\n",
      "            result[\"model\"] = self.model\n",
      "        return result\n",
      "\n",
      "    def __bool__(self) -> bool:\n",
      "        \"\"\"Check if we have valid usage data.\"\"\"\n",
      "        try:\n",
      "            return self.total_tokens is not None and self.total_tokens > 0\n",
      "        except (TypeError, AttributeError):\n",
      "            return False\n",
      "\n",
      "\n",
      "class ChunkingStrategy:\n",
      "    \"\"\"Strategies for splitting text into chunks while respecting token limits and natural boundaries.\"\"\"\n",
      "\n",
      "    @staticmethod\n",
      "    def _find_split_point(text: str, max_length: int) -> int:\n",
      "        \"\"\"\n",
      "        Find the best split point in text before max_length.\n",
      "        Prefers splitting at paragraph breaks, then at sentence boundaries, then at word boundaries.\n",
      "        \"\"\"\n",
      "        # First try to split at paragraph breaks\n",
      "        para_break = text.rfind(\"\\n\", 0, max_length)\n",
      "        if para_break > 0:\n",
      "            return para_break + 2  # Include the newlines\n",
      "\n",
      "        # Then try to split at sentence boundaries\n",
      "        sentence_break = max(\n",
      "            text.rfind(\". \", 0, max_length),\n",
      "            text.rfind(\"! \", 0, max_length),\n",
      "            text.rfind(\"? \", 0, max_length),\n",
      "            text.rfind(\"\\n\", 0, max_length),  # Or at least at a newline\n",
      "        )\n",
      "\n",
      "        if sentence_break > 0:\n",
      "            return sentence_break + 1  # Include the space or newline\n",
      "\n",
      "        # Finally, split at the last space before max_length\n",
      "        space_break = text.rfind(\" \", 0, max_length)\n",
      "        if space_break > 0:\n",
      "            return space_break\n",
      "\n",
      "        # If no good break found, split at max_length\n",
      "        return max_length\n",
      "\n",
      "    @staticmethod\n",
      "    def split_by_sentences(\n",
      "        text: str,\n",
      "        max_tokens: int,\n",
      "        token_counter: TokenCounter,\n",
      "        model: str,\n",
      "        overlap_ratio: float = 0.1,\n",
      "        safety_margin_tokens: int = 50,\n",
      "    ) -> List[str]:\n",
      "        \"\"\"\n",
      "        Split text into chunks that don't exceed max_tokens, trying to respect sentence boundaries.\n",
      "\n",
      "        Args:\n",
      "            text: Text to split\n",
      "            max_tokens: Maximum tokens per chunk\n",
      "            token_counter: TokenCounter instance\n",
      "            model: Model name for token counting\n",
      "            overlap_ratio: Ratio of overlap between chunks (0.0 to 0.5)\n",
      "\n",
      "        Returns:\n",
      "            List of text chunks\n",
      "        \"\"\"\n",
      "        if not text.strip():\n",
      "            return []\n",
      "\n",
      "        target_tokens = max_tokens - safety_margin_tokens\n",
      "        chunks = []\n",
      "        remaining_text = text\n",
      "        overlap_tokens = int(max_tokens * overlap_ratio)\n",
      "\n",
      "        while remaining_text:\n",
      "            # Estimate chunk size\n",
      "            estimated_chars = len(remaining_text)\n",
      "            estimated_tokens = token_counter.count_tokens(remaining_text[:estimated_chars], model)\n",
      "\n",
      "            # Adjust chunk size based on token count\n",
      "            if estimated_tokens > target_tokens:\n",
      "                # Binary search for the right split point\n",
      "                low = 0\n",
      "                high = len(remaining_text)\n",
      "                best_split = len(remaining_text)\n",
      "\n",
      "                while low <= high:\n",
      "                    mid = (low + high) // 2\n",
      "                    chunk = remaining_text[:mid]\n",
      "                    tokens = token_counter.count_tokens(chunk, model)\n",
      "\n",
      "                    if tokens <= target_tokens:\n",
      "                        best_split = mid\n",
      "                        low = mid + 1\n",
      "                    else:\n",
      "                        high = mid - 1\n",
      "\n",
      "                # Find the best split point near the token limit\n",
      "                if best_split < len(remaining_text):\n",
      "                    split_pos = ChunkingStrategy._find_split_point(\n",
      "                        remaining_text[:best_split], best_split\n",
      "                    )\n",
      "                    # Ensure we make progress\n",
      "                    if split_pos == 0 or split_pos == best_split:\n",
      "                        split_pos = best_split\n",
      "                else:\n",
      "                    split_pos = best_split\n",
      "            else:\n",
      "                split_pos = len(remaining_text)\n",
      "\n",
      "            # Extract the chunk and remaining text\n",
      "            chunk = remaining_text[:split_pos].strip()\n",
      "            remaining_text = remaining_text[split_pos:].strip()\n",
      "\n",
      "            if not chunk:\n",
      "                break\n",
      "\n",
      "            chunks.append(chunk)\n",
      "\n",
      "            # Add overlap if there's more text to process\n",
      "            if remaining_text and overlap_tokens > 0:\n",
      "                # Find the start of the next sentence for overlap\n",
      "                next_sentence_start = 0\n",
      "                for i, c in enumerate(remaining_text):\n",
      "                    if c in \".!?\":\n",
      "                        next_sentence_start = i + 1\n",
      "                        if (\n",
      "                            next_sentence_start < len(remaining_text)\n",
      "                            and remaining_text[next_sentence_start] == \" \"\n",
      "                        ):\n",
      "                            next_sentence_start += 1\n",
      "                        break\n",
      "\n",
      "                if next_sentence_start > 0 and next_sentence_start < len(remaining_text):\n",
      "                    overlap_text = remaining_text[:next_sentence_start]\n",
      "                    remaining_text = overlap_text + remaining_text[next_sentence_start:]\n",
      "\n",
      "        return chunks\n",
      "\n",
      "    @staticmethod\n",
      "    def split_by_paragraphs(\n",
      "        text: str,\n",
      "        max_tokens: int,\n",
      "        token_counter: TokenCounter,\n",
      "        model: str,\n",
      "        overlap_ratio: float = 0.1,\n",
      "        safety_margin_tokens: int = 50,\n",
      "    ) -> List[str]:\n",
      "        \"\"\"\n",
      "        Split text into chunks by paragraphs, respecting token limits.\n",
      "\n",
      "        Args:\n",
      "            text: Text to split\n",
      "            max_tokens: Maximum tokens per chunk\n",
      "            token_counter: TokenCounter instance\n",
      "            model: Model name for token counting\n",
      "            overlap_ratio: Ratio of overlap between chunks (0.0 to 0.5)\n",
      "\n",
      "        Returns:\n",
      "            List of text chunks\n",
      "        \"\"\"\n",
      "        if not text.strip():\n",
      "            return []\n",
      "\n",
      "        target_tokens = max_tokens - safety_margin_tokens\n",
      "        # First split by paragraphs\n",
      "        paragraphs = [p for p in text.split(\"\\n\") if p.strip()]\n",
      "        chunks = []\n",
      "        current_chunk = []\n",
      "        current_tokens = 0\n",
      "\n",
      "        for para in paragraphs:\n",
      "            para = para.strip()\n",
      "            if not para:\n",
      "                continue\n",
      "\n",
      "            para_tokens = token_counter.count_tokens(para, model)\n",
      "\n",
      "            # If paragraph is too large, split it by sentences\n",
      "            if para_tokens > target_tokens:\n",
      "                # Flush current chunk if not empty\n",
      "                if current_chunk:\n",
      "                    chunks.append(\"\\n\".join(current_chunk))\n",
      "                    current_chunk = []\n",
      "                    current_tokens = 0\n",
      "\n",
      "                # Split the large paragraph by sentences\n",
      "                chunks.extend(\n",
      "                    ChunkingStrategy.split_by_sentences(\n",
      "                        para, max_tokens, token_counter, model, overlap_ratio, safety_margin_tokens\n",
      "                    )\n",
      "                )\n",
      "            # If adding this paragraph would exceed the token limit\n",
      "            elif current_tokens + para_tokens > target_tokens and current_chunk:\n",
      "                chunks.append(\"\\n\".join(current_chunk))\n",
      "\n",
      "                # Add overlap from previous chunk if needed\n",
      "                if overlap_ratio > 0 and chunks:\n",
      "                    overlap_tokens = int(max_tokens * overlap_ratio)\n",
      "                    overlap_text = \"\\n\".join(current_chunk)\n",
      "                    overlap_text = overlap_text[\n",
      "                        -overlap_tokens * 4 :\n",
      "                    ]  # Rough estimate of chars per token\n",
      "                    current_chunk = [overlap_text, para]\n",
      "                    current_tokens = token_counter.count_tokens(\"\\n\".join(current_chunk), model)\n",
      "                else:\n",
      "                    current_chunk = [para]\n",
      "                    current_tokens = para_tokens\n",
      "            else:\n",
      "                current_chunk.append(para)\n",
      "                current_tokens += para_tokens + 2  # Account for newlines\n",
      "\n",
      "        # Add the last chunk if not empty\n",
      "        if current_chunk:\n",
      "            chunks.append(\"\\n\".join(current_chunk))\n",
      "\n",
      "        return chunks\n",
      "\n",
      "\n",
      "class LLMManager:\n",
      "    \"\"\"Unified manager for LLM calls with automatic chunking.\"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        openai_client: Optional[AsyncOpenAI] = None,\n",
      "        gemini_client: Optional[genai.Client] = None,\n",
      "        config: Optional[Dict[str, Any]] = None,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Initialize LLM Manager.\n",
      "\n",
      "        Args:\n",
      "            openai_client: OpenAI client instance\n",
      "            gemini_client: Gemini client instance\n",
      "            config: Configuration dictionary\n",
      "        \"\"\"\n",
      "        self.token_counter = TokenCounter(config)\n",
      "        self.openai_client = openai_client\n",
      "        self.gemini_client = gemini_client\n",
      "        self.config = config or {}\n",
      "\n",
      "        # Default configuration\n",
      "        self.safety_margin = self.config.get(\"safety_margin_tokens\", 1000)\n",
      "        self.overlap_ratio = self.config.get(\"overlap_ratio\", 0.1)\n",
      "        self.aggregation_strategy = self.config.get(\"aggregation_strategy\", \"concatenate\")\n",
      "        self.chunk_strategy = self.config.get(\"chunk_strategy\", \"sentences\")\n",
      "\n",
      "        # Track usage metadata from last call\n",
      "        self._last_usage_metadata = None\n",
      "\n",
      "    def _is_openai_model(self, model: str) -> bool:\n",
      "        \"\"\"Check if model is an OpenAI model.\"\"\"\n",
      "        openai_prefixes = [\"gpt-\", \"o3\", \"o4-\"]\n",
      "        return any(model.startswith(prefix) for prefix in openai_prefixes)\n",
      "\n",
      "    def _is_gemini_model(self, model: str) -> bool:\n",
      "        \"\"\"Check if model is a Gemini model.\"\"\"\n",
      "        return model.startswith(\"gemini-\") or model.startswith(\"mock-\")\n",
      "\n",
      "    def _is_deep_research_model(self, model: str) -> bool:\n",
      "        \"\"\"Check if model is a deep research model that supports web search and code interpreter tools.\"\"\"\n",
      "        # Deep research models typically include o3, o4, and other reasoning models\n",
      "        deep_research_prefixes = [\"o3\", \"o4-\"]\n",
      "        return any(model.startswith(prefix) for prefix in deep_research_prefixes)\n",
      "\n",
      "    async def call_llm(\n",
      "        self,\n",
      "        prompt: str,\n",
      "        model: str,\n",
      "        temperature: float = 0.7,\n",
      "        system_message: Optional[str] = None,\n",
      "        response_format: Optional[str] = None,\n",
      "        **kwargs,\n",
      "    ) -> Union[str, Dict[str, Any]]:\n",
      "        \"\"\"\n",
      "        Call LLM with automatic chunking if needed.\n",
      "\n",
      "        Args:\n",
      "            prompt: The prompt to send\n",
      "            model: Model name\n",
      "            temperature: Temperature for generation\n",
      "            system_message: Optional system message\n",
      "            response_format: Optional response format (e.g., \"json\")\n",
      "            **kwargs: Additional model-specific parameters\n",
      "\n",
      "        Returns:\n",
      "            Generated response (string or dict if JSON format)\n",
      "        \"\"\"\n",
      "        # Check if chunking is needed\n",
      "        if not self.token_counter.can_fit_in_context(prompt, model, self.safety_margin):\n",
      "            return await self._chunked_call(\n",
      "                prompt, model, temperature, system_message, response_format, **kwargs\n",
      "            )\n",
      "\n",
      "        # Single call\n",
      "        return await self._single_call(\n",
      "            prompt, model, temperature, system_message, response_format, **kwargs\n",
      "        )\n",
      "\n",
      "    async def _single_call(\n",
      "        self,\n",
      "        prompt: str,\n",
      "        model: str,\n",
      "        temperature: float,\n",
      "        system_message: Optional[str],\n",
      "        response_format: Optional[str],\n",
      "        **kwargs,\n",
      "    ) -> Union[str, Dict[str, Any]]:\n",
      "        \"\"\"Make a single LLM call.\"\"\"\n",
      "        if self._is_openai_model(model):\n",
      "            return await self._call_openai(\n",
      "                prompt, model, temperature, system_message, response_format, **kwargs\n",
      "            )\n",
      "        elif self._is_gemini_model(model):\n",
      "            return await self._call_gemini(\n",
      "                prompt, model, temperature, system_message, response_format, **kwargs\n",
      "            )\n",
      "        else:\n",
      "            raise ValueError(f\"Unknown model type: {model}\")\n",
      "\n",
      "    @api_retry\n",
      "    async def _call_openai(\n",
      "        self,\n",
      "        prompt: str,\n",
      "        model: str,\n",
      "        temperature: float,\n",
      "        system_message: Optional[str],\n",
      "        response_format: Optional[str],\n",
      "        **kwargs,\n",
      "    ) -> Union[str, Dict[str, Any]]:\n",
      "        \"\"\"Call OpenAI API with automatic retry on rate limits.\n",
      "\n",
      "        Args:\n",
      "            prompt: The prompt to send\n",
      "            model: Model name\n",
      "            temperature: Temperature for generation\n",
      "            system_message: Optional system message\n",
      "            response_format: Optional response format (e.g., \"json\")\n",
      "            **kwargs: Additional parameters for the OpenAI API\n",
      "\n",
      "        Returns:\n",
      "            Generated response (string or dict if JSON format)\n",
      "\n",
      "        Raises:\n",
      "            RateLimitError: If rate limited and max retries exceeded\n",
      "            ValueError: If OpenAI client is not initialized\n",
      "        \"\"\"\n",
      "        if not self.openai_client:\n",
      "            raise ValueError(\"OpenAI client not initialized\")\n",
      "\n",
      "        # Build params for Responses API\n",
      "        params = {\n",
      "            \"model\": model,\n",
      "            \"input\": prompt,  # User prompt goes to input\n",
      "            \"temperature\": 1.0 if model.startswith(\"o\") else temperature,\n",
      "            # store: false can be set to not persist the conversation state\n",
      "            **kwargs,\n",
      "        }\n",
      "\n",
      "        # System message goes to instructions\n",
      "        if system_message:\n",
      "            params[\"instructions\"] = system_message\n",
      "\n",
      "        # Enable Deep Research tools for supported models\n",
      "        if self._is_deep_research_model(model):\n",
      "            logger.info(f\"Enabling Deep Research tools for model {model}\")\n",
      "            params[\"tools\"] = [\n",
      "                {\"type\": \"web_search_preview\"},\n",
      "                {\"type\": \"code_interpreter\", \"container\": {\"type\": \"auto\", \"file_ids\": []}},\n",
      "            ]\n",
      "\n",
      "        if response_format == \"json\":\n",
      "            params[\"response_format\"] = {\"type\": \"json_object\"}\n",
      "\n",
      "        try:\n",
      "            # Use the new Responses API endpoint\n",
      "            response = await self.openai_client.responses.create(**params)\n",
      "\n",
      "            # Extract content from new response structure\n",
      "            # Handle case where output might be a list (Deep Research models sometimes return multiple outputs)\n",
      "            if hasattr(response, \"output_text\"):\n",
      "                content = response.output_text\n",
      "            else:\n",
      "                content = response.output[0].content[0].text\n",
      "\n",
      "            # Store usage metadata (same structure as before)\n",
      "            if hasattr(response, \"usage\"):\n",
      "                self._last_usage_metadata = UsageMetadata(response.usage)\n",
      "\n",
      "            if response_format == \"json\":\n",
      "                try:\n",
      "                    return json.loads(content)\n",
      "                except json.JSONDecodeError:\n",
      "                    return {\"error\": \"Failed to parse JSON\", \"content\": content}\n",
      "\n",
      "            return content\n",
      "\n",
      "        except Exception as e:\n",
      "            logger.error(f\"OpenAI API call failed: {str(e)}\")\n",
      "            raise\n",
      "\n",
      "    @api_retry\n",
      "    async def _call_gemini(\n",
      "        self,\n",
      "        prompt: str,\n",
      "        model: str,\n",
      "        temperature: float,\n",
      "        system_message: Optional[str],\n",
      "        response_format: Optional[str],\n",
      "        **kwargs,\n",
      "    ) -> Union[str, Dict[str, Any]]:\n",
      "        \"\"\"Call Gemini API with automatic retry on rate limits.\n",
      "\n",
      "        Args:\n",
      "            prompt: The prompt to send\n",
      "            model: Model name\n",
      "            temperature: Temperature for generation\n",
      "            system_message: Optional system message\n",
      "            response_format: Optional response format (e.g., \"json\")\n",
      "            **kwargs: Additional parameters for the Gemini API\n",
      "\n",
      "        Returns:\n",
      "            Generated response (string or dict if JSON format)\n",
      "\n",
      "        Raises:\n",
      "            google.api_core.exceptions.ResourceExhausted: If rate limited and max retries exceeded\n",
      "            ValueError: If Gemini client is not configured\n",
      "        \"\"\"\n",
      "        if not self.gemini_client:\n",
      "            raise ValueError(\"Gemini client not configured\")\n",
      "\n",
      "        # Combine system message with prompt if provided\n",
      "        full_prompt = prompt\n",
      "        if system_message:\n",
      "            full_prompt = f\"{system_message}\\n\\n{prompt}\"\n",
      "\n",
      "        # Import GenerateContentConfig with fallback\n",
      "        try:\n",
      "            from google.genai.types import GenerateContentConfig\n",
      "\n",
      "            config_class = GenerateContentConfig\n",
      "        except ImportError:\n",
      "            # Fallback to dict config\n",
      "            config_class = dict\n",
      "\n",
      "        # Extract generation_config from kwargs if present (for search grounding)\n",
      "        generation_config = kwargs.pop(\"generation_config\", None)\n",
      "\n",
      "        # Build config\n",
      "        config_dict = {\n",
      "            \"temperature\": temperature,\n",
      "            \"response_mime_type\": \"application/json\" if response_format == \"json\" else \"text/plain\",\n",
      "        }\n",
      "\n",
      "        # Add any additional kwargs (excluding generation_config which we already extracted)\n",
      "        config_dict.update(kwargs)\n",
      "\n",
      "        # If we have generation_config (search grounding), merge it with our config\n",
      "        if generation_config and config_class == GenerateContentConfig:\n",
      "            # Extract tools from generation_config if it has them\n",
      "            if hasattr(generation_config, \"tools\") and generation_config.tools:\n",
      "                config_dict[\"tools\"] = generation_config.tools\n",
      "            # Extract any other attributes from generation_config\n",
      "            for attr in [\n",
      "                \"response_schema\",\n",
      "                \"response_mime_type\",\n",
      "                \"candidate_count\",\n",
      "                \"stop_sequences\",\n",
      "                \"max_output_tokens\",\n",
      "                \"temperature\",\n",
      "                \"top_p\",\n",
      "                \"top_k\",\n",
      "            ]:\n",
      "                if hasattr(generation_config, attr):\n",
      "                    value = getattr(generation_config, attr)\n",
      "                    config_dict[attr] = value\n",
      "\n",
      "        # Create config instance\n",
      "        if config_class == GenerateContentConfig:\n",
      "            config = config_class(**config_dict)\n",
      "        else:\n",
      "            config = config_dict\n",
      "\n",
      "        try:\n",
      "            # Prepare API call parameters\n",
      "            api_params = {\"model\": f\"models/{model}\", \"contents\": full_prompt, \"config\": config}\n",
      "\n",
      "            # Make the API call\n",
      "            response = await self.gemini_client.aio.models.generate_content(**api_params)\n",
      "\n",
      "            # Extract text from response\n",
      "            if hasattr(response, \"text\"):\n",
      "                content = response.text\n",
      "            else:\n",
      "                content = str(response)\n",
      "\n",
      "            # Store usage metadata if available\n",
      "            if hasattr(response, \"usage_metadata\"):\n",
      "                usage = response.usage_metadata\n",
      "                self._last_usage_metadata = UsageMetadata(usage)\n",
      "\n",
      "            self._last_gemini_response = response\n",
      "\n",
      "            # Parse JSON if requested\n",
      "            if response_format == \"json\":\n",
      "                # Try to extract JSON from the response\n",
      "                import re\n",
      "\n",
      "                json_pattern = r\"\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}\"\n",
      "                json_matches = re.findall(json_pattern, content, re.DOTALL)\n",
      "\n",
      "                if json_matches:\n",
      "                    try:\n",
      "                        return json.loads(json_matches[0])\n",
      "                    except json.JSONDecodeError:\n",
      "                        return {\"error\": \"No valid JSON found in response\", \"content\": content}\n",
      "                else:\n",
      "                    return {\"error\": \"No JSON content found in response\"}\n",
      "\n",
      "            return content\n",
      "\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Gemini API call failed: {str(e)}\")\n",
      "            raise\n",
      "\n",
      "    async def _chunked_call(\n",
      "        self,\n",
      "        prompt: str,\n",
      "        model: str,\n",
      "        temperature: float,\n",
      "        system_message: Optional[str],\n",
      "        response_format: Optional[str],\n",
      "        **kwargs,\n",
      "    ) -> Union[str, Dict[str, Any]]:\n",
      "        \"\"\"Make chunked LLM calls and aggregate results with rate limit handling.\n",
      "\n",
      "        Args:\n",
      "            prompt: The prompt to send\n",
      "            model: Model name\n",
      "            temperature: Temperature for generation\n",
      "            system_message: Optional system message\n",
      "            response_format: Optional response format (e.g., \"json\")\n",
      "            **kwargs: Additional parameters for the LLM API\n",
      "\n",
      "        Returns:\n",
      "            Aggregated response (string or dict if JSON format)\n",
      "        \"\"\"\n",
      "        # Calculate available tokens for content\n",
      "        model_limit = self.token_counter.get_model_limit(model)\n",
      "        system_tokens = self.token_counter.count_tokens(system_message or \"\", model)\n",
      "        available_tokens = model_limit - system_tokens - self.safety_margin\n",
      "\n",
      "        # Split prompt into chunks\n",
      "        chunks = self._chunk_prompt(prompt, model, available_tokens)\n",
      "\n",
      "        # Log the number of chunks created\n",
      "        logger.info(f\"Split prompt into {len(chunks)} chunks for model {model}\")\n",
      "\n",
      "        # Process chunks\n",
      "        responses = []\n",
      "        total_usage = UsageMetadata()\n",
      "\n",
      "        for i, chunk in enumerate(chunks):\n",
      "            # Add context for multi-chunk processing\n",
      "            chunk_prompt = chunk\n",
      "            if len(chunks) > 1:\n",
      "                chunk_prompt = f\"[Chunk {i+1}/{len(chunks)}]\\n\\n{chunk}\"\n",
      "                if i > 0:\n",
      "                    chunk_prompt = f\"[Continuing from previous chunk...]\\n\\n{chunk_prompt}\"\n",
      "\n",
      "            # Debug log for each LLM call\n",
      "            logger.debug(\n",
      "                f\"Making LLM call {i+1}/{len(chunks)} to model {model} with chunk size: {len(chunk_prompt)} characters\"\n",
      "            )\n",
      "\n",
      "            response = await self._single_call(\n",
      "                chunk_prompt, model, temperature, system_message, response_format, **kwargs\n",
      "            )\n",
      "            responses.append(response)\n",
      "\n",
      "            # Debug log for response received\n",
      "            logger.debug(\n",
      "                f\"Received response from LLM call {i+1}/{len(chunks)}, response length: {len(str(response)) if response else 0} characters\"\n",
      "            )\n",
      "\n",
      "            # Aggregate usage metadata\n",
      "            if self._last_usage_metadata:\n",
      "                total_usage.prompt_tokens += self._last_usage_metadata.prompt_tokens\n",
      "                total_usage.completion_tokens += self._last_usage_metadata.completion_tokens\n",
      "                total_usage.total_tokens += self._last_usage_metadata.total_tokens\n",
      "\n",
      "        # Store aggregated usage\n",
      "        self._last_usage_metadata = total_usage\n",
      "\n",
      "        # Aggregate responses\n",
      "        return self._aggregate_responses(responses, response_format)\n",
      "\n",
      "    def _chunk_prompt(self, prompt: str, model: str, max_tokens: int) -> List[str]:\n",
      "        \"\"\"Split prompt into chunks based on strategy.\"\"\"\n",
      "        if self.chunk_strategy == \"paragraphs\":\n",
      "            return ChunkingStrategy.split_by_paragraphs(\n",
      "                prompt, max_tokens, self.token_counter, model, self.overlap_ratio\n",
      "            )\n",
      "        else:  # default to sentences\n",
      "            return ChunkingStrategy.split_by_sentences(\n",
      "                prompt, max_tokens, self.token_counter, model, self.overlap_ratio\n",
      "            )\n",
      "\n",
      "    def _aggregate_responses(\n",
      "        self, responses: List[Union[str, Dict]], response_format: Optional[str]\n",
      "    ) -> Union[str, Dict[str, Any]]:\n",
      "        \"\"\"Aggregate multiple responses based on strategy.\"\"\"\n",
      "        if response_format == \"json\":\n",
      "            # For JSON responses, try to merge\n",
      "            if all(isinstance(r, dict) for r in responses):\n",
      "                # Merge dictionaries\n",
      "                result = {}\n",
      "                for resp in responses:\n",
      "                    if isinstance(resp, dict):\n",
      "                        # Deep merge logic\n",
      "                        for key, value in resp.items():\n",
      "                            if key in result:\n",
      "                                if isinstance(result[key], list) and isinstance(value, list):\n",
      "                                    result[key].extend(value)\n",
      "                                elif isinstance(result[key], dict) and isinstance(value, dict):\n",
      "                                    result[key].update(value)\n",
      "                                else:\n",
      "                                    # Create list of values\n",
      "                                    if not isinstance(result[key], list):\n",
      "                                        result[key] = [result[key]]\n",
      "                                    result[key].append(value)\n",
      "                            else:\n",
      "                                result[key] = value\n",
      "                return result\n",
      "            else:\n",
      "                # Fallback to list of responses\n",
      "                return {\"chunks\": responses}\n",
      "\n",
      "        # For text responses\n",
      "        if self.aggregation_strategy == \"summarize\":\n",
      "            # Would require another LLM call to summarize\n",
      "            # For now, fall back to concatenation\n",
      "            pass\n",
      "\n",
      "        # Default: concatenate\n",
      "        text_responses = [str(r) for r in responses]\n",
      "        return \"\\n\\n---\\n\\n\".join(text_responses)\n",
      "\n",
      "    async def call_llm_with_citations(\n",
      "        self,\n",
      "        prompt: str,\n",
      "        model: str,\n",
      "        temperature: float = 0.7,\n",
      "        system_message: Optional[str] = None,\n",
      "        response_format: Optional[str] = None,\n",
      "        **kwargs,\n",
      "    ) -> Dict[str, Any]:\n",
      "        \"\"\"\n",
      "        Call LLM and return both response and citation metadata if available.\n",
      "\n",
      "        This is specifically useful for Gemini models with search grounding enabled.\n",
      "\n",
      "        Args:\n",
      "            prompt: The prompt to send\n",
      "            model: Model name\n",
      "            temperature: Temperature for generation\n",
      "            system_message: Optional system message\n",
      "            response_format: Optional response format (e.g., \"json\")\n",
      "            **kwargs: Additional arguments passed to the LLM\n",
      "\n",
      "        Returns:\n",
      "            Dictionary with 'content', 'usage_metadata', and optionally 'grounding_metadata'\n",
      "        \"\"\"\n",
      "        # Reset last response\n",
      "        self._last_gemini_response = None\n",
      "        self._last_usage_metadata = None\n",
      "\n",
      "        # Make the regular call\n",
      "        content = await self.call_llm(\n",
      "            prompt=prompt,\n",
      "            model=model,\n",
      "            temperature=temperature,\n",
      "            system_message=system_message,\n",
      "            response_format=response_format,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        # Build result with content and usage\n",
      "        result = {\n",
      "            \"content\": content,\n",
      "            \"usage_metadata\": (\n",
      "                self._last_usage_metadata if self._last_usage_metadata else UsageMetadata()\n",
      "            ),\n",
      "        }\n",
      "\n",
      "        # Check if we have grounding metadata from Gemini\n",
      "        if self._is_gemini_model(model) and hasattr(self, \"_last_gemini_response\"):\n",
      "            response = getattr(self, \"_last_gemini_response\", None)\n",
      "            if response:\n",
      "                # Check for grounding metadata in response directly\n",
      "                if hasattr(response, \"grounding_metadata\") and response.grounding_metadata:\n",
      "                    result[\"grounding_metadata\"] = response.grounding_metadata\n",
      "                # Check for grounding metadata in candidates[0] (most common location)\n",
      "                elif (\n",
      "                    hasattr(response, \"candidates\")\n",
      "                    and response.candidates\n",
      "                    and len(response.candidates) > 0\n",
      "                    and hasattr(response.candidates[0], \"grounding_metadata\")\n",
      "                    and response.candidates[0].grounding_metadata\n",
      "                ):\n",
      "                    result[\"grounding_metadata\"] = response.candidates[0].grounding_metadata\n",
      "\n",
      "        return result\n",
      "\n",
      "    async def call_llm_with_usage(\n",
      "        self,\n",
      "        prompt: str,\n",
      "        model: str,\n",
      "        temperature: float = 0.7,\n",
      "        system_message: Optional[str] = None,\n",
      "        response_format: Optional[str] = None,\n",
      "        **kwargs,\n",
      "    ) -> Dict[str, Any]:\n",
      "        \"\"\"\n",
      "        Call LLM and return both response content and usage metadata.\n",
      "\n",
      "        Args:\n",
      "            prompt: The prompt to send\n",
      "            model: Model name\n",
      "            temperature: Temperature for generation\n",
      "            system_message: Optional system message\n",
      "            response_format: Optional response format (e.g., \"json\")\n",
      "            **kwargs: Additional arguments passed to the LLM\n",
      "\n",
      "        Returns:\n",
      "            Dictionary with 'content' and 'usage_metadata' (as UsageMetadata object)\n",
      "        \"\"\"\n",
      "        # Reset usage metadata\n",
      "        self._last_usage_metadata = None\n",
      "\n",
      "        # Make the regular call\n",
      "        content = await self.call_llm(\n",
      "            prompt=prompt,\n",
      "            model=model,\n",
      "            temperature=temperature,\n",
      "            system_message=system_message,\n",
      "            response_format=response_format,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        # Return content and usage\n",
      "        return {\n",
      "            \"content\": content,\n",
      "            \"usage_metadata\": (\n",
      "                self._last_usage_metadata if self._last_usage_metadata else UsageMetadata()\n",
      "            ),\n",
      "        }\n",
      "\n",
      "    def get_last_usage_metadata(self) -> Optional[UsageMetadata]:\n",
      "        \"\"\"\n",
      "        Get the usage metadata from the last LLM call.\n",
      "\n",
      "        Returns:\n",
      "            UsageMetadata object or None if not available\n",
      "        \"\"\"\n",
      "        return self._last_usage_metadata\n",
      "\n",
      "--- File: yellhorn_mcp/models/metadata_models.py ---\n",
      "\"\"\"Metadata models for Yellhorn MCP GitHub issue comments.\"\"\"\n",
      "\n",
      "from datetime import datetime\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "\n",
      "class UsageMetrics(BaseModel):\n",
      "    \"\"\"Token usage metrics from LLM API calls.\"\"\"\n",
      "\n",
      "    prompt_tokens: int = Field(description=\"Number of prompt/input tokens\")\n",
      "    completion_tokens: int = Field(description=\"Number of completion/output tokens\")\n",
      "    total_tokens: int = Field(description=\"Total tokens used\")\n",
      "    model_name: str = Field(description=\"Model name used for the request\")\n",
      "\n",
      "\n",
      "class SubmissionMetadata(BaseModel):\n",
      "    \"\"\"Metadata for the initial submission comment when a workplan or judgement is requested.\"\"\"\n",
      "\n",
      "    status: str = Field(description=\"Current status (e.g., 'Generating workplan...')\")\n",
      "    model_name: str = Field(description=\"LLM model name being used\")\n",
      "    search_grounding_enabled: bool = Field(description=\"Whether search grounding is enabled\")\n",
      "    yellhorn_version: str = Field(description=\"Version of Yellhorn MCP\")\n",
      "    submitted_urls: list[str] | None = Field(default=None, description=\"URLs found in the request\")\n",
      "    codebase_reasoning_mode: str = Field(\n",
      "        description=\"The codebase reasoning mode (full, lsp, file_structure, none)\"\n",
      "    )\n",
      "    timestamp: datetime = Field(description=\"Timestamp of submission\")\n",
      "\n",
      "\n",
      "class CompletionMetadata(BaseModel):\n",
      "    \"\"\"Metadata for the completion comment after LLM processing finishes.\"\"\"\n",
      "\n",
      "    model_name: str = Field(description=\"Model name requested (e.g., 'gemini-1.5-pro-latest')\")\n",
      "    status: str = Field(\n",
      "        description=\"Completion status (e.g., '✅ Workplan generated successfully')\"\n",
      "    )\n",
      "    generation_time_seconds: float = Field(description=\"Time taken for LLM generation\")\n",
      "    input_tokens: int | None = Field(default=None, description=\"Number of input tokens\")\n",
      "    output_tokens: int | None = Field(default=None, description=\"Number of output tokens\")\n",
      "    total_tokens: int | None = Field(default=None, description=\"Total tokens used\")\n",
      "    estimated_cost: float | None = Field(default=None, description=\"Estimated cost in USD\")\n",
      "    model_version_used: str | None = Field(\n",
      "        default=None, description=\"Actual model version reported by API\"\n",
      "    )\n",
      "    system_fingerprint: str | None = Field(default=None, description=\"OpenAI system fingerprint\")\n",
      "    search_results_used: int | None = Field(\n",
      "        default=None, description=\"Number of search results used (Gemini)\"\n",
      "    )\n",
      "    finish_reason: str | None = Field(default=None, description=\"LLM finish reason\")\n",
      "    safety_ratings: list[dict] | None = Field(\n",
      "        default=None, description=\"Safety ratings from the model\"\n",
      "    )\n",
      "    context_size_chars: int | None = Field(\n",
      "        default=None, description=\"Total characters in the prompt\"\n",
      "    )\n",
      "    warnings: list[str] | None = Field(default=None, description=\"Any warnings to report\")\n",
      "    timestamp: datetime | None = Field(default=None, description=\"Timestamp of completion\")\n",
      "\n",
      "--- File: yellhorn_mcp/processors/__init__.py ---\n",
      "\"\"\"Processors for Yellhorn MCP.\"\"\"\n",
      "\n",
      "from yellhorn_mcp.processors.context_processor import process_context_curation_async\n",
      "from yellhorn_mcp.processors.judgement_processor import get_git_diff, process_judgement_async\n",
      "from yellhorn_mcp.processors.workplan_processor import (\n",
      "    build_file_structure_context,\n",
      "    format_codebase_for_prompt,\n",
      "    get_codebase_snapshot,\n",
      "    process_revision_async,\n",
      "    process_workplan_async,\n",
      ")\n",
      "\n",
      "__all__ = [\n",
      "    \"process_context_curation_async\",\n",
      "    \"process_judgement_async\",\n",
      "    \"process_workplan_async\",\n",
      "    \"process_revision_async\",\n",
      "    \"get_git_diff\",\n",
      "    \"get_codebase_snapshot\",\n",
      "    \"build_file_structure_context\",\n",
      "    \"format_codebase_for_prompt\",\n",
      "]\n",
      "\n",
      "--- File: yellhorn_mcp/processors/context_processor.py ---\n",
      "\"\"\"Context curation processing for Yellhorn MCP.\n",
      "\n",
      "This module handles the context curation process for optimizing AI context\n",
      "by analyzing the codebase and creating .yellhorncontext files.\n",
      "\"\"\"\n",
      "\n",
      "import asyncio\n",
      "import json\n",
      "import os\n",
      "import re\n",
      "from collections import defaultdict\n",
      "from pathlib import Path\n",
      "from typing import Any, List, Optional, Set\n",
      "\n",
      "from mcp.server.fastmcp import Context\n",
      "\n",
      "from yellhorn_mcp.formatters.codebase_snapshot import get_codebase_snapshot\n",
      "from yellhorn_mcp.formatters.context_fetcher import get_codebase_context\n",
      "from yellhorn_mcp.formatters.prompt_formatter import (\n",
      "    build_file_structure_context,\n",
      "    format_codebase_for_prompt,\n",
      ")\n",
      "from yellhorn_mcp.llm_manager import LLMManager\n",
      "from yellhorn_mcp.token_counter import TokenCounter\n",
      "from yellhorn_mcp.utils.git_utils import YellhornMCPError\n",
      "\n",
      "\n",
      "async def build_codebase_context(\n",
      "    repo_path: Path,\n",
      "    codebase_reasoning_mode: str,\n",
      "    model: str,\n",
      "    ctx: Context | None = None,\n",
      "    git_command_func=None,\n",
      ") -> tuple[str, list[str], set[str]]:\n",
      "    \"\"\"Build the codebase context for analysis.\n",
      "\n",
      "    Args:\n",
      "        repo_path: Path to the repository.\n",
      "        codebase_reasoning_mode: How to analyze the codebase.\n",
      "        model: Model name for token counting.\n",
      "        ctx: Optional context for logging.\n",
      "        git_command_func: Optional git command function.\n",
      "\n",
      "    Returns:\n",
      "        Tuple of (directory_context, file_paths, all_dirs)\n",
      "    \"\"\"\n",
      "\n",
      "    # Define log function for get_codebase_context\n",
      "    def sync_context_log(msg: str):\n",
      "        if ctx:\n",
      "            asyncio.create_task(ctx.log(level=\"info\", message=msg))\n",
      "\n",
      "    if ctx:\n",
      "        await ctx.log(\n",
      "            level=\"info\",\n",
      "            message=f\"Getting codebase context using {codebase_reasoning_mode} mode\",\n",
      "        )\n",
      "\n",
      "    # Get the codebase context\n",
      "    directory_context, context_file_paths = await get_codebase_context(\n",
      "        repo_path=repo_path,\n",
      "        reasoning_mode=codebase_reasoning_mode,\n",
      "        log_function=sync_context_log if ctx else None,\n",
      "        git_command_func=git_command_func,\n",
      "    )\n",
      "\n",
      "    # Log key metrics\n",
      "    if ctx:\n",
      "        token_counter = TokenCounter()\n",
      "        token_count = token_counter.count_tokens(directory_context, model)\n",
      "        file_count = len(directory_context.split(\"\\n\")) if directory_context else 0\n",
      "        await ctx.log(\n",
      "            level=\"info\",\n",
      "            message=f\"Codebase context metrics: {file_count} files, {token_count} tokens based on ({model})\",\n",
      "        )\n",
      "\n",
      "    # Extract directories from file paths\n",
      "    all_dirs = set()\n",
      "    for file_path in context_file_paths:\n",
      "        parts = file_path.split(\"/\")\n",
      "        for i in range(1, len(parts)):\n",
      "            dir_path = \"/\".join(parts[:i])\n",
      "            if dir_path:\n",
      "                all_dirs.add(dir_path)\n",
      "\n",
      "    # Add root directory if there are root-level files\n",
      "    if any(\"/\" not in f for f in context_file_paths):\n",
      "        all_dirs.add(\".\")\n",
      "\n",
      "    if ctx:\n",
      "        await ctx.log(\n",
      "            level=\"info\",\n",
      "            message=f\"Extracted {len(all_dirs)} directories from {len(context_file_paths)} filtered files\",\n",
      "        )\n",
      "\n",
      "    return directory_context, context_file_paths, all_dirs\n",
      "\n",
      "\n",
      "async def analyze_with_llm(\n",
      "    llm_manager: LLMManager,\n",
      "    model: str,\n",
      "    directory_context: str,\n",
      "    user_task: str,\n",
      "    debug: bool = False,\n",
      "    ctx: Context | None = None,\n",
      ") -> str:\n",
      "    \"\"\"Analyze the codebase with LLM to identify important directories.\n",
      "\n",
      "    Args:\n",
      "        llm_manager: LLM Manager instance.\n",
      "        model: Model name to use.\n",
      "        directory_context: The codebase context string.\n",
      "        user_task: Description of the task.\n",
      "        debug: Whether to log debug information.\n",
      "        ctx: Optional context for logging.\n",
      "\n",
      "    Returns:\n",
      "        LLM response containing directory analysis.\n",
      "    \"\"\"\n",
      "    # Construct the system message\n",
      "    system_message = f\"\"\"You are an expert software developer tasked with analyzing a codebase structure to identify important directories for building and executing a workplan.\n",
      "\n",
      "Your goal is to identify the most important directories that should be included for the user's task.\n",
      "\n",
      "Analyze the directories and identify the ones that:\n",
      "1. Contain core application code relevant to the user's task\n",
      "2. Likely contain important business logic\n",
      "3. Would be essential for understanding the codebase architecture\n",
      "4. Are needed to implement the requested task\n",
      "5. Contain SDKs or libraries relevant to the user's task\n",
      "\n",
      "Ignore directories that:\n",
      "1. Contain only build artifacts or generated code\n",
      "2. Store dependencies or vendor code\n",
      "3. Contain temporary or cache files\n",
      "4. Probably aren't relevant to the user's specific task\n",
      "\n",
      "User Task: {user_task}\n",
      "\n",
      "Return your analysis as a list of important directories, one per line, without any additional text or formatting as below:\n",
      "\n",
      "```context\n",
      "dir1/subdir1/\n",
      "dir2/\n",
      "dir3/subdir3/file3.filetype\n",
      "```\n",
      "\n",
      "Prefer to include directories, and not just file paths but include just file paths when appropriate.\n",
      "Don't include explanations for your choices, just return the list in the specified format.\"\"\"\n",
      "\n",
      "    prompt = f\"\"\"{directory_context}\"\"\"\n",
      "\n",
      "    if ctx:\n",
      "        await ctx.log(\n",
      "            level=\"info\",\n",
      "            message=f\"Analyzing directory structure with {model}\",\n",
      "        )\n",
      "\n",
      "    # Debug logging\n",
      "    if debug and ctx:\n",
      "        await ctx.log(level=\"info\", message=f\"[DEBUG] System message: {system_message}\")\n",
      "        await ctx.log(\n",
      "            level=\"info\", message=f\"[DEBUG] User prompt ({len(prompt)} chars): {prompt[:5000]}...\"\n",
      "        )\n",
      "\n",
      "    # Call LLM\n",
      "    result = await llm_manager.call_llm(\n",
      "        model=model,\n",
      "        prompt=prompt,\n",
      "        system_message=system_message,\n",
      "        temperature=0.0,\n",
      "    )\n",
      "\n",
      "    return result if isinstance(result, str) else str(result)\n",
      "\n",
      "\n",
      "async def parse_llm_directories(\n",
      "    llm_result: str,\n",
      "    all_dirs: set[str],\n",
      "    ctx: Context | None = None,\n",
      ") -> set[str]:\n",
      "    \"\"\"Parse LLM output to extract important directories.\n",
      "\n",
      "    Args:\n",
      "        llm_result: The LLM response string.\n",
      "        all_dirs: Set of all available directories.\n",
      "        ctx: Optional context for logging.\n",
      "\n",
      "    Returns:\n",
      "        Set of important directories identified by the LLM.\n",
      "    \"\"\"\n",
      "    all_important_dirs = set()\n",
      "\n",
      "    def match_path_to_directories(path: str, all_dirs: set[str]) -> set[str]:\n",
      "        \"\"\"Match a path to directories it belongs to, or return the file path itself if it's a specific file.\n",
      "\n",
      "        For example:\n",
      "        - 'yellhorn_mcp/token_counter.py' matches 'yellhorn_mcp'\n",
      "        - 'yellhorn_mcp/processors/context.py' matches 'yellhorn_mcp/processors'\n",
      "        - 'tests' matches 'tests' if it exists as a directory\n",
      "        - '.python-version' returns '.python-version' as a specific file\n",
      "        \"\"\"\n",
      "        matched = set()\n",
      "\n",
      "        # Direct match (exact directory)\n",
      "        if path in all_dirs or path == \".\":\n",
      "            matched.add(path)\n",
      "            return matched\n",
      "\n",
      "        # Check if it's a specific file (can have slashes, detected by file characteristics)\n",
      "        # Look for file extensions or dot files\n",
      "        path_parts = path.split(\"/\")\n",
      "        last_part = path_parts[-1]\n",
      "        # Check for common file extensions or dot files, but exclude prose text\n",
      "        if (\n",
      "            \".\" in last_part\n",
      "            and not last_part.endswith(\"/\")\n",
      "            and (last_part.count(\".\") == 1 or last_part.startswith(\".\"))\n",
      "            and not any(\n",
      "                word in path.lower()\n",
      "                for word in [\"found\", \"error\", \"directory\", \"directories\", \"important\"]\n",
      "            )\n",
      "        ):\n",
      "            # This looks like a file (has extension or is a dot file)\n",
      "            matched.add(path)\n",
      "            return matched\n",
      "\n",
      "        # Check if it's a file path that belongs to directories\n",
      "        if not matched and \"/\" in path:\n",
      "            # Split path and find the lowest (most specific) parent directory\n",
      "            parts = path.split(\"/\")\n",
      "            # Start from the most specific (longest) path and work backwards\n",
      "            for i in range(len(parts), 0, -1):\n",
      "                parent_dir = \"/\".join(parts[:i])\n",
      "                if parent_dir in all_dirs:\n",
      "                    matched.add(parent_dir)\n",
      "                    break  # Found the most specific match, stop here\n",
      "\n",
      "        # Check if the path is a parent of any directory in all_dirs\n",
      "        # Only do this if no matches were found in the previous checks\n",
      "        if not matched:\n",
      "            for dir_path in all_dirs:\n",
      "                if dir_path.startswith(path + \"/\") or dir_path == path:\n",
      "                    matched.add(path if path in all_dirs else dir_path)\n",
      "\n",
      "        return matched\n",
      "\n",
      "    # Find all context blocks\n",
      "    context_blocks = re.findall(r\"```context\\n([\\s\\S]*?)\\n```\", llm_result, re.MULTILINE)\n",
      "\n",
      "    # Process each block\n",
      "    for block in context_blocks:\n",
      "        for line in block.split(\"\\n\"):\n",
      "            line = line.strip()\n",
      "            # Remove trailing slashes for consistency\n",
      "            line = line.rstrip(\"/\")\n",
      "\n",
      "            if line and not line.startswith(\"#\"):\n",
      "                # Try to match the line to directories\n",
      "                matched_dirs = match_path_to_directories(line, all_dirs)\n",
      "                if matched_dirs:\n",
      "                    all_important_dirs.update(matched_dirs)\n",
      "\n",
      "    # If no directories found in context blocks, try direct extraction\n",
      "    if not all_important_dirs:\n",
      "        for line in llm_result.split(\"\\n\"):\n",
      "            line = line.strip().rstrip(\"/\")\n",
      "            if line and not line.startswith(\"```\") and not line.startswith(\"#\"):\n",
      "                # Try to match the line to directories\n",
      "                matched_dirs = match_path_to_directories(line, all_dirs)\n",
      "                if matched_dirs:\n",
      "                    all_important_dirs.update(matched_dirs)\n",
      "\n",
      "    # Fallback to all directories if none found\n",
      "    if not all_important_dirs:\n",
      "        if ctx:\n",
      "            await ctx.log(\n",
      "                level=\"warning\",\n",
      "                message=\"No important directories identified, including all directories\",\n",
      "            )\n",
      "        all_important_dirs = set(all_dirs)\n",
      "\n",
      "    return all_important_dirs\n",
      "\n",
      "\n",
      "async def save_context_file(\n",
      "    repo_path: Path,\n",
      "    output_path: str,\n",
      "    user_task: str,\n",
      "    all_important_dirs: set[str],\n",
      "    file_paths: list[str],\n",
      "    ctx: Context | None = None,\n",
      ") -> str:\n",
      "    \"\"\"Save the context file with important directories.\n",
      "\n",
      "    Args:\n",
      "        repo_path: Path to the repository.\n",
      "        output_path: Path where the context file will be created.\n",
      "        user_task: Description of the task.\n",
      "        all_important_dirs: Set of important directories.\n",
      "        file_paths: List of all file paths.\n",
      "        ctx: Optional context for logging.\n",
      "\n",
      "    Returns:\n",
      "        Success message with the created file path.\n",
      "\n",
      "    Raises:\n",
      "        YellhornMCPError: If writing fails.\n",
      "    \"\"\"\n",
      "    # Generate file content\n",
      "    final_content = \"# Yellhorn Context File - AI context optimization\\n\"\n",
      "    final_content += f\"# Generated by yellhorn-mcp curate_context tool\\n\"\n",
      "    final_content += f\"# Based on task: {user_task[:80]}\\n\\n\"\n",
      "\n",
      "    # Sort directories for consistent output\n",
      "    # Separate files from directories\n",
      "    important_dirs = set()\n",
      "    important_files = set()\n",
      "    \n",
      "    for item in all_important_dirs:\n",
      "        # Check if this looks like a file (has extension or is a dot file)\n",
      "        if \"/\" in item:\n",
      "            parts = item.split(\"/\")\n",
      "            last_part = parts[-1]\n",
      "            is_file = (\n",
      "                \".\" in last_part\n",
      "                and not last_part.endswith(\"/\")\n",
      "                and (last_part.count(\".\") == 1 or last_part.startswith(\".\"))\n",
      "            )\n",
      "        else:\n",
      "            is_file = (\n",
      "                \".\" in item\n",
      "                and (item.count(\".\") == 1 or item.startswith(\".\"))\n",
      "            )\n",
      "            \n",
      "        if is_file:\n",
      "            important_files.add(item)\n",
      "        else:\n",
      "            important_dirs.add(item)\n",
      "    \n",
      "    sorted_important_dirs = sorted(list(important_dirs))\n",
      "    sorted_important_files = sorted(list(important_files))\n",
      "\n",
      "    # Generate .yellhorncontext file content\n",
      "    if sorted_important_dirs or sorted_important_files:\n",
      "        final_content += \"# Important directories to specifically include\\n\"\n",
      "        dir_includes = []\n",
      "        \n",
      "        # Add specific files first\n",
      "        for file_path in sorted_important_files:\n",
      "            dir_includes.append(file_path)\n",
      "\n",
      "        # Add directories\n",
      "        for dir_path in sorted_important_dirs:\n",
      "            # Check if directory has files\n",
      "            has_files = False\n",
      "            if dir_path == \".\":\n",
      "                has_files = any(\"/\" not in f for f in file_paths)\n",
      "            else:\n",
      "                has_files = any(f.startswith(dir_path + \"/\") for f in file_paths)\n",
      "\n",
      "            if dir_path == \".\":\n",
      "                if has_files:\n",
      "                    dir_includes.append(\"./\")\n",
      "                else:\n",
      "                    dir_includes.append(\"./**\")\n",
      "            else:\n",
      "                if has_files:\n",
      "                    dir_includes.append(f\"{dir_path}/\")\n",
      "                else:\n",
      "                    dir_includes.append(f\"{dir_path}/**\")\n",
      "\n",
      "        final_content += \"\\n\".join(dir_includes) + \"\\n\\n\"\n",
      "\n",
      "    # Remove duplicate lines\n",
      "    content_lines = final_content.splitlines()\n",
      "    content_lines.reverse()\n",
      "\n",
      "    seen_lines = set()\n",
      "    unique_lines = []\n",
      "\n",
      "    for line in content_lines:\n",
      "        if line.strip() == \"\" or line.strip().startswith(\"#\"):\n",
      "            unique_lines.append(line)\n",
      "            continue\n",
      "\n",
      "        if line not in seen_lines:\n",
      "            seen_lines.add(line)\n",
      "            unique_lines.append(line)\n",
      "\n",
      "    unique_lines.reverse()\n",
      "    final_content = \"\\n\".join(unique_lines)\n",
      "\n",
      "    # Write the file\n",
      "    output_file_path = repo_path / output_path\n",
      "    try:\n",
      "        with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
      "            f.write(final_content)\n",
      "\n",
      "        if ctx:\n",
      "            await ctx.log(\n",
      "                level=\"info\",\n",
      "                message=f\"Successfully wrote .yellhorncontext file to {output_file_path}\",\n",
      "            )\n",
      "\n",
      "        return f\"Successfully created .yellhorncontext file at {output_file_path} with {len(sorted_important_files)} files and {len(sorted_important_dirs)} directories.\"\n",
      "\n",
      "    except Exception as e:\n",
      "        raise YellhornMCPError(f\"Failed to write .yellhorncontext file: {str(e)}\")\n",
      "\n",
      "\n",
      "async def process_context_curation_async(\n",
      "    repo_path: Path,\n",
      "    llm_manager: LLMManager,\n",
      "    model: str,\n",
      "    user_task: str,\n",
      "    output_path: str = \".yellhorncontext\",\n",
      "    codebase_reasoning: str = \"file_structure\",\n",
      "    disable_search_grounding: bool = False,\n",
      "    debug: bool = False,\n",
      "    ctx: Context | None = None,\n",
      ") -> str:\n",
      "    \"\"\"Analyze codebase and create a context curation file.\n",
      "\n",
      "    Args:\n",
      "        repo_path: Path to the repository.\n",
      "        llm_manager: LLM Manager instance.\n",
      "        model: Model name to use.\n",
      "        user_task: Description of the task to accomplish.\n",
      "        output_path: Path where the .yellhorncontext file will be created.\n",
      "        codebase_reasoning: How to analyze the codebase.\n",
      "        ignore_file_path: Path to the ignore file.\n",
      "        disable_search_grounding: Whether to disable search grounding.\n",
      "        debug: Whether to log the full prompt sent to the LLM.\n",
      "        ctx: Optional context for logging.\n",
      "\n",
      "    Returns:\n",
      "        Success message with the created file path.\n",
      "\n",
      "    Raises:\n",
      "        YellhornMCPError: If context curation fails.\n",
      "    \"\"\"\n",
      "    # Check if LLM manager is provided\n",
      "    if not llm_manager:\n",
      "        raise YellhornMCPError(\"LLM Manager not initialized\")\n",
      "\n",
      "    try:\n",
      "        # Store original search grounding setting\n",
      "        original_search_grounding = None\n",
      "        if disable_search_grounding and ctx:\n",
      "            original_search_grounding = ctx.request_context.lifespan_context.get(\n",
      "                \"use_search_grounding\", True\n",
      "            )\n",
      "            ctx.request_context.lifespan_context[\"use_search_grounding\"] = False\n",
      "\n",
      "        if ctx:\n",
      "            await ctx.log(level=\"info\", message=\"Starting context curation process\")\n",
      "\n",
      "        # Get git command function from context if available\n",
      "        git_command_func = (\n",
      "            ctx.request_context.lifespan_context.get(\"git_command_func\") if ctx else None\n",
      "        )\n",
      "\n",
      "        # Determine the codebase reasoning mode to use\n",
      "        codebase_reasoning_mode = (\n",
      "            ctx.request_context.lifespan_context.get(\"codebase_reasoning\", codebase_reasoning)\n",
      "            if ctx\n",
      "            else codebase_reasoning\n",
      "        )\n",
      "\n",
      "        # Delete existing .yellhorncontext file to prevent it from influencing file filtering\n",
      "        context_file_path = repo_path / output_path\n",
      "        if context_file_path.exists():\n",
      "            try:\n",
      "                context_file_path.unlink()\n",
      "                if ctx:\n",
      "                    await ctx.log(\n",
      "                        level=\"info\",\n",
      "                        message=f\"Deleted existing {output_path} file before analysis\",\n",
      "                    )\n",
      "            except Exception as e:\n",
      "                if ctx:\n",
      "                    await ctx.log(\n",
      "                        level=\"warning\",\n",
      "                        message=f\"Could not delete existing {output_path} file: {e}\",\n",
      "                    )\n",
      "\n",
      "        # Step 1: Build the codebase context\n",
      "        directory_context, file_paths, all_dirs = await build_codebase_context(\n",
      "            repo_path=repo_path,\n",
      "            codebase_reasoning_mode=codebase_reasoning_mode,\n",
      "            model=model,\n",
      "            ctx=ctx,\n",
      "            git_command_func=git_command_func,\n",
      "        )\n",
      "\n",
      "        # Log peek of directory context\n",
      "        if ctx:\n",
      "            await ctx.log(\n",
      "                level=\"info\",\n",
      "                message=(\n",
      "                    f\"Directory context:\\n{directory_context[:500]}...\"\n",
      "                    if len(directory_context) > 500\n",
      "                    else f\"Directory context:\\n{directory_context}\"\n",
      "                ),\n",
      "            )\n",
      "\n",
      "        # Step 2: Analyze with LLM\n",
      "        all_important_dirs = set()\n",
      "        try:\n",
      "            llm_result = await analyze_with_llm(\n",
      "                llm_manager=llm_manager,\n",
      "                model=model,\n",
      "                directory_context=directory_context,\n",
      "                user_task=user_task,\n",
      "                debug=debug,\n",
      "                ctx=ctx,\n",
      "            )\n",
      "\n",
      "            # Step 3: Parse LLM output for directories\n",
      "            all_important_dirs = await parse_llm_directories(\n",
      "                llm_result=llm_result,\n",
      "                all_dirs=all_dirs,\n",
      "                ctx=ctx,\n",
      "            )\n",
      "\n",
      "            # Log the directories found\n",
      "            if ctx:\n",
      "                dirs_str = \", \".join(sorted(list(all_important_dirs))[:5])\n",
      "                if len(all_important_dirs) > 5:\n",
      "                    dirs_str += f\", ... ({len(all_important_dirs) - 5} more)\"\n",
      "\n",
      "                await ctx.log(\n",
      "                    level=\"info\",\n",
      "                    message=f\"Analysis complete, found {len(all_important_dirs)} important directories: {dirs_str}\",\n",
      "                )\n",
      "\n",
      "        except Exception as e:\n",
      "            if ctx:\n",
      "                await ctx.log(\n",
      "                    level=\"error\",\n",
      "                    message=f\"Error during LLM analysis: {str(e)} ({type(e).__name__})\",\n",
      "                )\n",
      "            # Fallback to all directories\n",
      "            all_important_dirs = set(all_dirs)\n",
      "\n",
      "        # If no directories identified, use all (already handled in parse_llm_directories)\n",
      "        if not all_important_dirs:\n",
      "            all_important_dirs = set(all_dirs)\n",
      "\n",
      "        if ctx:\n",
      "            await ctx.log(\n",
      "                level=\"info\",\n",
      "                message=f\"Processing complete, identified {len(all_important_dirs)} important directories\",\n",
      "            )\n",
      "\n",
      "        # Step 4: Save the context file\n",
      "        result = await save_context_file(\n",
      "            repo_path=repo_path,\n",
      "            output_path=output_path,\n",
      "            user_task=user_task,\n",
      "            all_important_dirs=all_important_dirs,\n",
      "            file_paths=file_paths,\n",
      "            ctx=ctx,\n",
      "        )\n",
      "\n",
      "        # Restore original search grounding setting if modified\n",
      "        if disable_search_grounding and ctx:\n",
      "            ctx.request_context.lifespan_context[\"use_search_grounding\"] = original_search_grounding\n",
      "\n",
      "        return result\n",
      "\n",
      "    except Exception as e:\n",
      "        error_message = f\"Failed to generate .yellhorncontext file: {str(e)}\"\n",
      "        if ctx:\n",
      "            await ctx.log(level=\"error\", message=error_message)\n",
      "        raise YellhornMCPError(error_message)\n",
      "\n",
      "--- File: yellhorn_mcp/processors/judgement_processor.py ---\n",
      "\"\"\"Judgement processing for Yellhorn MCP.\n",
      "\n",
      "This module handles the asynchronous judgement generation process,\n",
      "comparing code changes against workplans.\n",
      "\"\"\"\n",
      "\n",
      "import asyncio\n",
      "import re\n",
      "from datetime import datetime, timezone\n",
      "from pathlib import Path\n",
      "from typing import Any, Callable\n",
      "\n",
      "from google import genai\n",
      "from mcp.server.fastmcp import Context\n",
      "from openai import AsyncOpenAI\n",
      "\n",
      "from yellhorn_mcp import __version__\n",
      "from yellhorn_mcp.integrations.github_integration import (\n",
      "    add_issue_comment,\n",
      "    create_judgement_subissue,\n",
      "    update_github_issue,\n",
      ")\n",
      "from yellhorn_mcp.llm_manager import LLMManager, UsageMetadata\n",
      "from yellhorn_mcp.models.metadata_models import CompletionMetadata, SubmissionMetadata\n",
      "from yellhorn_mcp.token_counter import TokenCounter\n",
      "from yellhorn_mcp.formatters.context_fetcher import get_codebase_context\n",
      "from yellhorn_mcp.utils.comment_utils import (\n",
      "    extract_urls,\n",
      "    format_completion_comment,\n",
      "    format_submission_comment,\n",
      ")\n",
      "from yellhorn_mcp.utils.cost_tracker_utils import calculate_cost, format_metrics_section\n",
      "from yellhorn_mcp.utils.git_utils import YellhornMCPError, run_git_command\n",
      "\n",
      "\n",
      "async def get_git_diff(\n",
      "    repo_path: Path, base_ref: str, head_ref: str, codebase_reasoning: str = \"full\", git_command_func=None\n",
      ") -> str:\n",
      "    \"\"\"Get the diff content between two git references.\n",
      "\n",
      "    Args:\n",
      "        repo_path: Path to the repository.\n",
      "        base_ref: Base reference (branch/commit).\n",
      "        head_ref: Head reference (branch/commit).\n",
      "        codebase_reasoning: Mode for diff generation.\n",
      "        git_command_func: Optional Git command function (for mocking).\n",
      "\n",
      "    Returns:\n",
      "        The diff content as a string.\n",
      "\n",
      "    Raises:\n",
      "        YellhornMCPError: If the diff generation fails.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        if codebase_reasoning in [\"file_structure\", \"none\"]:\n",
      "            # For file_structure or none, just list changed files\n",
      "            changed_files = await run_git_command(\n",
      "                repo_path, [\"diff\", \"--name-only\", f\"{base_ref}...{head_ref}\"], git_command_func\n",
      "            )\n",
      "            if changed_files:\n",
      "                return f\"Changed files between {base_ref} and {head_ref}:\\n{changed_files}\"\n",
      "            else:\n",
      "                return f\"Changed files between {base_ref} and {head_ref}:\"\n",
      "\n",
      "        elif codebase_reasoning == \"lsp\":\n",
      "            # Import LSP utilities\n",
      "            from yellhorn_mcp.utils.lsp_utils import get_lsp_diff\n",
      "\n",
      "            # For lsp mode, get changed files and create LSP diff\n",
      "            changed_files_output = await run_git_command(\n",
      "                repo_path, [\"diff\", \"--name-only\", f\"{base_ref}...{head_ref}\"], git_command_func\n",
      "            )\n",
      "            changed_files = changed_files_output.strip().split(\"\\n\") if changed_files_output else []\n",
      "\n",
      "            if changed_files:\n",
      "                # Get LSP diff which shows signatures of changed functions and full content of changed files\n",
      "                lsp_diff = await get_lsp_diff(repo_path, base_ref, head_ref, changed_files, git_command_func)\n",
      "                return lsp_diff\n",
      "            else:\n",
      "                return \"\"\n",
      "\n",
      "        else:\n",
      "            # Default: full diff content\n",
      "            diff = await run_git_command(repo_path, [\"diff\", \"--patch\", f\"{base_ref}...{head_ref}\"], git_command_func)\n",
      "            return diff if diff else \"\"\n",
      "\n",
      "    except Exception as e:\n",
      "        raise YellhornMCPError(f\"Failed to generate git diff: {str(e)}\")\n",
      "\n",
      "\n",
      "async def process_judgement_async(\n",
      "    repo_path: Path,\n",
      "    llm_manager: LLMManager,\n",
      "    model: str,\n",
      "    workplan_content: str,\n",
      "    diff_content: str,\n",
      "    base_ref: str,\n",
      "    head_ref: str,\n",
      "    base_commit_hash: str,\n",
      "    head_commit_hash: str,\n",
      "    parent_workplan_issue_number: str,\n",
      "    subissue_to_update: str | None = None,\n",
      "    debug: bool = False,\n",
      "    codebase_reasoning: str = \"full\",\n",
      "    disable_search_grounding: bool = False,\n",
      "    _meta: dict[str, Any] | None = None,\n",
      "    ctx: Context | None = None,\n",
      "    github_command_func: Callable | None = None,\n",
      "    git_command_func: Callable | None = None,\n",
      ") -> None:\n",
      "    \"\"\"Judge a code diff against a workplan asynchronously.\n",
      "\n",
      "    Args:\n",
      "        repo_path: Path to the repository.\n",
      "        llm_manager: LLM Manager instance for API calls.\n",
      "        model: Model name to use (Gemini or OpenAI).\n",
      "        workplan_content: The original workplan content.\n",
      "        diff_content: The code diff to judge.\n",
      "        base_ref: Base reference name.\n",
      "        head_ref: Head reference name.\n",
      "        base_commit_hash: Base commit hash.\n",
      "        head_commit_hash: Head commit hash.\n",
      "        parent_workplan_issue_number: Parent workplan issue number.\n",
      "        subissue_to_update: Optional existing sub-issue to update.\n",
      "        debug: If True, add a comment with the full prompt.\n",
      "        codebase_reasoning: Mode for codebase context.\n",
      "        disable_search_grounding: If True, disables search grounding.\n",
      "        _meta: Optional metadata from the caller.\n",
      "        ctx: Optional context for logging.\n",
      "        github_command_func: Optional GitHub command function (for mocking).\n",
      "        git_command_func: Optional Git command function (for mocking).\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # Get codebase info based on reasoning mode\n",
      "        codebase_info = \"\"\n",
      "\n",
      "        # Create a simple logging function\n",
      "        def context_log(msg: str):\n",
      "            if ctx:\n",
      "                asyncio.create_task(ctx.log(level=\"info\", message=msg))\n",
      "\n",
      "        # Use get_codebase_context for all reasoning modes\n",
      "        if codebase_reasoning in [\"lsp\", \"file_structure\", \"full\"]:\n",
      "            # Calculate token limit for codebase context\n",
      "            token_counter = TokenCounter()\n",
      "            model_limit = token_counter.get_model_limit(model)\n",
      "            # Reserve tokens for prompt template, workplan, diff, and response\n",
      "            # Estimate: prompt template ~1000, workplan ~2000, diff ~2000, safety margin ~4000\n",
      "            codebase_token_limit = int((model_limit - 9000) * 0.7)\n",
      "            \n",
      "            codebase_info, _ = await get_codebase_context(\n",
      "                repo_path, \n",
      "                codebase_reasoning, \n",
      "                context_log,\n",
      "                token_limit=codebase_token_limit,\n",
      "                model=model,\n",
      "                git_command_func=git_command_func\n",
      "            )\n",
      "\n",
      "        # Construct prompt\n",
      "        prompt = f\"\"\"You are an expert software reviewer tasked with judging whether a code diff successfully implements a given workplan.\n",
      "\n",
      "# Original Workplan\n",
      "{workplan_content}\n",
      "\n",
      "# Code Diff\n",
      "{diff_content}\n",
      "\n",
      "# Codebase Context\n",
      "{codebase_info}\n",
      "\n",
      "# Task\n",
      "Review the code diff against the original workplan and provide a detailed judgement. Consider:\n",
      "\n",
      "1. **Completeness**: Does the diff implement all the steps and requirements outlined in the workplan?\n",
      "2. **Correctness**: Is the implementation technically correct and does it follow best practices?\n",
      "3. **Missing Elements**: What parts of the workplan, if any, were not addressed?\n",
      "4. **Additional Changes**: Were there any changes made that weren't part of the original workplan?\n",
      "5. **Quality**: Comment on code quality, testing, documentation, and any potential issues.\n",
      "\n",
      "The diff represents changes between '{base_ref}' and '{head_ref}'.\n",
      "\n",
      "Structure your response with these clear sections:\n",
      "\n",
      "## Judgement Summary\n",
      "Provide a clear verdict: APPROVED, NEEDS_WORK, or INCOMPLETE, followed by a brief explanation.\n",
      "\n",
      "## Implementation Analysis\n",
      "Detail what was successfully implemented from the workplan.\n",
      "\n",
      "## Missing or Incomplete Items\n",
      "List specific items from the workplan that were not addressed or were only partially implemented.\n",
      "\n",
      "## Code Quality Assessment\n",
      "Evaluate the quality of the implementation including:\n",
      "- Code style and consistency\n",
      "- Error handling\n",
      "- Test coverage\n",
      "- Documentation\n",
      "\n",
      "## Recommendations\n",
      "Provide specific, actionable recommendations for improvement.\n",
      "\n",
      "## References\n",
      "Extract any URLs mentioned in the workplan or that would be helpful for understanding the implementation and list them here. This ensures important links are preserved.\n",
      "\n",
      "IMPORTANT: Respond *only* with the Markdown content for the judgement. Do *not* wrap your entire response in a single Markdown code block (```). Start directly with the '## Judgement Summary' heading.\n",
      "\"\"\"\n",
      "        # Check if we should use search grounding\n",
      "        use_search_grounding = not disable_search_grounding\n",
      "        if _meta and \"original_search_grounding\" in _meta:\n",
      "            use_search_grounding = (\n",
      "                _meta[\"original_search_grounding\"] and not disable_search_grounding\n",
      "            )\n",
      "\n",
      "        # Prepare additional kwargs for the LLM call\n",
      "        llm_kwargs = {}\n",
      "        is_openai_model = llm_manager._is_openai_model(model)\n",
      "\n",
      "        # Handle search grounding for Gemini models\n",
      "        if not is_openai_model and use_search_grounding:\n",
      "            if ctx:\n",
      "                await ctx.log(\n",
      "                    level=\"info\", message=f\"Attempting to enable search grounding for model {model}\"\n",
      "                )\n",
      "            try:\n",
      "                from google.genai.types import GenerateContentConfig\n",
      "\n",
      "                from yellhorn_mcp.utils.search_grounding_utils import _get_gemini_search_tools\n",
      "\n",
      "                search_tools = _get_gemini_search_tools(model)\n",
      "                if search_tools:\n",
      "                    llm_kwargs[\"generation_config\"] = GenerateContentConfig(tools=search_tools)\n",
      "                    if ctx:\n",
      "                        await ctx.log(\n",
      "                            level=\"info\", message=f\"Search grounding enabled for model {model}\"\n",
      "                        )\n",
      "            except ImportError:\n",
      "                if ctx:\n",
      "                    await ctx.log(\n",
      "                        level=\"warning\",\n",
      "                        message=\"GenerateContentConfig not available, skipping search grounding\",\n",
      "                    )\n",
      "\n",
      "        # Call LLM through the manager with citation support\n",
      "        if is_openai_model:\n",
      "            # OpenAI models don't support citations\n",
      "            response_data = await llm_manager.call_llm_with_usage(\n",
      "                prompt=prompt, model=model, temperature=0.0, **llm_kwargs\n",
      "            )\n",
      "            judgement_content = response_data[\"content\"]\n",
      "            usage_metadata = response_data[\"usage_metadata\"]\n",
      "            completion_metadata = CompletionMetadata(\n",
      "                model_name=model,\n",
      "                status=\"✅ Judgement generated successfully\",\n",
      "                generation_time_seconds=0.0,  # Will be calculated below\n",
      "                input_tokens=usage_metadata.prompt_tokens,\n",
      "                output_tokens=usage_metadata.completion_tokens,\n",
      "                total_tokens=usage_metadata.total_tokens,\n",
      "                timestamp=None,  # Will be set below\n",
      "            )\n",
      "        else:\n",
      "            # Gemini models - use citation-aware call\n",
      "            response_data = await llm_manager.call_llm_with_citations(\n",
      "                prompt=prompt, model=model, temperature=0.0, **llm_kwargs\n",
      "            )\n",
      "\n",
      "            judgement_content = response_data[\"content\"]\n",
      "            usage_metadata = response_data[\"usage_metadata\"]\n",
      "\n",
      "            # Process citations if available\n",
      "            if \"grounding_metadata\" in response_data and response_data[\"grounding_metadata\"]:\n",
      "                from yellhorn_mcp.utils.search_grounding_utils import add_citations_from_metadata\n",
      "\n",
      "                judgement_content = add_citations_from_metadata(\n",
      "                    judgement_content, response_data[\"grounding_metadata\"]\n",
      "                )\n",
      "\n",
      "            # Create completion metadata\n",
      "            completion_metadata = CompletionMetadata(\n",
      "                model_name=model,\n",
      "                status=\"✅ Judgement generated successfully\",\n",
      "                generation_time_seconds=0.0,  # Will be calculated below\n",
      "                input_tokens=usage_metadata.prompt_tokens,\n",
      "                output_tokens=usage_metadata.completion_tokens,\n",
      "                total_tokens=usage_metadata.total_tokens,\n",
      "                search_results_used=getattr(\n",
      "                    response_data.get(\"grounding_metadata\"), \"grounding_chunks\", None\n",
      "                )\n",
      "                is not None,\n",
      "                timestamp=None,  # Will be set below\n",
      "            )\n",
      "\n",
      "        if not judgement_content:\n",
      "            api_name = \"OpenAI\" if is_openai_model else \"Gemini\"\n",
      "            raise YellhornMCPError(\n",
      "                f\"Failed to generate judgement: Received an empty response from {api_name} API.\"\n",
      "            )\n",
      "\n",
      "        # Calculate generation time if we have metadata\n",
      "        if completion_metadata and _meta and \"start_time\" in _meta:\n",
      "            generation_time = (datetime.now(timezone.utc) - _meta[\"start_time\"]).total_seconds()\n",
      "            completion_metadata.generation_time_seconds = generation_time\n",
      "            completion_metadata.timestamp = datetime.now(timezone.utc)\n",
      "\n",
      "        # Calculate cost if we have token counts\n",
      "        if (\n",
      "            completion_metadata\n",
      "            and completion_metadata.input_tokens\n",
      "            and completion_metadata.output_tokens\n",
      "        ):\n",
      "            completion_metadata.estimated_cost = calculate_cost(\n",
      "                model, completion_metadata.input_tokens, completion_metadata.output_tokens\n",
      "            )\n",
      "\n",
      "        # Add context size\n",
      "        if completion_metadata:\n",
      "            completion_metadata.context_size_chars = len(prompt)\n",
      "\n",
      "        # Construct metadata section for the final body\n",
      "        metadata_section = f\"\"\"## Comparison Metadata\n",
      "- **Workplan Issue**: `#{parent_workplan_issue_number}`\n",
      "- **Base Ref**: `{base_ref}` (Commit: `{base_commit_hash}`)\n",
      "- **Head Ref**: `{head_ref}` (Commit: `{head_commit_hash}`)\n",
      "- **Codebase Reasoning Mode**: `{codebase_reasoning}`\n",
      "- **AI Model**: `{model}`\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "        # Add parent issue link at the top\n",
      "        parent_link = f\"Parent workplan: #{parent_workplan_issue_number}\\n\\n\"\n",
      "\n",
      "        # Construct the full body (no metrics in body)\n",
      "        full_body = f\"{parent_link}{metadata_section}{judgement_content}\"\n",
      "\n",
      "        # Construct title\n",
      "        judgement_title = f\"Judgement for #{parent_workplan_issue_number}: {head_ref} vs {base_ref}\"\n",
      "\n",
      "        # Create or update the sub-issue\n",
      "        if subissue_to_update:\n",
      "            # Update existing issue\n",
      "            await update_github_issue(\n",
      "                repo_path=repo_path,\n",
      "                issue_number=subissue_to_update,\n",
      "                title=judgement_title,\n",
      "                body=full_body,\n",
      "                github_command_func=github_command_func,\n",
      "            )\n",
      "\n",
      "            # Construct the URL for the updated issue\n",
      "            repo_info = await run_git_command(repo_path, [\"remote\", \"get-url\", \"origin\"], git_command_func)\n",
      "            # Clean up the repo URL to get the proper format\n",
      "            if repo_info.endswith(\".git\"):\n",
      "                repo_info = repo_info[:-4]\n",
      "            if repo_info.startswith(\"git@github.com:\"):\n",
      "                repo_info = repo_info.replace(\"git@github.com:\", \"https://github.com/\")\n",
      "\n",
      "            subissue_url = f\"{repo_info}/issues/{subissue_to_update}\"\n",
      "        else:\n",
      "            subissue_url = await create_judgement_subissue(\n",
      "                repo_path,\n",
      "                parent_workplan_issue_number,\n",
      "                judgement_title,\n",
      "                full_body,\n",
      "                github_command_func=github_command_func,\n",
      "            )\n",
      "\n",
      "        if ctx:\n",
      "            await ctx.log(\n",
      "                level=\"info\",\n",
      "                message=f\"Successfully created judgement sub-issue: {subissue_url}\",\n",
      "            )\n",
      "\n",
      "        # Add debug comment if requested\n",
      "        if debug:\n",
      "            # Extract issue number from URL\n",
      "            issue_match = re.search(r\"/issues/(\\d+)\", subissue_url)\n",
      "            if issue_match:\n",
      "                sub_issue_number = issue_match.group(1)\n",
      "                debug_comment = f\"<details>\\n<summary>Debug: Full prompt used for generation</summary>\\n\\n```\\n{prompt}\\n```\\n</details>\"\n",
      "                await add_issue_comment(\n",
      "                    repo_path,\n",
      "                    sub_issue_number,\n",
      "                    debug_comment,\n",
      "                    github_command_func=github_command_func,\n",
      "                )\n",
      "\n",
      "        # Add completion comment to the PARENT issue, not the sub-issue\n",
      "        if completion_metadata and _meta:\n",
      "            submission_metadata = SubmissionMetadata(\n",
      "                status=\"Generating judgement...\",\n",
      "                model_name=model,\n",
      "                search_grounding_enabled=not disable_search_grounding,\n",
      "                yellhorn_version=__version__,\n",
      "                submitted_urls=_meta.get(\"submitted_urls\"),\n",
      "                codebase_reasoning_mode=codebase_reasoning,\n",
      "                timestamp=_meta.get(\"start_time\", datetime.now(timezone.utc)),\n",
      "            )\n",
      "\n",
      "            # Post completion comment to the sub-issue\n",
      "            completion_comment = format_completion_comment(completion_metadata)\n",
      "            # Extract sub-issue number from URL or use the provided one\n",
      "            if subissue_to_update:\n",
      "                sub_issue_number = subissue_to_update\n",
      "            else:\n",
      "                # Extract issue number from URL\n",
      "                issue_match = re.search(r\"/issues/(\\d+)\", subissue_url)\n",
      "                if issue_match:\n",
      "                    sub_issue_number = issue_match.group(1)\n",
      "                else:\n",
      "                    # Fallback to parent if we can't extract sub-issue number\n",
      "                    sub_issue_number = parent_workplan_issue_number\n",
      "\n",
      "            await add_issue_comment(\n",
      "                repo_path,\n",
      "                sub_issue_number,\n",
      "                completion_comment,\n",
      "                github_command_func=github_command_func,\n",
      "            )\n",
      "\n",
      "    except Exception as e:\n",
      "        error_msg = f\"Error processing judgement: {str(e)}\"\n",
      "        if ctx:\n",
      "            await ctx.log(level=\"error\", message=error_msg)\n",
      "\n",
      "        # Try to add error comment to parent issue\n",
      "        try:\n",
      "            error_comment = f\"❌ **Error generating judgement**\\n\\n{str(e)}\"\n",
      "            await add_issue_comment(\n",
      "                repo_path,\n",
      "                parent_workplan_issue_number,\n",
      "                error_comment,\n",
      "                github_command_func=github_command_func,\n",
      "            )\n",
      "        except Exception:\n",
      "            # If we can't even add a comment, just log\n",
      "            if ctx:\n",
      "                await ctx.log(\n",
      "                    level=\"error\", message=f\"Failed to add error comment to issue: {str(e)}\"\n",
      "                )\n",
      "\n",
      "        # Re-raise as YellhornMCPError to signal failure outward\n",
      "        raise YellhornMCPError(error_msg)\n",
      "\n",
      "--- File: yellhorn_mcp/processors/workplan_processor.py ---\n",
      "\"\"\"Workplan processing for Yellhorn MCP.\n",
      "\n",
      "This module handles the asynchronous workplan generation process,\n",
      "including codebase snapshot retrieval and AI model interaction.\n",
      "\"\"\"\n",
      "\n",
      "import asyncio\n",
      "import os\n",
      "from datetime import datetime, timezone\n",
      "from pathlib import Path\n",
      "from typing import Any, Callable\n",
      "\n",
      "from google import genai\n",
      "from mcp.server.fastmcp import Context\n",
      "from openai import AsyncOpenAI\n",
      "\n",
      "from yellhorn_mcp import __version__\n",
      "from yellhorn_mcp.integrations.github_integration import (\n",
      "    add_issue_comment,\n",
      "    update_issue_with_workplan,\n",
      ")\n",
      "from yellhorn_mcp.llm_manager import LLMManager, UsageMetadata\n",
      "from yellhorn_mcp.models.metadata_models import CompletionMetadata, SubmissionMetadata\n",
      "from yellhorn_mcp.token_counter import TokenCounter\n",
      "from yellhorn_mcp.utils.comment_utils import (\n",
      "    extract_urls,\n",
      "    format_completion_comment,\n",
      "    format_submission_comment,\n",
      ")\n",
      "from yellhorn_mcp.utils.cost_tracker_utils import calculate_cost, format_metrics_section\n",
      "from yellhorn_mcp.formatters import (\n",
      "    get_codebase_snapshot,\n",
      "    build_file_structure_context,\n",
      "    format_codebase_for_prompt,\n",
      "    get_codebase_context,\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "async def _generate_and_update_issue(\n",
      "    repo_path: Path,\n",
      "    llm_manager: LLMManager | None,\n",
      "    model: str,\n",
      "    prompt: str,\n",
      "    issue_number: str,\n",
      "    title: str,\n",
      "    content_prefix: str,\n",
      "    disable_search_grounding: bool,\n",
      "    debug: bool,\n",
      "    codebase_reasoning: str,\n",
      "    _meta: dict[str, Any] | None,\n",
      "    ctx: Context | None,\n",
      "    github_command_func: Callable | None = None,\n",
      "    git_command_func: Callable | None = None,\n",
      ") -> None:\n",
      "    \"\"\"Generate content with AI and update the GitHub issue.\n",
      "\n",
      "    Args:\n",
      "        repo_path: Path to the repository.\n",
      "        llm_manager: LLM Manager instance.\n",
      "        model: Model name to use.\n",
      "        prompt: Prompt to send to AI.\n",
      "        issue_number: GitHub issue number to update.\n",
      "        title: Title for the issue.\n",
      "        content_prefix: Prefix to add before the generated content.\n",
      "        disable_search_grounding: If True, disables search grounding.\n",
      "        debug: If True, add debug comment with full prompt.\n",
      "        codebase_reasoning: Codebase reasoning mode used.\n",
      "        _meta: Optional metadata from caller.\n",
      "        ctx: Optional context for logging.\n",
      "        github_command_func: Optional GitHub command function (for mocking).\n",
      "        git_command_func: Optional Git command function (for mocking).\n",
      "    \"\"\"\n",
      "    # Use LLM Manager for unified LLM calls\n",
      "    if not llm_manager:\n",
      "        if ctx:\n",
      "            await ctx.log(level=\"error\", message=\"LLM Manager not initialized\")\n",
      "        await add_issue_comment(\n",
      "            repo_path,\n",
      "            issue_number,\n",
      "            \"❌ **Error generating workplan** – LLM Manager not initialized\",\n",
      "            github_command_func=github_command_func,\n",
      "        )\n",
      "        return\n",
      "\n",
      "    # Add debug comment if requested\n",
      "    if debug:\n",
      "        debug_comment = f\"<details>\\n<summary>Debug: Full prompt used for generation</summary>\\n\\n```\\n{prompt}\\n```\\n</details>\"\n",
      "        await add_issue_comment(\n",
      "            repo_path, issue_number, debug_comment, github_command_func=github_command_func\n",
      "        )\n",
      "\n",
      "    # Check if we should use search grounding\n",
      "    use_search_grounding = not disable_search_grounding\n",
      "    if _meta and \"original_search_grounding\" in _meta:\n",
      "        use_search_grounding = _meta[\"original_search_grounding\"] and not disable_search_grounding\n",
      "\n",
      "    # Prepare additional kwargs for the LLM call\n",
      "    llm_kwargs = {}\n",
      "    is_openai_model = llm_manager._is_openai_model(model)\n",
      "\n",
      "    # Handle search grounding for Gemini models\n",
      "    search_tools = None\n",
      "    if not is_openai_model and use_search_grounding:\n",
      "        if ctx:\n",
      "            await ctx.log(\n",
      "                level=\"info\", message=f\"Attempting to enable search grounding for model {model}\"\n",
      "            )\n",
      "        try:\n",
      "            from yellhorn_mcp.utils.search_grounding_utils import _get_gemini_search_tools\n",
      "\n",
      "            search_tools = _get_gemini_search_tools(model)\n",
      "            if search_tools:\n",
      "                if ctx:\n",
      "                    await ctx.log(\n",
      "                        level=\"info\", message=f\"Search grounding enabled for model {model}\"\n",
      "                    )\n",
      "        except ImportError:\n",
      "            if ctx:\n",
      "                await ctx.log(\n",
      "                    level=\"warning\",\n",
      "                    message=\"Search grounding tools not available, skipping search grounding\",\n",
      "                )\n",
      "\n",
      "    try:\n",
      "        # Call LLM through the manager with citation support\n",
      "        if is_openai_model:\n",
      "            # OpenAI models don't support citations\n",
      "            response_data = await llm_manager.call_llm_with_usage(\n",
      "                prompt=prompt, model=model, temperature=0.0, **llm_kwargs\n",
      "            )\n",
      "            workplan_content = response_data[\"content\"]\n",
      "            usage_metadata = response_data[\"usage_metadata\"]\n",
      "            completion_metadata = CompletionMetadata(\n",
      "                model_name=model,\n",
      "                status=\"✅ Workplan generated successfully\",\n",
      "                generation_time_seconds=0.0,  # Will be calculated below\n",
      "                input_tokens=usage_metadata.prompt_tokens,\n",
      "                output_tokens=usage_metadata.completion_tokens,\n",
      "                total_tokens=usage_metadata.total_tokens,\n",
      "                timestamp=None,  # Will be set below\n",
      "            )\n",
      "        else:\n",
      "            # Gemini models - use citation-aware call\n",
      "            response_data = await llm_manager.call_llm_with_citations(\n",
      "                prompt=prompt, model=model, temperature=0.0, tools=search_tools, **llm_kwargs\n",
      "            )\n",
      "\n",
      "            workplan_content = response_data[\"content\"]\n",
      "            usage_metadata = response_data[\"usage_metadata\"]\n",
      "\n",
      "            # Process citations if available\n",
      "            if \"grounding_metadata\" in response_data and response_data[\"grounding_metadata\"]:\n",
      "                from yellhorn_mcp.utils.search_grounding_utils import add_citations_from_metadata\n",
      "\n",
      "                workplan_content = add_citations_from_metadata(\n",
      "                    workplan_content, response_data[\"grounding_metadata\"]\n",
      "                )\n",
      "\n",
      "            # Create completion metadata\n",
      "            completion_metadata = CompletionMetadata(\n",
      "                model_name=model,\n",
      "                status=\"✅ Workplan generated successfully\",\n",
      "                generation_time_seconds=0.0,  # Will be calculated below\n",
      "                input_tokens=usage_metadata.prompt_tokens,\n",
      "                output_tokens=usage_metadata.completion_tokens,\n",
      "                total_tokens=usage_metadata.total_tokens,\n",
      "                search_results_used=getattr(\n",
      "                    response_data.get(\"grounding_metadata\"), \"grounding_chunks\", None\n",
      "                )\n",
      "                is not None,\n",
      "                timestamp=None,  # Will be set below\n",
      "            )\n",
      "\n",
      "    except Exception as e:\n",
      "        error_message = f\"Failed to generate workplan: {str(e)}\"\n",
      "        if ctx:\n",
      "            await ctx.log(level=\"error\", message=error_message)\n",
      "        await add_issue_comment(\n",
      "            repo_path,\n",
      "            issue_number,\n",
      "            f\"❌ **Error generating workplan** – {str(e)}\",\n",
      "            github_command_func=github_command_func,\n",
      "        )\n",
      "        return\n",
      "\n",
      "    if not workplan_content:\n",
      "        api_name = \"OpenAI\" if is_openai_model else \"Gemini\"\n",
      "        error_message = (\n",
      "            f\"Failed to generate workplan: Received an empty response from {api_name} API.\"\n",
      "        )\n",
      "        if ctx:\n",
      "            await ctx.log(level=\"error\", message=error_message)\n",
      "        # Add comment instead of overwriting\n",
      "        error_message_comment = (\n",
      "            f\"⚠️ AI workplan enhancement failed: Received an empty response from {api_name} API.\"\n",
      "        )\n",
      "        await add_issue_comment(\n",
      "            repo_path, issue_number, error_message_comment, github_command_func=github_command_func\n",
      "        )\n",
      "        return\n",
      "\n",
      "    # Calculate generation time if we have metadata\n",
      "    if completion_metadata and _meta and \"start_time\" in _meta:\n",
      "        generation_time = (datetime.now(timezone.utc) - _meta[\"start_time\"]).total_seconds()\n",
      "        completion_metadata.generation_time_seconds = generation_time\n",
      "        completion_metadata.timestamp = datetime.now(timezone.utc)\n",
      "\n",
      "    # Calculate cost if we have token counts\n",
      "    if (\n",
      "        completion_metadata\n",
      "        and completion_metadata.input_tokens\n",
      "        and completion_metadata.output_tokens\n",
      "    ):\n",
      "        completion_metadata.estimated_cost = calculate_cost(\n",
      "            model, completion_metadata.input_tokens, completion_metadata.output_tokens\n",
      "        )\n",
      "\n",
      "    # Add context size\n",
      "    if completion_metadata:\n",
      "        completion_metadata.context_size_chars = len(prompt)\n",
      "\n",
      "    # Add the prefix to the workplan content\n",
      "    full_body = f\"{content_prefix}{workplan_content}\"\n",
      "\n",
      "    # Update the GitHub issue with the generated workplan\n",
      "    await update_issue_with_workplan(\n",
      "        repo_path,\n",
      "        issue_number,\n",
      "        full_body,\n",
      "        completion_metadata,\n",
      "        title,\n",
      "        github_command_func=github_command_func,\n",
      "    )\n",
      "    if ctx:\n",
      "        await ctx.log(\n",
      "            level=\"info\",\n",
      "            message=f\"Successfully updated GitHub issue #{issue_number} with generated workplan and metrics\",\n",
      "        )\n",
      "\n",
      "    # Add completion comment if we have submission metadata\n",
      "    if completion_metadata and _meta:\n",
      "        completion_comment = format_completion_comment(completion_metadata)\n",
      "        await add_issue_comment(\n",
      "            repo_path, issue_number, completion_comment, github_command_func=github_command_func\n",
      "        )\n",
      "\n",
      "\n",
      "async def process_workplan_async(\n",
      "    repo_path: Path,\n",
      "    llm_manager: LLMManager,\n",
      "    model: str,\n",
      "    title: str,\n",
      "    issue_number: str,\n",
      "    codebase_reasoning: str,\n",
      "    detailed_description: str,\n",
      "    debug: bool = False,\n",
      "    disable_search_grounding: bool = False,\n",
      "    _meta: dict[str, Any] | None = None,\n",
      "    ctx: Context | None = None,\n",
      "    github_command_func: Callable | None = None,\n",
      "    git_command_func: Callable | None = None,\n",
      ") -> None:\n",
      "    \"\"\"Generate a workplan asynchronously and update the GitHub issue.\n",
      "\n",
      "    Args:\n",
      "        repo_path: Path to the repository.\n",
      "        llm_manager: LLM Manager instance.\n",
      "        model: Model name to use (Gemini or OpenAI).\n",
      "        title: Title for the workplan.\n",
      "        issue_number: GitHub issue number to update.\n",
      "        codebase_reasoning: Reasoning mode to use for codebase analysis.\n",
      "        detailed_description: Detailed description for the workplan.\n",
      "        debug: If True, add a comment with the full prompt used for generation.\n",
      "        disable_search_grounding: If True, disables search grounding for this request.\n",
      "        _meta: Optional metadata from the caller.\n",
      "        ctx: Optional context for logging.\n",
      "        github_command_func: Optional GitHub command function (for mocking).\n",
      "        git_command_func: Optional Git command function (for mocking).\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # Create a simple logging function that uses ctx if available\n",
      "        def context_log(msg: str):\n",
      "            if ctx:\n",
      "                asyncio.create_task(ctx.log(level=\"info\", message=msg))\n",
      "\n",
      "        # Get codebase info based on reasoning mode\n",
      "        # Calculate token limit for codebase context (70% of model's context window)\n",
      "        token_counter = TokenCounter()\n",
      "        model_limit = token_counter.get_model_limit(model)\n",
      "        # Reserve tokens for prompt template, task details, and response\n",
      "        # Estimate: prompt template ~1000, task details ~500, safety margin for response ~4000\n",
      "        codebase_token_limit = int((model_limit - 5500) * 0.7)\n",
      "        \n",
      "        codebase_info, _ = await get_codebase_context(\n",
      "            repo_path, \n",
      "            codebase_reasoning, \n",
      "            context_log, \n",
      "            token_limit=codebase_token_limit,\n",
      "            model=model,\n",
      "            git_command_func=git_command_func\n",
      "        )\n",
      "\n",
      "        # Construct prompt\n",
      "        prompt = f\"\"\"You are a senior software architect and technical writer.  \n",
      "Your task is to output a GitHub-issue–ready **work-plan** that fully complies with the “Strong Work-Plan Rules” and the “Gap-Fix Guidelines” below.  \n",
      "The answer you return will be copied verbatim into a GitHub issue, so structure, order and precision matter.\n",
      "\n",
      "CONTEXT\n",
      "───────────────────\n",
      "Multi-file snippet of the current repo (trimmed for length)\n",
      "{codebase_info}\n",
      "\n",
      "One-line task title\n",
      "{title}\n",
      "\n",
      "Product / feature description from the PM\n",
      "{detailed_description}\n",
      "\n",
      "GLOBAL TONE & STYLE\n",
      "───────────────────\n",
      "• Write as one senior engineer explaining to another.  \n",
      "• Zero “TODO”, “placeholder”, or speculative wording—everything must be concrete and actionable.  \n",
      "• Be self-sufficient: an unfamiliar engineer can execute the plan end-to-end without additional guidance.  \n",
      "• All headings and check-box bullets must render correctly in GitHub Markdown.  \n",
      "• Keep line length ≤ 120 characters where feasible.\n",
      "\n",
      "TOP-LEVEL SECTIONS  (DO NOT ADD, REMOVE, OR RE-ORDER)\n",
      "──────────────────────────────────────────────────────\n",
      "## Summary  \n",
      "## Technical Details  \n",
      "## Architecture  \n",
      "## Completion Criteria & Metrics  \n",
      "## References  \n",
      "## Implementation Steps  \n",
      "## Global Test Strategy  \n",
      "## Files to Modify  \n",
      "## New Files to Create  \n",
      "\n",
      "MANDATORY CONTENT PER SECTION\n",
      "─────────────────────────────\n",
      "## Summary   (≤ 5 sentences)\n",
      "1 . Problem – one sentence that states the issue or feature.  \n",
      "2 . Proposed solution – what will be built.  \n",
      "3 . Why it matters – business or technical impact.  \n",
      "4 . Success criteria – concise, measurable, single sentence.  \n",
      "5 . Main subsystems touched.\n",
      "\n",
      "## Technical Details\n",
      "• Languages, frameworks, min versions.  \n",
      "• “External Dependencies” sub-section:  \n",
      "  – List every new third-party package AND specify how it will be introduced (e.g., `pyproject.toml` stanza, Dockerfile line).  \n",
      "• Dependency management & pinning strategy (poetry, npm, go-mods, etc.).  \n",
      "• Build, lint, formatting, type-checking commands.  \n",
      "• Logging & observability – logger names, redaction strategy, trace IDs, dashboards.  \n",
      "• Analytics/KPIs – event names, schema, when they fire.  \n",
      "• Testing frameworks & helpers (mention async helpers or fixtures unique to repo).\n",
      "\n",
      "## Architecture\n",
      "• “Existing Components Leveraged” bullet list (files / classes).  \n",
      "• “New Components Introduced” bullet list (fully enumerated).  \n",
      "• Control-flow & data-flow diagram (ASCII or Mermaid).  \n",
      "• State-management, retry/fallback, and error-handling patterns (e.g., three-strike fallback).\n",
      "\n",
      "## Completion Criteria & Metrics\n",
      "• Engineering metrics – latency, SLA, test coverage ≥ X %, lint/type-check clean, etc.  \n",
      "• Business metrics – conversion, NPS, error-rate < Y %, etc.  \n",
      "• Code-state definition of done – all CI jobs green, new DAG registered, talk-suite passes, docs updated.\n",
      "\n",
      "## References\n",
      "• Exact repo paths examined – include line numbers or symbols when helpful.  \n",
      "• External URLs (one per line).  \n",
      "• Commit hashes or tags if specific revisions were read.\n",
      "\n",
      "## Implementation Steps\n",
      "Break work into atomic tasks suitable for individual PRs.  \n",
      "Use the sub-template **verbatim** for every task:\n",
      "\n",
      "### - [ ] Step <N>: <Concise Title>  \n",
      "**Description**: 1–2 sentences.  \n",
      "**Files**: list of files created/modified in this step.  \n",
      "**Reference(s)**: pointer(s) to rows in “## References”.  \n",
      "**Test(s)**: concrete test file names, fixtures/mocks, and the CI command that must pass.\n",
      "\n",
      "Granularity rules:  \n",
      "• One node/class/function per step unless trivial.  \n",
      "• No mixed concerns (e.g., “Implement X and refactor Y” must be two steps).  \n",
      "• Each step’s **Test(s)** must name at least one assertion or expected behaviour.\n",
      "\n",
      "## Global Test Strategy\n",
      "• Unit, integration, e2e, load – what’s covered where.  \n",
      "• How to run locally (`make test`, `python -m pytest`, etc.).  \n",
      "• Env-vars / secrets layout (`.env.test`).  \n",
      "• Async helpers, fixtures, sandbox accounts.  \n",
      "• Coverage enforcement rule (PR fails if coverage < threshold).  \n",
      "\n",
      "## Files to Modify / ## New Files to Create\n",
      "• Use Markdown tables or bullet lists.  \n",
      "• For **new files** provide:  \n",
      "  – One-line purpose.  \n",
      "  – Stub code block with signature(s).  \n",
      "  – Required exports (`__all__`) or module wiring.  \n",
      "  – Note if protobuf, OpenAPI, or YAML specs also added.\n",
      "\n",
      "GAP-FIX GUIDELINES (Always Apply)\n",
      "────────────────────────────────\n",
      "1. ALWAYS describe how dependencies are added/pinned (e.g., `pyproject.toml`, `poetry.lock`).  \n",
      "2. If repo has custom test helpers (e.g., async graph helpers), reference & use them.  \n",
      "3. Call out existing services or models to be injected instead of rebuilt.  \n",
      "4. Explicitly enumerate **every** new component – no omissions.  \n",
      "5. Include retry/fallback/strike logic if part of the design pattern.  \n",
      "6. “Completion Criteria” must state both code-state and operational success metrics.  \n",
      "7. Each Implementation Step must have: references, granular scope, concrete tests.  \n",
      "8. Provide GitHub check-box list ready for copy-paste.  \n",
      "9. If conversational or persona suites are required, add a task for them.\n",
      "\n",
      "PRE-FLIGHT QUALITY GATE (Auto-check before you answer)\n",
      "───────────────────────────────────────────────────────\n",
      "✔ All top-level sections present and in correct order.  \n",
      "✔ “Summary” ≤ 5 sentences and includes Problem + Success criteria.  \n",
      "✔ “Technical Details” contains “External Dependencies” + dependency pinning method.  \n",
      "✔ Architecture lists both Existing & New components.  \n",
      "✔ Completion Criteria includes code-state AND operational metrics.  \n",
      "✔ Implementation Steps use the exact sub-template and include tests.  \n",
      "✔ Global Test Strategy explains commands and coverage enforcement.  \n",
      "✔ New Files section provides stubs and export notes.  \n",
      "✔ No placeholders, “TODO”, or speculative language.  \n",
      "✔ All repo paths / URLs referenced are enumerated in “## References”.\n",
      "\n",
      "IF ANY ITEM IS MISSING, STOP, FIX, AND RE-EMIT THE ENTIRE PLAN.\n",
      "\n",
      "BEGIN OUTPUT\n",
      "────────────\n",
      "Return only the GitHub-Markdown for the issue body, starting with “## Summary”.\n",
      "The workplan should be comprehensive enough that a developer or AI assistant could implement it without additional context, and structured in a way that makes it easy for an LLM to quickly understand and work with the contained information.\n",
      "IMPORTANT: Respond *only* with the Markdown content for the GitHub issue body. Do *not* wrap your entire response in a single Markdown code block (```). Start directly with the '## Summary' heading.\n",
      "\"\"\"\n",
      "\n",
      "        # Add the title as header prefix\n",
      "        content_prefix = f\"# {title}\\n\\n\"\n",
      "\n",
      "        # If not disable_search_grounding, use search grounding\n",
      "        if not disable_search_grounding:\n",
      "            prompt += (\n",
      "                \"Search the internet for latest package versions and describe how to use them.\"\n",
      "            )\n",
      "\n",
      "        # Generate and update issue using the helper\n",
      "        await _generate_and_update_issue(\n",
      "            repo_path,\n",
      "            llm_manager,\n",
      "            model,\n",
      "            prompt,\n",
      "            issue_number,\n",
      "            title,\n",
      "            content_prefix,\n",
      "            disable_search_grounding,\n",
      "            debug,\n",
      "            codebase_reasoning,\n",
      "            _meta,\n",
      "            ctx,\n",
      "            github_command_func,\n",
      "            git_command_func,\n",
      "        )\n",
      "\n",
      "    except Exception as e:\n",
      "        error_msg = f\"Error processing workplan: {str(e)}\"\n",
      "        if ctx:\n",
      "            await ctx.log(level=\"error\", message=error_msg)\n",
      "\n",
      "        # Try to add error comment to issue\n",
      "        try:\n",
      "            error_comment = f\"❌ **Error generating workplan**\\n\\n{str(e)}\"\n",
      "            await add_issue_comment(\n",
      "                repo_path, issue_number, error_comment, github_command_func=github_command_func\n",
      "            )\n",
      "        except Exception:\n",
      "            # If we can't even add a comment, just log\n",
      "            if ctx:\n",
      "                await ctx.log(\n",
      "                    level=\"error\", message=f\"Failed to add error comment to issue: {str(e)}\"\n",
      "                )\n",
      "\n",
      "\n",
      "async def process_revision_async(\n",
      "    repo_path: Path,\n",
      "    llm_manager: LLMManager,\n",
      "    model: str,\n",
      "    issue_number: str,\n",
      "    original_workplan: str,\n",
      "    revision_instructions: str,\n",
      "    codebase_reasoning: str,\n",
      "    debug: bool = False,\n",
      "    disable_search_grounding: bool = False,\n",
      "    _meta: dict[str, Any] | None = None,\n",
      "    ctx: Context | None = None,\n",
      "    github_command_func: Callable | None = None,\n",
      "    git_command_func: Callable | None = None,\n",
      ") -> None:\n",
      "    \"\"\"Revise an existing workplan asynchronously and update the GitHub issue.\n",
      "\n",
      "    Args:\n",
      "        repo_path: Path to the repository.\n",
      "        llm_manager: LLM Manager instance.\n",
      "        model: Model name to use.\n",
      "        issue_number: GitHub issue number to update.\n",
      "        original_workplan: The current workplan content.\n",
      "        revision_instructions: Instructions for how to revise the workplan.\n",
      "        codebase_reasoning: Reasoning mode to use for codebase analysis.\n",
      "        debug: If True, add a comment with the full prompt used for generation.\n",
      "        disable_search_grounding: If True, disables search grounding for this request.\n",
      "        _meta: Optional metadata from the caller.\n",
      "        ctx: Optional context for logging.\n",
      "        github_command_func: Optional GitHub command function (for mocking).\n",
      "        git_command_func: Optional Git command function (for mocking).\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # Create a simple logging function that uses ctx if available\n",
      "        def context_log(msg: str):\n",
      "            if ctx:\n",
      "                asyncio.create_task(ctx.log(level=\"info\", message=msg))\n",
      "\n",
      "        # Get codebase info based on reasoning mode\n",
      "        # Calculate token limit for codebase context (70% of model's context window)\n",
      "        token_counter = TokenCounter()\n",
      "        model_limit = token_counter.get_model_limit(model)\n",
      "        # Reserve tokens for prompt template, task details, and response\n",
      "        # Estimate: prompt template ~1000, task details ~500, safety margin for response ~4000\n",
      "        codebase_token_limit = int((model_limit - 5500) * 0.7)\n",
      "        \n",
      "        codebase_info, _ = await get_codebase_context(\n",
      "            repo_path, \n",
      "            codebase_reasoning, \n",
      "            context_log, \n",
      "            token_limit=codebase_token_limit,\n",
      "            model=model,\n",
      "            git_command_func=git_command_func\n",
      "        )\n",
      "\n",
      "        # Extract title from original workplan (assumes first line is # Title)\n",
      "        title_line = original_workplan.split(\"\\n\")[0] if original_workplan else \"\"\n",
      "        title = (\n",
      "            title_line.replace(\"# \", \"\").strip()\n",
      "            if title_line.startswith(\"# \")\n",
      "            else \"Workplan Revision\"\n",
      "        )\n",
      "\n",
      "        # Construct revision prompt\n",
      "        prompt = f\"\"\"You are an expert software developer tasked with revising an existing workplan based on revision instructions.\n",
      "\n",
      "# Original Workplan\n",
      "{original_workplan}\n",
      "\n",
      "# Revision Instructions\n",
      "{revision_instructions}\n",
      "\n",
      "# Codebase Context\n",
      "{codebase_info}\n",
      "\n",
      "# Instructions\n",
      "Revise the \"Original Workplan\" based on the \"Revision Instructions\" and the provided \"Codebase Context\".\n",
      "Your output should be the complete, revised workplan in the same format as the original.\n",
      "\n",
      "The revised workplan should:\n",
      "1. Incorporate all changes requested in the revision instructions\n",
      "2. Maintain the same overall structure and formatting as the original\n",
      "3. Update any implementation details that are affected by the changes\n",
      "4. Ensure all sections remain comprehensive and implementable\n",
      "\n",
      "Respond directly with the complete revised workplan in Markdown format.\n",
      "IMPORTANT: Respond *only* with the Markdown content for the GitHub issue body. Do *not* wrap your entire response in a single Markdown code block (```). Start directly with the '## Summary' heading.\n",
      "\"\"\"\n",
      "\n",
      "        # Add the title as header prefix\n",
      "        content_prefix = f\"# {title}\\n\\n\"\n",
      "\n",
      "        # llm_manager is now passed as a parameter\n",
      "\n",
      "        # Generate and update issue using the helper\n",
      "        await _generate_and_update_issue(\n",
      "            repo_path,\n",
      "            llm_manager,\n",
      "            model,\n",
      "            prompt,\n",
      "            issue_number,\n",
      "            title,\n",
      "            content_prefix,\n",
      "            disable_search_grounding,\n",
      "            debug,\n",
      "            codebase_reasoning,\n",
      "            _meta,\n",
      "            ctx,\n",
      "            github_command_func,\n",
      "            git_command_func,\n",
      "        )\n",
      "\n",
      "    except Exception as e:\n",
      "        error_msg = f\"Error processing revision: {str(e)}\"\n",
      "        if ctx:\n",
      "            await ctx.log(level=\"error\", message=error_msg)\n",
      "\n",
      "        # Try to add error comment to issue\n",
      "        try:\n",
      "            error_comment = f\"❌ **Error revising workplan**\\n\\n{str(e)}\"\n",
      "            await add_issue_comment(\n",
      "                repo_path, issue_number, error_comment, github_command_func=github_command_func\n",
      "            )\n",
      "        except Exception:\n",
      "            # If we can't even add a comment, just log\n",
      "            if ctx:\n",
      "                await ctx.log(\n",
      "                    level=\"error\", message=f\"Failed to add error comment to issue: {str(e)}\"\n",
      "                )\n",
      "\n",
      "--- File: yellhorn_mcp/server.py ---\n",
      "\"\"\"Yellhorn MCP server implementation.\n",
      "\n",
      "This module provides a Model Context Protocol (MCP) server that exposes Gemini 2.5 Pro\n",
      "and OpenAI capabilities to Claude Code for software development tasks. It offers these primary tools:\n",
      "\n",
      "1. create_workplan: Creates GitHub issues with detailed implementation plans based on\n",
      "   your codebase and task description. The workplan is generated asynchronously and the\n",
      "   issue is updated once it's ready.\n",
      "\n",
      "2. get_workplan: Retrieves the workplan content (GitHub issue body) associated with\n",
      "   a specified issue number.\n",
      "\n",
      "3. judge_workplan: Triggers an asynchronous code judgement for a Pull Request against its\n",
      "   original workplan issue.\n",
      "\n",
      "The server requires GitHub CLI to be installed and authenticated for GitHub operations.\n",
      "\"\"\"\n",
      "\n",
      "import asyncio\n",
      "import json\n",
      "import logging\n",
      "import os\n",
      "import sys\n",
      "from collections.abc import AsyncIterator\n",
      "from contextlib import asynccontextmanager\n",
      "from datetime import datetime, timezone\n",
      "from pathlib import Path\n",
      "from typing import Any\n",
      "\n",
      "from google import genai\n",
      "from mcp.server.fastmcp import Context, FastMCP\n",
      "from openai import AsyncOpenAI\n",
      "\n",
      "from yellhorn_mcp import __version__\n",
      "from yellhorn_mcp.integrations.github_integration import (\n",
      "    add_issue_comment,\n",
      "    create_github_issue,\n",
      "    get_issue_body,\n",
      ")\n",
      "from yellhorn_mcp.llm_manager import LLMManager, UsageMetadata\n",
      "from yellhorn_mcp.models.metadata_models import SubmissionMetadata\n",
      "from yellhorn_mcp.processors.context_processor import process_context_curation_async\n",
      "from yellhorn_mcp.processors.judgement_processor import get_git_diff, process_judgement_async\n",
      "from yellhorn_mcp.processors.workplan_processor import (\n",
      "    process_revision_async,\n",
      "    process_workplan_async,\n",
      ")\n",
      "from yellhorn_mcp.utils.comment_utils import extract_urls, format_submission_comment\n",
      "from yellhorn_mcp.utils.git_utils import (\n",
      "    YellhornMCPError,\n",
      "    get_default_branch,\n",
      "    get_github_pr_diff,\n",
      "    is_git_repository,\n",
      "    list_resources,\n",
      "    read_resource,\n",
      "    run_git_command,\n",
      ")\n",
      "\n",
      "logging.basicConfig(\n",
      "    stream=sys.stderr, level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\"\n",
      ")\n",
      "\n",
      "\n",
      "@asynccontextmanager\n",
      "async def app_lifespan(server: FastMCP) -> AsyncIterator[dict[str, Any]]:\n",
      "    \"\"\"Lifespan context manager for the FastMCP app.\n",
      "\n",
      "    Args:\n",
      "        server: The FastMCP server instance.\n",
      "\n",
      "    Yields:\n",
      "        Dictionary containing configuration for the server context.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If required environment variables are not set.\n",
      "    \"\"\"\n",
      "    # Get configuration from environment variables\n",
      "    repo_path = os.getenv(\"REPO_PATH\", \".\")\n",
      "    model = os.getenv(\"YELLHORN_MCP_MODEL\", \"gemini-2.5-pro\")\n",
      "    is_openai_model = model.startswith(\"gpt-\") or model.startswith(\"o\")\n",
      "\n",
      "    # Handle search grounding configuration (default to enabled for Gemini models only)\n",
      "    use_search_grounding = False\n",
      "    if not is_openai_model:  # Only enable search grounding for Gemini models\n",
      "        use_search_grounding = os.getenv(\"YELLHORN_MCP_SEARCH\", \"on\").lower() != \"off\"\n",
      "\n",
      "    # Initialize clients based on the model type\n",
      "    gemini_client = None\n",
      "    openai_client = None\n",
      "    llm_manager = None\n",
      "\n",
      "    # For Gemini models, require Gemini API key\n",
      "    if not is_openai_model:\n",
      "        gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
      "        if not gemini_api_key:\n",
      "            raise ValueError(\"GEMINI_API_KEY is required for Gemini models\")\n",
      "        # Configure Gemini API\n",
      "        gemini_client = genai.Client(api_key=gemini_api_key)\n",
      "    # For OpenAI models, require OpenAI API key\n",
      "    else:\n",
      "        openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
      "        if not openai_api_key:\n",
      "            raise ValueError(\"OPENAI_API_KEY is required for OpenAI models\")\n",
      "        # Import here to avoid loading the module if not needed\n",
      "        import httpx\n",
      "\n",
      "        # Configure OpenAI API with a custom httpx client to avoid proxy issues\n",
      "        http_client = httpx.AsyncClient()\n",
      "        openai_client = AsyncOpenAI(api_key=openai_api_key, http_client=http_client)\n",
      "\n",
      "    # Initialize LLM Manager with available clients\n",
      "    if gemini_client or openai_client:\n",
      "        llm_manager = LLMManager(\n",
      "            openai_client=openai_client,\n",
      "            gemini_client=gemini_client,\n",
      "            config={\n",
      "                \"safety_margin_tokens\": 2000,  # Reserve tokens for system prompts and responses\n",
      "                \"overlap_ratio\": 0.1,  # 10% overlap between chunks\n",
      "                \"chunk_strategy\": \"paragraph\",  # Use paragraph-based chunking\n",
      "                \"aggregation_strategy\": \"concatenate\",  # Concatenate chunk responses\n",
      "            },\n",
      "        )\n",
      "\n",
      "    # Validate repository path\n",
      "    repo_path = Path(repo_path).resolve()\n",
      "    if not is_git_repository(repo_path):\n",
      "        raise ValueError(f\"Path {repo_path} is not a Git repository\")\n",
      "\n",
      "    try:\n",
      "        # Logging happens outside lifespan context via logging statements since\n",
      "        # the server context is not available here\n",
      "        logging.info(f\"Starting Yellhorn MCP server at http://127.0.0.1:8000\")\n",
      "        logging.info(f\"Repository path: {repo_path}\")\n",
      "        logging.info(f\"Using model: {model}\")\n",
      "        logging.info(\n",
      "            f\"Google Search Grounding: {'enabled' if use_search_grounding else 'disabled'}\"\n",
      "        )\n",
      "\n",
      "        yield {\n",
      "            \"repo_path\": repo_path,\n",
      "            \"gemini_client\": gemini_client,\n",
      "            \"openai_client\": openai_client,\n",
      "            \"llm_manager\": llm_manager,\n",
      "            \"model\": model,\n",
      "            \"use_search_grounding\": use_search_grounding,\n",
      "        }\n",
      "    finally:\n",
      "        # Cleanup if needed\n",
      "        pass\n",
      "\n",
      "\n",
      "# Initialize MCP server\n",
      "mcp = FastMCP(\n",
      "    name=\"yellhorn-mcp\",\n",
      "    dependencies=[\"google-genai~=1.8.0\", \"aiohttp~=3.11.14\", \"pydantic~=2.11.1\", \"openai~=1.23.6\"],\n",
      "    lifespan=app_lifespan,\n",
      ")\n",
      "\n",
      "\n",
      "# Resources are not implemented with decorators in this version\n",
      "# They would need to be set up differently with FastMCP\n",
      "\n",
      "\n",
      "@mcp.tool(\n",
      "    name=\"create_workplan\",\n",
      "    description=\"\"\"Creates a GitHub issue with a detailed implementation plan.\n",
      "\n",
      "This tool will:\n",
      "1. Create a GitHub issue immediately with the provided title and description\n",
      "2. Launch a background AI process to generate a comprehensive workplan\n",
      "3. Update the issue with the generated workplan once complete\n",
      "\n",
      "The AI will analyze your entire codebase (respecting .gitignore) to create a detailed plan with:\n",
      "- Specific files to modify/create\n",
      "- Code snippets and examples\n",
      "- Step-by-step implementation instructions\n",
      "- Testing strategies\n",
      "\n",
      "Codebase reasoning modes:\n",
      "- \"full\": Complete file contents (most comprehensive)\n",
      "- \"lsp\": Function signatures and docstrings only (lighter weight)\n",
      "- \"file_structure\": Directory tree only (fastest)\n",
      "- \"none\": No codebase context\n",
      "\n",
      "Returns the created issue URL and number immediately.\"\"\",\n",
      ")\n",
      "async def create_workplan(\n",
      "    ctx: Context,\n",
      "    title: str,\n",
      "    detailed_description: str,\n",
      "    codebase_reasoning: str = \"full\",\n",
      "    debug: bool = False,\n",
      "    disable_search_grounding: bool = False,\n",
      ") -> str:\n",
      "    \"\"\"Creates a GitHub issue with a detailed implementation plan based on codebase analysis.\n",
      "\n",
      "    Args:\n",
      "        ctx: Server context.\n",
      "        title: Title for the GitHub issue and workplan.\n",
      "        detailed_description: Detailed description of what needs to be implemented.\n",
      "        codebase_reasoning: Reasoning mode for codebase analysis:\n",
      "               - \"full\": Include complete file contents (most comprehensive)\n",
      "               - \"lsp\": Include only function signatures and docstrings (lighter weight)\n",
      "               - \"file_structure\": Include only directory/file structure (fastest)\n",
      "               - \"none\": No codebase context (relies only on description)\n",
      "        debug: If True, adds a comment to the issue with the full prompt used for generation.\n",
      "        disable_search_grounding: If True, disables Google Search Grounding for this request.\n",
      "\n",
      "    Returns:\n",
      "        JSON string containing the issue URL and number.\n",
      "\n",
      "    Raises:\n",
      "        YellhornMCPError: If issue creation fails.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        repo_path: Path = ctx.request_context.lifespan_context[\"repo_path\"]\n",
      "\n",
      "        # Handle search grounding override if specified\n",
      "        original_search_grounding = ctx.request_context.lifespan_context.get(\n",
      "            \"use_search_grounding\", True\n",
      "        )\n",
      "        if disable_search_grounding:\n",
      "            ctx.request_context.lifespan_context[\"use_search_grounding\"] = False\n",
      "            await ctx.log(\n",
      "                level=\"info\",\n",
      "                message=\"Search grounding temporarily disabled for this request\",\n",
      "            )\n",
      "\n",
      "        # Create the GitHub issue first\n",
      "        issue_data = await create_github_issue(repo_path, title, detailed_description)\n",
      "        issue_number = issue_data[\"number\"]\n",
      "        issue_url = issue_data[\"url\"]\n",
      "\n",
      "        await ctx.log(\n",
      "            level=\"info\",\n",
      "            message=f\"Created GitHub issue #{issue_number}\",\n",
      "        )\n",
      "\n",
      "        # Extract URLs from the description\n",
      "        submitted_urls = extract_urls(detailed_description)\n",
      "\n",
      "        # Add submission comment\n",
      "        submission_metadata = SubmissionMetadata(\n",
      "            status=\"Generating workplan...\",\n",
      "            model_name=ctx.request_context.lifespan_context[\"model\"],\n",
      "            search_grounding_enabled=ctx.request_context.lifespan_context.get(\n",
      "                \"use_search_grounding\", False\n",
      "            ),\n",
      "            yellhorn_version=__version__,\n",
      "            submitted_urls=submitted_urls if submitted_urls else None,\n",
      "            codebase_reasoning_mode=codebase_reasoning,\n",
      "            timestamp=datetime.now(timezone.utc),\n",
      "        )\n",
      "\n",
      "        submission_comment = format_submission_comment(submission_metadata)\n",
      "        await add_issue_comment(repo_path, issue_number, submission_comment)\n",
      "\n",
      "        # Skip AI workplan generation if codebase_reasoning is \"none\"\n",
      "        if codebase_reasoning != \"none\":\n",
      "            # Get clients from context\n",
      "            gemini_client = ctx.request_context.lifespan_context.get(\"gemini_client\")\n",
      "            openai_client = ctx.request_context.lifespan_context.get(\"openai_client\")\n",
      "            llm_manager = ctx.request_context.lifespan_context.get(\"llm_manager\")\n",
      "            model = ctx.request_context.lifespan_context[\"model\"]\n",
      "\n",
      "            # Store codebase_reasoning in context for process_workplan_async\n",
      "            ctx.request_context.lifespan_context[\"codebase_reasoning\"] = codebase_reasoning\n",
      "\n",
      "            # Launch background task to process the workplan with AI\n",
      "            await ctx.log(\n",
      "                level=\"info\",\n",
      "                message=f\"Launching background task to generate workplan with AI model {model}\",\n",
      "            )\n",
      "            start_time = datetime.now(timezone.utc)\n",
      "\n",
      "            asyncio.create_task(\n",
      "                process_workplan_async(\n",
      "                    repo_path,\n",
      "                    llm_manager,\n",
      "                    model,\n",
      "                    title,\n",
      "                    issue_number,\n",
      "                    codebase_reasoning,\n",
      "                    detailed_description,\n",
      "                    debug=debug,\n",
      "                    disable_search_grounding=disable_search_grounding,\n",
      "                    _meta={\n",
      "                        \"original_search_grounding\": original_search_grounding,\n",
      "                        \"start_time\": start_time,\n",
      "                        \"submitted_urls\": submitted_urls,\n",
      "                    },\n",
      "                    ctx=ctx,\n",
      "                    github_command_func=ctx.request_context.lifespan_context.get(\n",
      "                        \"github_command_func\"\n",
      "                    ),\n",
      "                    git_command_func=ctx.request_context.lifespan_context.get(\n",
      "                        \"git_command_func\"\n",
      "                    ),\n",
      "                )\n",
      "            )\n",
      "        else:\n",
      "            await ctx.log(\n",
      "                level=\"info\",\n",
      "                message=\"Skipping AI workplan generation (codebase_reasoning='none')\",\n",
      "            )\n",
      "\n",
      "        # Restore original search grounding setting if modified\n",
      "        if disable_search_grounding:\n",
      "            ctx.request_context.lifespan_context[\"use_search_grounding\"] = original_search_grounding\n",
      "\n",
      "        # Return the issue URL and number as JSON\n",
      "        return json.dumps({\"issue_url\": issue_url, \"issue_number\": issue_number})\n",
      "\n",
      "    except Exception as e:\n",
      "        raise YellhornMCPError(f\"Failed to create workplan: {str(e)}\")\n",
      "\n",
      "\n",
      "@mcp.tool(\n",
      "    name=\"get_workplan\",\n",
      "    description=\"Retrieves the workplan content (GitHub issue body) for a specified issue number.\",\n",
      ")\n",
      "async def get_workplan(ctx: Context, issue_number: str) -> str:\n",
      "    \"\"\"Retrieves the workplan content for a specified issue number.\n",
      "\n",
      "    Args:\n",
      "        ctx: Server context.\n",
      "        issue_number: The GitHub issue number to retrieve.\n",
      "\n",
      "    Returns:\n",
      "        The workplan content as a string.\n",
      "\n",
      "    Raises:\n",
      "        YellhornMCPError: If retrieval fails.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        repo_path: Path = ctx.request_context.lifespan_context[\"repo_path\"]\n",
      "        return await get_issue_body(repo_path, issue_number)\n",
      "    except Exception as e:\n",
      "        raise YellhornMCPError(f\"Failed to retrieve workplan: {str(e)}\")\n",
      "\n",
      "\n",
      "@mcp.tool(\n",
      "    name=\"revise_workplan\",\n",
      "    description=\"\"\"Updates an existing workplan based on revision instructions.\n",
      "\n",
      "This tool will:\n",
      "1. Fetch the existing workplan from the specified GitHub issue\n",
      "2. Launch a background AI process to revise the workplan based on your instructions\n",
      "3. Update the issue with the revised workplan once complete\n",
      "\n",
      "The AI will use the same codebase analysis mode and model as the original workplan.\n",
      "\n",
      "Returns the issue URL and number immediately.\"\"\",\n",
      ")\n",
      "async def revise_workplan(\n",
      "    ctx: Context,\n",
      "    issue_number: str,\n",
      "    revision_instructions: str,\n",
      "    codebase_reasoning: str = \"full\",\n",
      "    debug: bool = False,\n",
      "    disable_search_grounding: bool = False,\n",
      ") -> str:\n",
      "    \"\"\"Revises an existing workplan based on revision instructions.\n",
      "\n",
      "    Args:\n",
      "        ctx: Server context.\n",
      "        issue_number: The GitHub issue number containing the workplan to revise.\n",
      "        revision_instructions: Instructions describing how to revise the workplan.\n",
      "        codebase_reasoning: Reasoning mode for codebase analysis (same options as create_workplan).\n",
      "        debug: If True, adds a comment to the issue with the full prompt used for generation.\n",
      "        disable_search_grounding: If True, disables Google Search Grounding for this request.\n",
      "\n",
      "    Returns:\n",
      "        JSON string containing the issue URL and number.\n",
      "\n",
      "    Raises:\n",
      "        YellhornMCPError: If revision fails.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        repo_path: Path = ctx.request_context.lifespan_context[\"repo_path\"]\n",
      "\n",
      "        # Fetch original workplan\n",
      "        original_workplan = await get_issue_body(repo_path, issue_number)\n",
      "        if not original_workplan:\n",
      "            raise YellhornMCPError(f\"Could not retrieve workplan for issue #{issue_number}\")\n",
      "\n",
      "        # Handle search grounding override if specified\n",
      "        original_search_grounding = ctx.request_context.lifespan_context.get(\n",
      "            \"use_search_grounding\", True\n",
      "        )\n",
      "        if disable_search_grounding:\n",
      "            ctx.request_context.lifespan_context[\"use_search_grounding\"] = False\n",
      "            await ctx.log(\n",
      "                level=\"info\",\n",
      "                message=\"Search grounding temporarily disabled for this request\",\n",
      "            )\n",
      "\n",
      "        # Extract URLs from the revision instructions\n",
      "        submitted_urls = extract_urls(revision_instructions)\n",
      "\n",
      "        # Add submission comment\n",
      "        submission_metadata = SubmissionMetadata(\n",
      "            status=\"Revising workplan...\",\n",
      "            model_name=ctx.request_context.lifespan_context[\"model\"],\n",
      "            search_grounding_enabled=ctx.request_context.lifespan_context.get(\n",
      "                \"use_search_grounding\", False\n",
      "            ),\n",
      "            yellhorn_version=__version__,\n",
      "            submitted_urls=submitted_urls if submitted_urls else None,\n",
      "            codebase_reasoning_mode=codebase_reasoning,\n",
      "            timestamp=datetime.now(timezone.utc),\n",
      "        )\n",
      "\n",
      "        submission_comment = format_submission_comment(submission_metadata)\n",
      "        await add_issue_comment(repo_path, issue_number, submission_comment)\n",
      "\n",
      "        # Get clients from context\n",
      "        gemini_client = ctx.request_context.lifespan_context.get(\"gemini_client\")\n",
      "        openai_client = ctx.request_context.lifespan_context.get(\"openai_client\")\n",
      "        llm_manager = ctx.request_context.lifespan_context.get(\"llm_manager\")\n",
      "        model = ctx.request_context.lifespan_context[\"model\"]\n",
      "\n",
      "        # Launch background task to process the revision\n",
      "        await ctx.log(\n",
      "            level=\"info\",\n",
      "            message=f\"Launching background task to revise workplan with AI model {model}\",\n",
      "        )\n",
      "        start_time = datetime.now(timezone.utc)\n",
      "\n",
      "        asyncio.create_task(\n",
      "            process_revision_async(\n",
      "                repo_path,\n",
      "                llm_manager,\n",
      "                model,\n",
      "                issue_number,\n",
      "                original_workplan,\n",
      "                revision_instructions,\n",
      "                codebase_reasoning,\n",
      "                debug=debug,\n",
      "                disable_search_grounding=disable_search_grounding,\n",
      "                _meta={\n",
      "                    \"original_search_grounding\": original_search_grounding,\n",
      "                    \"start_time\": start_time,\n",
      "                    \"submitted_urls\": submitted_urls,\n",
      "                },\n",
      "                ctx=ctx,\n",
      "                github_command_func=ctx.request_context.lifespan_context.get(\"github_command_func\"),\n",
      "                git_command_func=ctx.request_context.lifespan_context.get(\"git_command_func\"),\n",
      "            )\n",
      "        )\n",
      "\n",
      "        # Restore original search grounding setting if modified\n",
      "        if disable_search_grounding:\n",
      "            ctx.request_context.lifespan_context[\"use_search_grounding\"] = original_search_grounding\n",
      "\n",
      "        # Get issue URL\n",
      "        get_issue_url_cmd = await run_github_command(\n",
      "            repo_path, [\"issue\", \"view\", issue_number, \"--json\", \"url\"],\n",
      "            github_command_func=ctx.request_context.lifespan_context.get(\"github_command_func\")\n",
      "        )\n",
      "        issue_data = json.loads(get_issue_url_cmd)\n",
      "        issue_url = issue_data[\"url\"]\n",
      "\n",
      "        # Return the issue URL and number as JSON\n",
      "        return json.dumps({\"issue_url\": issue_url, \"issue_number\": issue_number})\n",
      "\n",
      "    except Exception as e:\n",
      "        raise YellhornMCPError(f\"Failed to revise workplan: {str(e)}\")\n",
      "\n",
      "\n",
      "@mcp.tool(\n",
      "    name=\"curate_context\",\n",
      "    description=\"\"\"Analyzes the codebase and creates a .yellhorncontext file listing directories to be included in AI context.\n",
      "\n",
      "This tool helps optimize AI context by:\n",
      "1. Analyzing your codebase structure\n",
      "2. Understanding the task you want to accomplish\n",
      "3. Creating a .yellhorncontext file that lists relevant directories\n",
      "4. Subsequent workplan/judgement calls will only include files from these directories\n",
      "\n",
      "The .yellhorncontext file acts as a whitelist - only files matching the patterns will be included.\n",
      "This significantly reduces token usage and improves AI focus on relevant code.\n",
      "\n",
      "Example .yellhorncontext:\n",
      "src/api/\n",
      "src/models/\n",
      "tests/api/\n",
      "*.config.js\"\"\",\n",
      ")\n",
      "async def curate_context(\n",
      "    ctx: Context,\n",
      "    user_task: str,\n",
      "    codebase_reasoning: str = \"file_structure\",\n",
      "    ignore_file_path: str = \".yellhornignore\",\n",
      "    output_path: str = \".yellhorncontext\",\n",
      "    disable_search_grounding: bool = False,\n",
      "    debug: bool = False,\n",
      ") -> str:\n",
      "    \"\"\"Analyzes codebase structure and creates a context curation file.\n",
      "\n",
      "    Args:\n",
      "        ctx: Server context.\n",
      "        user_task: Description of the task the user wants to accomplish.\n",
      "        codebase_reasoning: How to analyze the codebase:\n",
      "               - \"file_structure\": Only directory structure (recommended, fastest)\n",
      "               - \"lsp\": Include function signatures (slower)\n",
      "               - \"full\": Include file contents (slowest, not recommended)\n",
      "               - \"none\": No codebase analysis (not recommended)\n",
      "        ignore_file_path: Path to the ignore file. Defaults to \".yellhornignore\".\n",
      "        output_path: Path where the .yellhorncontext file will be created.\n",
      "        depth_limit: Maximum directory depth to analyze (0 means no limit).\n",
      "        disable_search_grounding: If True, disables Google Search Grounding.\n",
      "        debug: If True, logs the full prompt sent to the LLM.\n",
      "\n",
      "    Returns:\n",
      "        Success message with the created file path.\n",
      "\n",
      "    Raises:\n",
      "        YellhornMCPError: If context curation fails.\n",
      "    \"\"\"\n",
      "    original_search_grounding = True\n",
      "    try:\n",
      "        # Get repository path from context\n",
      "        repo_path: Path = ctx.request_context.lifespan_context[\"repo_path\"]\n",
      "        llm_manager: LLMManager = ctx.request_context.lifespan_context.get(\"llm_manager\")\n",
      "        model: str = ctx.request_context.lifespan_context[\"model\"]\n",
      "\n",
      "        if not llm_manager:\n",
      "            raise YellhornMCPError(\"LLM Manager not initialized\")\n",
      "\n",
      "        # Handle search grounding override if specified\n",
      "        original_search_grounding = ctx.request_context.lifespan_context.get(\n",
      "            \"use_search_grounding\", True\n",
      "        )\n",
      "        if disable_search_grounding:\n",
      "            ctx.request_context.lifespan_context[\"use_search_grounding\"] = False\n",
      "            await ctx.log(\n",
      "                level=\"info\",\n",
      "                message=\"Search grounding temporarily disabled for this request\",\n",
      "            )\n",
      "\n",
      "        # Delegate to the processor\n",
      "        result = await process_context_curation_async(\n",
      "            repo_path=repo_path,\n",
      "            llm_manager=llm_manager,\n",
      "            model=model,\n",
      "            user_task=user_task,\n",
      "            output_path=output_path,\n",
      "            codebase_reasoning=codebase_reasoning,\n",
      "            debug=debug,\n",
      "            ctx=ctx,\n",
      "        )\n",
      "\n",
      "        # Restore original search grounding setting if modified\n",
      "        if disable_search_grounding:\n",
      "            ctx.request_context.lifespan_context[\"use_search_grounding\"] = original_search_grounding\n",
      "\n",
      "        return json.dumps(\n",
      "            {\"status\": \"✅ Context curation completed successfully\", \"message\": result}\n",
      "        )\n",
      "\n",
      "    except Exception as e:\n",
      "        # Restore original search grounding setting on error\n",
      "        if disable_search_grounding:\n",
      "            try:\n",
      "                ctx.request_context.lifespan_context[\"use_search_grounding\"] = (\n",
      "                    original_search_grounding\n",
      "                )\n",
      "            except NameError:\n",
      "                pass  # original_search_grounding was not defined yet\n",
      "        raise YellhornMCPError(f\"Failed to curate context: {str(e)}\")\n",
      "\n",
      "\n",
      "@mcp.tool(\n",
      "    name=\"judge_workplan\",\n",
      "    description=\"\"\"Triggers an asynchronous code judgement comparing two git refs against a workplan.\n",
      "\n",
      "This tool will:\n",
      "1. Create a sub-issue linked to the workplan immediately\n",
      "2. Launch a background AI process to analyze the code changes\n",
      "3. Update the sub-issue with the judgement once complete\n",
      "\n",
      "The judgement will evaluate:\n",
      "- Whether the implementation follows the workplan\n",
      "- Code quality and completeness\n",
      "- Missing or incomplete items\n",
      "- Suggestions for improvement\n",
      "\n",
      "Supports comparing:\n",
      "- Branches (e.g., feature-branch vs main)\n",
      "- Commits (e.g., abc123 vs def456)\n",
      "- PR changes (automatically uses PR's base and head)\n",
      "\n",
      "Returns the sub-issue URL immediately.\"\"\",\n",
      ")\n",
      "async def judge_workplan(\n",
      "    ctx: Context,\n",
      "    issue_number: str,\n",
      "    base_ref: str = \"main\",\n",
      "    head_ref: str = \"HEAD\",\n",
      "    codebase_reasoning: str = \"full\",\n",
      "    debug: bool = False,\n",
      "    disable_search_grounding: bool = False,\n",
      "    subissue_to_update: str | None = None,\n",
      "    pr_url: str | None = None,\n",
      ") -> str:\n",
      "    \"\"\"Triggers an asynchronous code judgement for changes against a workplan.\n",
      "\n",
      "    Args:\n",
      "        ctx: Server context.\n",
      "        issue_number: The workplan issue number to judge against.\n",
      "        base_ref: The base git reference (default: \"main\").\n",
      "        head_ref: The head git reference (default: \"HEAD\").\n",
      "        codebase_reasoning: Reasoning mode for codebase analysis:\n",
      "               - \"full\": Include complete file contents and full diff\n",
      "               - \"lsp\": Include function signatures and diff of changed functions\n",
      "               - \"file_structure\": Include only file structure and list of changed files\n",
      "               - \"none\": No codebase context, only diff summary\n",
      "        debug: If True, adds a comment with the full prompt used for generation.\n",
      "        disable_search_grounding: If True, disables Google Search Grounding.\n",
      "\n",
      "    Returns:\n",
      "        JSON string containing the sub-issue URL and number.\n",
      "\n",
      "    Raises:\n",
      "        YellhornMCPError: If judgement creation fails.\n",
      "    \"\"\"\n",
      "    original_search_grounding = True\n",
      "    try:\n",
      "        repo_path: Path = ctx.request_context.lifespan_context[\"repo_path\"]\n",
      "        model = ctx.request_context.lifespan_context[\"model\"]\n",
      "        gemini_client = ctx.request_context.lifespan_context.get(\"gemini_client\")\n",
      "        openai_client = ctx.request_context.lifespan_context.get(\"openai_client\")\n",
      "        llm_manager = ctx.request_context.lifespan_context.get(\"llm_manager\")\n",
      "\n",
      "        # Handle search grounding override if specified\n",
      "        original_search_grounding = ctx.request_context.lifespan_context.get(\n",
      "            \"use_search_grounding\", True\n",
      "        )\n",
      "        if disable_search_grounding:\n",
      "            ctx.request_context.lifespan_context[\"use_search_grounding\"] = False\n",
      "            await ctx.log(\n",
      "                level=\"info\",\n",
      "                message=\"Search grounding temporarily disabled for this request\",\n",
      "            )\n",
      "\n",
      "        # Use default branch if base_ref is \"main\" but the repo uses \"master\"\n",
      "        if base_ref == \"main\":\n",
      "            default_branch = await get_default_branch(repo_path)\n",
      "            if default_branch != \"main\":\n",
      "                await ctx.log(\n",
      "                    level=\"info\",\n",
      "                    message=f\"Using default branch '{default_branch}' instead of 'main'\",\n",
      "                )\n",
      "                base_ref = default_branch\n",
      "\n",
      "        # Check if issue_number is a PR URL\n",
      "        if issue_number.startswith(\"http\") and \"/pull/\" in issue_number:\n",
      "            # This is a PR URL, we need to extract the diff and find the related workplan\n",
      "            pr_diff = await get_github_pr_diff(repo_path, issue_number)\n",
      "\n",
      "            # Extract PR number for finding related workplan\n",
      "            import re\n",
      "\n",
      "            pr_match = re.search(r\"/pull/(\\d+)\", issue_number)\n",
      "            if not pr_match:\n",
      "                raise YellhornMCPError(f\"Invalid PR URL: {issue_number}\")\n",
      "\n",
      "            pr_number = pr_match.group(1)\n",
      "\n",
      "            # Try to find workplan issue number in PR description or title\n",
      "            # For now, we'll ask the user to provide the workplan issue number\n",
      "            raise YellhornMCPError(\n",
      "                f\"PR URL detected. Please provide the workplan issue number instead of PR URL. \"\n",
      "                f\"You can find the workplan issue number in the PR description.\"\n",
      "            )\n",
      "\n",
      "        # Fetch the workplan\n",
      "        workplan = await get_issue_body(repo_path, issue_number)\n",
      "\n",
      "        # Handle PR URL or git refs for diff generation\n",
      "        if pr_url:\n",
      "            # Use PR diff instead of git refs\n",
      "            diff = await get_github_pr_diff(repo_path, pr_url)\n",
      "            # For PR, use placeholder commit hashes\n",
      "            base_commit_hash = \"pr_base\"\n",
      "            head_commit_hash = \"pr_head\"\n",
      "        else:\n",
      "            # Resolve git references to commit hashes\n",
      "            base_commit_hash = await run_git_command(repo_path, [\"rev-parse\", base_ref], ctx.request_context.lifespan_context.get(\"git_command_func\"))\n",
      "            head_commit_hash = await run_git_command(repo_path, [\"rev-parse\", head_ref], ctx.request_context.lifespan_context.get(\"git_command_func\"))\n",
      "            # Generate diff for review\n",
      "            diff = await get_git_diff(repo_path, base_ref, head_ref, codebase_reasoning, ctx.request_context.lifespan_context.get(\"git_command_func\"))\n",
      "\n",
      "        # Check if diff is empty or only contains the header for file_structure mode\n",
      "        is_empty = not diff.strip() or (\n",
      "            codebase_reasoning in [\"file_structure\", \"none\"]\n",
      "            and diff.strip() == f\"Changed files between {base_ref} and {head_ref}:\"\n",
      "        )\n",
      "\n",
      "        if is_empty:\n",
      "            # No changes to judge\n",
      "            return json.dumps(\n",
      "                {\n",
      "                    \"error\": f\"No changes found between {base_ref} and {head_ref}\",\n",
      "                    \"base_commit\": base_commit_hash,\n",
      "                    \"head_commit\": head_commit_hash,\n",
      "                }\n",
      "            )\n",
      "\n",
      "        # Extract URLs from the workplan\n",
      "        submitted_urls = extract_urls(workplan)\n",
      "\n",
      "        # Create a placeholder sub-issue immediately\n",
      "        submission_metadata = SubmissionMetadata(\n",
      "            status=\"Generating judgement...\",\n",
      "            model_name=model,\n",
      "            search_grounding_enabled=ctx.request_context.lifespan_context.get(\n",
      "                \"use_search_grounding\", False\n",
      "            ),\n",
      "            yellhorn_version=__version__,\n",
      "            submitted_urls=submitted_urls if submitted_urls else None,\n",
      "            codebase_reasoning_mode=codebase_reasoning,\n",
      "            timestamp=datetime.now(timezone.utc),\n",
      "        )\n",
      "\n",
      "        submission_comment = format_submission_comment(submission_metadata)\n",
      "        placeholder_body = f\"Parent workplan: #{issue_number}\\n\\n## Status\\nGenerating judgement...\\n\\n{submission_comment}\"\n",
      "        judgement_title = f\"Judgement for #{issue_number}: {head_ref} vs {base_ref}\"\n",
      "\n",
      "        # Create or update the sub-issue\n",
      "        if subissue_to_update:\n",
      "            # Update existing subissue\n",
      "            subissue_number = subissue_to_update\n",
      "            subissue_url = f\"https://github.com/{repo_path.name}/issues/{subissue_number}\"\n",
      "            await update_github_issue(repo_path, subissue_number, placeholder_body)\n",
      "        else:\n",
      "            # Create new sub-issue\n",
      "            from yellhorn_mcp.integrations.github_integration import create_judgement_subissue\n",
      "\n",
      "            subissue_url = await create_judgement_subissue(\n",
      "                repo_path, issue_number, judgement_title, placeholder_body\n",
      "            )\n",
      "\n",
      "            # Extract sub-issue number from URL\n",
      "            import re\n",
      "\n",
      "            issue_match = re.search(r\"/issues/(\\d+)\", subissue_url)\n",
      "            subissue_number = issue_match.group(1) if issue_match else None\n",
      "\n",
      "        await ctx.log(\n",
      "            level=\"info\",\n",
      "            message=f\"Created judgement sub-issue: {subissue_url}\",\n",
      "        )\n",
      "\n",
      "        # Launch background task to generate judgement\n",
      "        await ctx.log(\n",
      "            level=\"info\",\n",
      "            message=f\"Launching background task to generate judgement with AI model {model}\",\n",
      "        )\n",
      "\n",
      "        # Prepare metadata for async processing\n",
      "        start_time = datetime.now(timezone.utc)\n",
      "\n",
      "        asyncio.create_task(\n",
      "            process_judgement_async(\n",
      "                repo_path,\n",
      "                llm_manager,\n",
      "                model,\n",
      "                workplan,\n",
      "                diff,\n",
      "                base_ref,\n",
      "                head_ref,\n",
      "                base_commit_hash,\n",
      "                head_commit_hash,\n",
      "                issue_number,\n",
      "                subissue_to_update=subissue_number,\n",
      "                debug=debug,\n",
      "                codebase_reasoning=codebase_reasoning,\n",
      "                disable_search_grounding=disable_search_grounding,\n",
      "                _meta={\n",
      "                    \"original_search_grounding\": original_search_grounding,\n",
      "                    \"start_time\": start_time,\n",
      "                    \"submitted_urls\": submitted_urls,\n",
      "                },\n",
      "                ctx=ctx,\n",
      "                github_command_func=ctx.request_context.lifespan_context.get(\"github_command_func\"),\n",
      "                git_command_func=ctx.request_context.lifespan_context.get(\"git_command_func\"),\n",
      "            )\n",
      "        )\n",
      "\n",
      "        # Restore original search grounding setting if modified\n",
      "        if disable_search_grounding:\n",
      "            ctx.request_context.lifespan_context[\"use_search_grounding\"] = original_search_grounding\n",
      "\n",
      "        # Return the sub-issue URL and number as JSON\n",
      "        return json.dumps({\"subissue_url\": subissue_url, \"subissue_number\": subissue_number})\n",
      "\n",
      "    except Exception as e:\n",
      "        # Restore original search grounding setting on error\n",
      "        if disable_search_grounding:\n",
      "            try:\n",
      "                ctx.request_context.lifespan_context[\"use_search_grounding\"] = (\n",
      "                    original_search_grounding\n",
      "                )\n",
      "            except NameError:\n",
      "                pass  # original_search_grounding was not defined yet\n",
      "        raise YellhornMCPError(f\"Failed to create judgement: {str(e)}\")\n",
      "\n",
      "\n",
      "from yellhorn_mcp.integrations.gemini_integration import async_generate_content_with_config\n",
      "from yellhorn_mcp.integrations.github_integration import (\n",
      "    add_issue_comment as add_github_issue_comment,\n",
      ")\n",
      "from yellhorn_mcp.processors.judgement_processor import get_git_diff\n",
      "from yellhorn_mcp.formatters import (\n",
      "    build_file_structure_context,\n",
      "    format_codebase_for_prompt,\n",
      "    get_codebase_snapshot,\n",
      ")\n",
      "from yellhorn_mcp.utils.comment_utils import format_completion_comment, format_submission_comment\n",
      "\n",
      "# Re-export for backward compatibility with tests\n",
      "from yellhorn_mcp.utils.cost_tracker_utils import calculate_cost, format_metrics_section\n",
      "from yellhorn_mcp.utils.git_utils import (\n",
      "    add_github_issue_comment as add_github_issue_comment_from_git_utils,\n",
      ")\n",
      "from yellhorn_mcp.utils.git_utils import (\n",
      "    create_github_subissue,\n",
      "    ensure_label_exists,\n",
      "    get_default_branch,\n",
      "    get_github_issue_body,\n",
      "    get_github_pr_diff,\n",
      "    post_github_pr_review,\n",
      "    run_git_command,\n",
      "    run_github_command,\n",
      "    update_github_issue,\n",
      ")\n",
      "from yellhorn_mcp.utils.lsp_utils import get_lsp_diff, get_lsp_snapshot\n",
      "from yellhorn_mcp.utils.search_grounding_utils import _get_gemini_search_tools\n",
      "\n",
      "# Export for use by the CLI\n",
      "__all__ = [\n",
      "    \"mcp\",\n",
      "    \"process_workplan_async\",\n",
      "    \"process_judgement_async\",\n",
      "    \"process_context_curation_async\",\n",
      "    \"calculate_cost\",\n",
      "    \"format_metrics_section\",\n",
      "    \"get_codebase_snapshot\",\n",
      "    \"build_file_structure_context\",\n",
      "    \"format_codebase_for_prompt\",\n",
      "    \"get_git_diff\",\n",
      "    \"get_lsp_snapshot\",\n",
      "    \"get_lsp_diff\",\n",
      "    \"is_git_repository\",\n",
      "    \"YellhornMCPError\",\n",
      "    \"add_github_issue_comment\",\n",
      "    \"update_github_issue\",\n",
      "    \"create_github_subissue\",\n",
      "    \"get_github_issue_body\",\n",
      "    \"run_github_command\",\n",
      "    \"run_git_command\",\n",
      "    \"ensure_label_exists\",\n",
      "    \"get_default_branch\",\n",
      "    \"get_github_pr_diff\",\n",
      "    \"format_submission_comment\",\n",
      "    \"format_completion_comment\",\n",
      "    \"create_workplan\",\n",
      "    \"get_workplan\",\n",
      "    \"judge_workplan\",\n",
      "    \"curate_context\",\n",
      "    \"app_lifespan\",\n",
      "    \"_get_gemini_search_tools\",\n",
      "    \"async_generate_content_with_config\",\n",
      "    \"add_github_issue_comment_from_git_utils\",\n",
      "    \"post_github_pr_review\",\n",
      "]\n",
      "\n",
      "--- File: yellhorn_mcp/token_counter.py ---\n",
      "\"\"\"Token counting utility using tiktoken for accurate token estimation.\"\"\"\n",
      "\n",
      "from typing import Any, Dict, Optional\n",
      "\n",
      "import tiktoken\n",
      "\n",
      "\n",
      "class TokenCounter:\n",
      "    \"\"\"Handles token counting for different models using tiktoken.\"\"\"\n",
      "\n",
      "    # Model-specific token limits\n",
      "    MODEL_LIMITS: Dict[str, int] = {\n",
      "        # OpenAI models\n",
      "        \"gpt-4o\": 128_000,\n",
      "        \"gpt-4o-mini\": 128_000,\n",
      "        \"o4-mini\": 200_000,\n",
      "        \"o3\": 200_000,\n",
      "        \"gpt-4.1\": 1_000_000,\n",
      "        # Google models\n",
      "        \"gemini-2.0-flash-exp\": 1_048_576,\n",
      "        \"gemini-1.5-flash\": 1_048_576,\n",
      "        \"gemini-1.5-pro\": 2_097_152,\n",
      "        \"gemini-2.5-pro\": 1_048_576,\n",
      "        \"gemini-2.5-flash\": 1_048_576,\n",
      "    }\n",
      "\n",
      "    # Model to encoding mapping\n",
      "    MODEL_TO_ENCODING: Dict[str, str] = {\n",
      "        # GPT-4o models use o200k_base\n",
      "        \"gpt-4o\": \"o200k_base\",\n",
      "        \"gpt-4o-mini\": \"o200k_base\",\n",
      "        \"o4-mini\": \"o200k_base\",\n",
      "        \"o3\": \"o200k_base\",\n",
      "        \"gpt-4.1\": \"o200k_base\",\n",
      "        # Gemini models - we'll use cl100k_base as approximation\n",
      "        \"gemini-2.0-flash-exp\": \"cl100k_base\",\n",
      "        \"gemini-1.5-flash\": \"cl100k_base\",\n",
      "        \"gemini-1.5-pro\": \"cl100k_base\",\n",
      "        \"gemini-2.5-pro\": \"cl100k_base\",\n",
      "        \"gemini-2.5-flash\": \"cl100k_base\",\n",
      "    }\n",
      "\n",
      "    def __init__(self, config: Optional[Dict[str, Any]] = None):\n",
      "        \"\"\"\n",
      "        Initialize TokenCounter with encoding cache and optional configuration.\n",
      "\n",
      "        Args:\n",
      "            config: Optional configuration dictionary that can contain:\n",
      "                - model_limits: Override default model token limits\n",
      "                - model_encodings: Override default model to encoding mapping\n",
      "                - default_encoding: Default encoding to use (default: \"cl100k_base\")\n",
      "                - default_token_limit: Default token limit for unknown models (default: 8192)\n",
      "        \"\"\"\n",
      "        self._encoding_cache: Dict[str, tiktoken.Encoding] = {}\n",
      "        self.config = config or {}\n",
      "\n",
      "        # Initialize with config overrides if provided\n",
      "        if \"model_limits\" in self.config and isinstance(self.config[\"model_limits\"], dict):\n",
      "            # Update default limits with any overrides from config\n",
      "            self.MODEL_LIMITS = {**self.MODEL_LIMITS, **self.config[\"model_limits\"]}\n",
      "\n",
      "        if \"model_encodings\" in self.config and isinstance(self.config[\"model_encodings\"], dict):\n",
      "            # Update default encodings with any overrides from config\n",
      "            self.MODEL_TO_ENCODING = {**self.MODEL_TO_ENCODING, **self.config[\"model_encodings\"]}\n",
      "\n",
      "    def _get_encoding(self, model: str) -> tiktoken.Encoding:\n",
      "        \"\"\"Get the appropriate encoding for a model, with caching.\"\"\"\n",
      "        # Get encoding name from config overrides with flexible matching\n",
      "        config_encodings = self.config.get(\"model_encodings\", {})\n",
      "        config_key = self._find_matching_model_key(model, config_encodings)\n",
      "        encoding_name = config_encodings.get(config_key) if config_key else None\n",
      "\n",
      "        # If not found in config, try default mapping with flexible matching\n",
      "        if not encoding_name:\n",
      "            default_key = self._find_matching_model_key(model, self.MODEL_TO_ENCODING)\n",
      "            encoding_name = self.MODEL_TO_ENCODING.get(default_key) if default_key else None\n",
      "\n",
      "        if not encoding_name:\n",
      "            # Use default from config or fallback to cl100k_base\n",
      "            encoding_name = self.config.get(\"default_encoding\", \"cl100k_base\")\n",
      "\n",
      "        if encoding_name not in self._encoding_cache:\n",
      "            try:\n",
      "                self._encoding_cache[encoding_name] = tiktoken.get_encoding(encoding_name)\n",
      "            except Exception:\n",
      "                # Fallback to default encoding if specified encoding not found\n",
      "                default_encoding = self.config.get(\"default_encoding\", \"cl100k_base\")\n",
      "                self._encoding_cache[encoding_name] = tiktoken.get_encoding(default_encoding)\n",
      "\n",
      "        return self._encoding_cache[encoding_name]\n",
      "\n",
      "    def count_tokens(self, text: str, model: str) -> int:\n",
      "        \"\"\"\n",
      "        Count the number of tokens in the given text for the specified model.\n",
      "\n",
      "        Args:\n",
      "            text: The text to count tokens for\n",
      "            model: The model name to use for tokenization\n",
      "\n",
      "        Returns:\n",
      "            Number of tokens in the text\n",
      "        \"\"\"\n",
      "        if not text:\n",
      "            return 0\n",
      "\n",
      "        encoding = self._get_encoding(model)\n",
      "        return len(encoding.encode(text))\n",
      "\n",
      "    def _find_matching_model_key(self, model: str, model_dict: Dict[str, Any]) -> Optional[str]:\n",
      "        \"\"\"\n",
      "        Find a model key that matches the given model name.\n",
      "        First tries exact match, then looks for keys that are substrings of the model.\n",
      "\n",
      "        Args:\n",
      "            model: The model name to search for\n",
      "            model_dict: Dictionary of model configurations\n",
      "\n",
      "        Returns:\n",
      "            Matching key or None if no match found\n",
      "        \"\"\"\n",
      "        # First try exact match\n",
      "        if model in model_dict:\n",
      "            return model\n",
      "\n",
      "        # Then try substring matching - find keys that are substrings of the model\n",
      "        for key in model_dict.keys():\n",
      "            if key in model:\n",
      "                return key\n",
      "\n",
      "        return None\n",
      "\n",
      "    def get_model_limit(self, model: str) -> int:\n",
      "        \"\"\"\n",
      "        Get the token limit for the specified model.\n",
      "        Uses flexible matching to find model configurations.\n",
      "\n",
      "        Args:\n",
      "            model: The model name\n",
      "\n",
      "        Returns:\n",
      "            Token limit for the model, using config overrides or defaults\n",
      "        \"\"\"\n",
      "        # First check config overrides with flexible matching\n",
      "        config_limits = self.config.get(\"model_limits\", {})\n",
      "        config_key = self._find_matching_model_key(model, config_limits)\n",
      "        if config_key:\n",
      "            return config_limits[config_key]\n",
      "\n",
      "        # Then check default limits with flexible matching\n",
      "        default_key = self._find_matching_model_key(model, self.MODEL_LIMITS)\n",
      "        if default_key:\n",
      "            return self.MODEL_LIMITS[default_key]\n",
      "\n",
      "        # Fallback to configured default\n",
      "        return self.config.get(\"default_token_limit\", 128_000)\n",
      "\n",
      "    def estimate_response_tokens(self, prompt: str, model: str) -> int:\n",
      "        \"\"\"\n",
      "        Estimate the number of tokens that might be used in the response.\n",
      "\n",
      "        This is a heuristic that estimates response tokens as 20% of prompt tokens,\n",
      "        with a minimum of 500 tokens and maximum of 4096 tokens.\n",
      "\n",
      "        Args:\n",
      "            prompt: The prompt text\n",
      "            model: The model name\n",
      "\n",
      "        Returns:\n",
      "            Estimated response tokens\n",
      "        \"\"\"\n",
      "        prompt_tokens = self.count_tokens(prompt, model)\n",
      "        # Estimate response as 20% of prompt, with bounds\n",
      "        estimated = int(prompt_tokens * 0.2)\n",
      "        return max(500, min(estimated, 4096))\n",
      "\n",
      "    def can_fit_in_context(self, prompt: str, model: str, safety_margin: int = 1000) -> bool:\n",
      "        \"\"\"\n",
      "        Check if a prompt can fit within the model's context window.\n",
      "\n",
      "        Args:\n",
      "            prompt: The prompt text\n",
      "            model: The model name\n",
      "            safety_margin: Extra tokens to reserve for response and system prompts\n",
      "\n",
      "        Returns:\n",
      "            True if the prompt fits, False otherwise\n",
      "        \"\"\"\n",
      "        prompt_tokens = self.count_tokens(prompt, model)\n",
      "        response_tokens = self.estimate_response_tokens(prompt, model)\n",
      "        total_needed = prompt_tokens + response_tokens + safety_margin\n",
      "\n",
      "        return total_needed <= self.get_model_limit(model)\n",
      "\n",
      "    def remaining_tokens(self, prompt: str, model: str, safety_margin: int = 1000) -> int:\n",
      "        \"\"\"\n",
      "        Calculate how many tokens remain available in the context window.\n",
      "\n",
      "        Args:\n",
      "            prompt: The prompt text\n",
      "            model: The model name\n",
      "            safety_margin: Extra tokens to reserve\n",
      "\n",
      "        Returns:\n",
      "            Number of remaining tokens (can be negative if over limit)\n",
      "        \"\"\"\n",
      "        prompt_tokens = self.count_tokens(prompt, model)\n",
      "        response_tokens = self.estimate_response_tokens(prompt, model)\n",
      "        total_used = prompt_tokens + response_tokens + safety_margin\n",
      "\n",
      "        return self.get_model_limit(model) - total_used\n",
      "\n",
      "--- File: yellhorn_mcp/utils/cost_tracker_utils.py ---\n",
      "\"\"\"Cost tracking and metrics utilities for Yellhorn MCP.\n",
      "\n",
      "This module handles token usage tracking, cost calculation,\n",
      "and metrics formatting for various AI models.\n",
      "\"\"\"\n",
      "\n",
      "from yellhorn_mcp.llm_manager import UsageMetadata\n",
      "from yellhorn_mcp.models.metadata_models import CompletionMetadata\n",
      "\n",
      "# Pricing configuration for models (USD per 1M tokens)\n",
      "MODEL_PRICING = {\n",
      "    # Gemini models\n",
      "    \"gemini-2.5-pro\": {\n",
      "        \"input\": {\"default\": 1.25},\n",
      "        \"output\": {\"default\": 10.00},\n",
      "    },\n",
      "    \"gemini-2.5-flash\": {\n",
      "        \"input\": {\n",
      "            \"default\": 0.30,\n",
      "        },\n",
      "        \"output\": {\n",
      "            \"default\": 2.50,\n",
      "        },\n",
      "        \"cache\": {\n",
      "            \"default\": 0.075,\n",
      "            \"storage\": 1.00,  # per 1M tokens per hour\n",
      "        },\n",
      "    },\n",
      "    \"gemini-2.5-flash-lite\": {\n",
      "        \"input\": {\n",
      "            \"default\": 0.10,\n",
      "            \"audio\": 0.50,\n",
      "        },\n",
      "        \"output\": {\n",
      "            \"default\": 0.40,\n",
      "        },\n",
      "        \"cache\": {\n",
      "            \"default\": 0.025,\n",
      "            \"audio\": 0.125,\n",
      "            \"storage\": 1.00,  # per 1M tokens per hour\n",
      "        },\n",
      "    },\n",
      "    # OpenAI models\n",
      "    \"gpt-4.1\": {\n",
      "        \"input\": {\"default\": 2.00, \"cached\": 0.50},\n",
      "        \"output\": {\"default\": 8.00},\n",
      "    },\n",
      "    \"gpt-4o\": {\n",
      "        \"input\": {\"default\": 5.00},  # $5 per 1M input tokens\n",
      "        \"output\": {\"default\": 15.00},  # $15 per 1M output tokens\n",
      "    },\n",
      "    \"gpt-4o-mini\": {\n",
      "        \"input\": {\"default\": 0.15},  # $0.15 per 1M input tokens\n",
      "        \"output\": {\"default\": 0.60},  # $0.60 per 1M output tokens\n",
      "    },\n",
      "    \"o4-mini\": {\n",
      "        \"input\": {\"default\": 1.1},  # $1.1 per 1M input tokens\n",
      "        \"output\": {\"default\": 4.4},  # $4.4 per 1M output tokens\n",
      "    },\n",
      "    \"o3\": {\n",
      "        \"input\": {\"default\": 10.0},  # $10 per 1M input tokens\n",
      "        \"output\": {\"default\": 40.0},  # $40 per 1M output tokens\n",
      "    },\n",
      "    # Deep Research Models\n",
      "    \"o3-deep-research\": {\n",
      "        \"input\": {\"default\": 10.00},\n",
      "        \"output\": {\"default\": 40.00},\n",
      "    },\n",
      "    \"o4-mini-deep-research\": {\n",
      "        \"input\": {\"default\": 1.10},  # Same as o4-mini\n",
      "        \"output\": {\"default\": 4.40},  # Same as o4-mini\n",
      "    },\n",
      "}\n",
      "\n",
      "\n",
      "def calculate_cost(model: str, input_tokens: int, output_tokens: int) -> float | None:\n",
      "    \"\"\"Calculates the estimated cost for a model API call.\n",
      "\n",
      "    Args:\n",
      "        model: The model name (Gemini or OpenAI).\n",
      "        input_tokens: Number of input tokens used.\n",
      "        output_tokens: Number of output tokens generated.\n",
      "\n",
      "    Returns:\n",
      "        The estimated cost in USD, or None if pricing is unavailable for the model.\n",
      "    \"\"\"\n",
      "    pricing = MODEL_PRICING.get(model)\n",
      "    if not pricing:\n",
      "        return None\n",
      "\n",
      "    # Calculate costs (convert to millions for rate multiplication)\n",
      "    input_rate = pricing[\"input\"][\"default\"]\n",
      "    output_rate = pricing[\"output\"][\"default\"]\n",
      "    input_cost = (input_tokens / 1_000_000) * input_rate\n",
      "    output_cost = (output_tokens / 1_000_000) * output_rate\n",
      "    return input_cost + output_cost\n",
      "\n",
      "\n",
      "def format_metrics_section(model: str, usage: UsageMetadata | None) -> str:\n",
      "    \"\"\"Formats the completion metrics into a Markdown section.\n",
      "\n",
      "    Args:\n",
      "        model: The model name used for generation.\n",
      "        usage: CompletionMetadata object containing token usage information.\n",
      "\n",
      "    Returns:\n",
      "        Formatted Markdown section with completion metrics.\n",
      "    \"\"\"\n",
      "    na_metrics = \"\\n\\n---\\n## Completion Metrics\\n* **Model Used**: N/A\\n* **Input Tokens**: N/A\\n* **Output Tokens**: N/A\\n* **Total Tokens**: N/A\\n* **Estimated Cost**: N/A\"\n",
      "\n",
      "    if usage is None:\n",
      "        return na_metrics\n",
      "\n",
      "    # Extract token counts\n",
      "    input_tokens = usage.prompt_tokens\n",
      "    output_tokens = usage.completion_tokens\n",
      "    total_tokens = usage.total_tokens\n",
      "\n",
      "    if input_tokens is None or output_tokens is None:\n",
      "        return na_metrics\n",
      "\n",
      "    # Calculate cost\n",
      "    cost = calculate_cost(model, input_tokens, output_tokens)\n",
      "    cost_str = f\"${cost:.4f}\" if cost is not None else \"N/A\"\n",
      "\n",
      "    # If total_tokens is None, calculate it\n",
      "    if total_tokens is None:\n",
      "        total_tokens = input_tokens + output_tokens\n",
      "\n",
      "    return f\"\"\"\\n\\n---\\n## Completion Metrics\n",
      "*   **Model Used**: `{model}`\n",
      "*   **Input Tokens**: {input_tokens}\n",
      "*   **Output Tokens**: {output_tokens}\n",
      "*   **Total Tokens**: {total_tokens}\n",
      "*   **Estimated Cost**: {cost_str}\"\"\"\n",
      "\n",
      "--- File: yellhorn_mcp/utils/lsp_utils.py ---\n",
      "\"\"\"\n",
      "LSP-style utilities for extracting function signatures and docstrings.\n",
      "\n",
      "This module provides functions to extract Python function signatures, class signatures,\n",
      "class attributes, and docstrings using AST parsing (with fallback to jedi) for use in\n",
      "the \"lsp\" codebase reasoning mode. This mode gathers function/method signatures, class\n",
      "attributes, Go struct fields, and docstrings for supported languages (Python, Go), plus\n",
      "the full contents of files that appear in diffs, to create a more lightweight but still\n",
      "useful codebase snapshot for AI processing.\n",
      "\"\"\"\n",
      "\n",
      "import ast\n",
      "import json\n",
      "import re\n",
      "import shutil\n",
      "import subprocess\n",
      "from pathlib import Path\n",
      "from yellhorn_mcp.utils.git_utils import run_git_command\n",
      "\n",
      "def _class_attributes_from_ast(node: ast.ClassDef) -> list[str]:\n",
      "    \"\"\"\n",
      "    Extract class attributes from an AST ClassDef node.\n",
      "\n",
      "    Handles regular assignments, type annotations, dataclass fields,\n",
      "    Pydantic model fields, and Enum literals. Skips private attributes\n",
      "    (starting with \"_\").\n",
      "\n",
      "    Args:\n",
      "        node: AST ClassDef node to extract attributes from\n",
      "\n",
      "    Returns:\n",
      "        List of attribute strings with type annotations when available\n",
      "    \"\"\"\n",
      "    attrs = []\n",
      "    for stmt in node.body:\n",
      "        # AnnAssign  => typed attribute  e.g. age: int = 0\n",
      "        if isinstance(stmt, ast.AnnAssign) and isinstance(stmt.target, ast.Name):\n",
      "            name = stmt.target.id\n",
      "            if not name.startswith(\"_\"):\n",
      "                annotation = getattr(stmt.annotation, \"id\", \"Any\")\n",
      "                attrs.append(f\"{name}: {annotation}\")\n",
      "        # Assign      => untyped attr e.g. name = \"foo\"\n",
      "        elif isinstance(stmt, ast.Assign):\n",
      "            if len(stmt.targets) == 1 and isinstance(stmt.targets[0], ast.Name):\n",
      "                name = stmt.targets[0].id\n",
      "                if not name.startswith(\"_\"):\n",
      "                    attrs.append(name)\n",
      "\n",
      "    # Handle Enum literals for classes that inherit from Enum\n",
      "    if node.bases and any(\n",
      "        isinstance(base, ast.Name)\n",
      "        and base.id == \"Enum\"\n",
      "        or isinstance(base, ast.Attribute)\n",
      "        and base.attr == \"Enum\"\n",
      "        for base in node.bases\n",
      "    ):\n",
      "        for stmt in node.body:\n",
      "            # Enum constants are typically defined as CLASS_NAME = value\n",
      "            if (\n",
      "                isinstance(stmt, ast.Assign)\n",
      "                and len(stmt.targets) == 1\n",
      "                and isinstance(stmt.targets[0], ast.Name)\n",
      "            ):\n",
      "                name = stmt.targets[0].id\n",
      "                if not name.startswith(\"_\") and name.isupper():  # Most enum values are UPPERCASE\n",
      "                    attrs.append(f\"{name}\")\n",
      "\n",
      "    return attrs\n",
      "\n",
      "\n",
      "def _sig_from_ast(node: ast.AST) -> str | None:\n",
      "    \"\"\"\n",
      "    Extract a function or class signature from an AST node.\n",
      "\n",
      "    Args:\n",
      "        node: AST node to extract signature from\n",
      "\n",
      "    Returns:\n",
      "        String representation of the signature or None if not a function/class\n",
      "    \"\"\"\n",
      "    if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
      "        # Handle function arguments\n",
      "        args = []\n",
      "\n",
      "        # Add regular args with type annotations if available\n",
      "        for arg in node.args.args:\n",
      "            arg_name = arg.arg\n",
      "            # Get annotation if present\n",
      "            annotation = getattr(arg, \"annotation\", None)\n",
      "            if annotation:\n",
      "                try:\n",
      "                    # Python 3.9+ supports ast.unparse\n",
      "                    if hasattr(ast, \"unparse\"):\n",
      "                        annotation_str = ast.unparse(annotation)\n",
      "                    else:\n",
      "                        # Fallback for older Python versions\n",
      "                        annotation_str = getattr(annotation, \"id\", \"Any\")\n",
      "                    arg_name = f\"{arg.arg}: {annotation_str}\"\n",
      "                except Exception:\n",
      "                    # If we can't unparse, just use the name\n",
      "                    pass\n",
      "            args.append(arg_name)\n",
      "\n",
      "        # Add *args if present\n",
      "        if node.args.vararg:\n",
      "            vararg_name = node.args.vararg.arg\n",
      "            # Add type annotation for *args if present\n",
      "            annotation = getattr(node.args.vararg, \"annotation\", None)\n",
      "            if annotation:\n",
      "                try:\n",
      "                    if hasattr(ast, \"unparse\"):\n",
      "                        annotation_str = ast.unparse(annotation)\n",
      "                        vararg_name = f\"{vararg_name}: {annotation_str}\"\n",
      "                except Exception:\n",
      "                    pass\n",
      "            args.append(f\"*{vararg_name}\")\n",
      "\n",
      "        # Add keyword-only args with type annotations if available\n",
      "        if node.args.kwonlyargs:\n",
      "            if not node.args.vararg:\n",
      "                args.append(\"*\")\n",
      "            for kwarg in node.args.kwonlyargs:\n",
      "                kwarg_name = kwarg.arg\n",
      "                # Get annotation if present\n",
      "                annotation = getattr(kwarg, \"annotation\", None)\n",
      "                if annotation:\n",
      "                    try:\n",
      "                        if hasattr(ast, \"unparse\"):\n",
      "                            annotation_str = ast.unparse(annotation)\n",
      "                            kwarg_name = f\"{kwarg.arg}: {annotation_str}\"\n",
      "                    except Exception:\n",
      "                        pass\n",
      "                args.append(kwarg_name)\n",
      "\n",
      "        # Add **kwargs if present\n",
      "        if node.args.kwarg:\n",
      "            kwargs_name = node.args.kwarg.arg\n",
      "            # Add type annotation for **kwargs if present\n",
      "            annotation = getattr(node.args.kwarg, \"annotation\", None)\n",
      "            if annotation:\n",
      "                try:\n",
      "                    if hasattr(ast, \"unparse\"):\n",
      "                        annotation_str = ast.unparse(annotation)\n",
      "                        kwargs_name = f\"{kwargs_name}: {annotation_str}\"\n",
      "                except Exception:\n",
      "                    pass\n",
      "            args.append(f\"**{kwargs_name}\")\n",
      "\n",
      "        # Format as regular or async function\n",
      "        prefix = \"async def\" if isinstance(node, ast.AsyncFunctionDef) else \"def\"\n",
      "        sig = f\"{prefix} {node.name}({', '.join(args)})\"\n",
      "\n",
      "        # Add return type if available\n",
      "        returns = getattr(node, \"returns\", None)\n",
      "        if returns:\n",
      "            try:\n",
      "                if hasattr(ast, \"unparse\"):\n",
      "                    return_type = ast.unparse(returns)\n",
      "                    sig = f\"{sig} -> {return_type}\"\n",
      "            except Exception:\n",
      "                pass\n",
      "\n",
      "        return sig\n",
      "\n",
      "    elif isinstance(node, ast.ClassDef):\n",
      "        # Get base classes if any\n",
      "        bases = []\n",
      "        for base in node.bases:\n",
      "            if isinstance(base, ast.Name):\n",
      "                bases.append(base.id)\n",
      "            elif isinstance(base, ast.Attribute):\n",
      "                bases.append(\n",
      "                    f\"{base.value.id}.{base.attr}\" if isinstance(base.value, ast.Name) else \"...\"\n",
      "                )\n",
      "\n",
      "        if bases:\n",
      "            return f\"class {node.name}({', '.join(bases)})\"\n",
      "        return f\"class {node.name}\"\n",
      "\n",
      "    return None\n",
      "\n",
      "\n",
      "def extract_python_api(file_path: Path) -> list[str]:\n",
      "    \"\"\"\n",
      "    Extract Python API (function and class signatures with docstrings) from a file.\n",
      "\n",
      "    Uses AST parsing for speed, with fallback to jedi if AST parsing fails.\n",
      "    Only includes non-private, non-dunder methods and functions.\n",
      "\n",
      "    Args:\n",
      "        file_path: Path to the Python file\n",
      "\n",
      "    Returns:\n",
      "        List of signature strings with first line of docstring\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # Try AST parsing first (faster)\n",
      "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
      "            source = f.read()\n",
      "\n",
      "        tree = ast.parse(source)\n",
      "        sigs: list[str] = []\n",
      "\n",
      "        # Process module-level definitions\n",
      "        for node in tree.body:\n",
      "            # Skip private members\n",
      "            if hasattr(node, \"name\") and (node.name.startswith(\"_\") or node.name.startswith(\"__\")):\n",
      "                continue\n",
      "\n",
      "            sig = _sig_from_ast(node)\n",
      "            if sig:\n",
      "                # Add first line of docstring if available\n",
      "                doc = ast.get_docstring(node)\n",
      "                doc_summary = f\"  # {doc.splitlines()[0]}\" if doc else \"\"\n",
      "                sigs.append(f\"{sig}{doc_summary}\")\n",
      "\n",
      "                # For classes, also process methods and attributes\n",
      "                if isinstance(node, ast.ClassDef):\n",
      "                    # Extract class attributes\n",
      "                    for attr in _class_attributes_from_ast(node):\n",
      "                        sigs.append(f\"    {attr}\")\n",
      "\n",
      "                    # Process methods\n",
      "                    for method in node.body:\n",
      "                        # Skip private methods\n",
      "                        if hasattr(method, \"name\") and (\n",
      "                            method.name.startswith(\"_\") or method.name.startswith(\"__\")\n",
      "                        ):\n",
      "                            continue\n",
      "\n",
      "                        method_sig = _sig_from_ast(method)\n",
      "                        if method_sig:\n",
      "                            # Add class prefix to method signature\n",
      "                            method_sig = method_sig.replace(\"def \", f\"def {node.name}.\")\n",
      "                            # Add first line of docstring if available\n",
      "                            method_doc = ast.get_docstring(method)\n",
      "                            method_doc_summary = (\n",
      "                                f\"  # {method_doc.splitlines()[0]}\" if method_doc else \"\"\n",
      "                            )\n",
      "                            sigs.append(f\"    {method_sig}{method_doc_summary}\")\n",
      "\n",
      "        return sigs\n",
      "\n",
      "    except SyntaxError:\n",
      "        # Fall back to jedi for more complex cases\n",
      "        try:\n",
      "            # Try dynamic import to handle cases where jedi is not installed\n",
      "            import importlib\n",
      "\n",
      "            jedi = importlib.import_module(\"jedi\")\n",
      "\n",
      "            script = jedi.Script(path=str(file_path))\n",
      "            signatures = []\n",
      "\n",
      "            # Get all functions and classes\n",
      "            for name in script.get_names():\n",
      "                # Skip private members\n",
      "                if name.name.startswith(\"_\") or name.name.startswith(\"__\"):\n",
      "                    continue\n",
      "\n",
      "                if name.type in (\"function\", \"class\"):\n",
      "                    sig = str(name.get_signatures()[0] if name.get_signatures() else name)\n",
      "                    doc = name.docstring()\n",
      "                    doc_summary = f\"  # {doc.splitlines()[0]}\" if doc and doc.strip() else \"\"\n",
      "                    signatures.append(f\"{sig}{doc_summary}\")\n",
      "\n",
      "            return signatures\n",
      "\n",
      "        except (ImportError, ModuleNotFoundError, Exception) as e:\n",
      "            # If jedi is not available or fails, return an empty list\n",
      "            return []\n",
      "\n",
      "\n",
      "def extract_go_api(file_path: Path) -> list[str]:\n",
      "    \"\"\"\n",
      "    Extract Go API (function, type, interface signatures, struct fields) from a file.\n",
      "\n",
      "    Uses regex-based parsing for basic extraction, with fallback to gopls\n",
      "    when available for higher fidelity.\n",
      "\n",
      "    Args:\n",
      "        file_path: Path to the Go file\n",
      "\n",
      "    Returns:\n",
      "        List of Go API signature strings\n",
      "    \"\"\"\n",
      "    # Check for gopls first - it provides the best extraction\n",
      "    if shutil.which(\"gopls\"):\n",
      "        try:\n",
      "            # Run gopls to get symbols in JSON format\n",
      "            process = subprocess.run(\n",
      "                [\"gopls\", \"symbols\", \"-format\", \"json\", str(file_path)],\n",
      "                capture_output=True,\n",
      "                text=True,\n",
      "                check=False,\n",
      "                timeout=2.0,  # Reasonable timeout for gopls\n",
      "            )\n",
      "\n",
      "            if process.returncode == 0 and process.stdout:\n",
      "                # Parse JSON output\n",
      "                symbols = json.loads(process.stdout)\n",
      "                sigs = []\n",
      "\n",
      "                for symbol in symbols:\n",
      "                    # Filter for exported symbols only (uppercase first letter)\n",
      "                    name = symbol.get(\"name\", \"\")\n",
      "                    kind = symbol.get(\"kind\", \"\")\n",
      "\n",
      "                    if name and name[0].isupper():\n",
      "                        if kind in [\"function\", \"method\", \"interface\", \"type\"]:\n",
      "                            sigs.append(f\"{kind} {name}\")\n",
      "                        elif kind == \"struct\":\n",
      "                            # For structs, check for fields\n",
      "                            children = symbol.get(\"children\", [])\n",
      "                            if children:\n",
      "                                # Extract field names from children where kind is \"field\"\n",
      "                                fields = []\n",
      "                                for child in children:\n",
      "                                    if child.get(\"kind\") == \"field\":\n",
      "                                        child_name = child.get(\"name\", \"\")\n",
      "                                        child_detail = child.get(\"detail\", \"\")\n",
      "                                        fields.append(f\"{child_name} {child_detail}\")\n",
      "\n",
      "                                # Add struct with fields\n",
      "                                if fields:\n",
      "                                    fields_str = \"; \".join(fields)\n",
      "                                    sigs.append(f\"struct {name} {{ {fields_str} }}\")\n",
      "                                else:\n",
      "                                    sigs.append(f\"struct {name}\")\n",
      "                            else:\n",
      "                                sigs.append(f\"struct {name}\")\n",
      "\n",
      "                return sorted(sigs)\n",
      "        except (subprocess.SubprocessError, json.JSONDecodeError, Exception):\n",
      "            # Fall back to regex if gopls fails\n",
      "            pass\n",
      "\n",
      "    # Regex-based extraction as fallback\n",
      "    try:\n",
      "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
      "            content = f.read()\n",
      "\n",
      "        # Enhanced regex for functions to capture parameters and return types\n",
      "        FUNC_SIG_RE = re.compile(r\"^func\\s+([A-Z]\\w*)\\s*\\(([^)]*)\\)\\s*([^{\\n]*)\", re.MULTILINE)\n",
      "\n",
      "        # Enhanced regex for receiver methods to handle pointers and generics\n",
      "        # This matches patterns like:\n",
      "        # - func (o *Oven) Heat(...)\n",
      "        # - func (p Pizza) Method(...)\n",
      "        # - func (s *Server[T]) Method(...)\n",
      "        RECEIVER_METHOD_RE = re.compile(\n",
      "            r\"^func\\s+\\(([^)]*)\\)\\s+([A-Z]\\w*)\\s*(?:\\[([^\\]]*)\\])?\\s*\\(([^)]*)\\)\\s*([^{\\n]*)\",\n",
      "            re.MULTILINE,\n",
      "        )\n",
      "\n",
      "        # Find exported type definitions (interfaces, structs)\n",
      "        TYPE_RE = re.compile(r\"^type\\s+([A-Z]\\w*)\\s+([^\\s{]+)\", re.MULTILINE)\n",
      "\n",
      "        # Find interface definitions\n",
      "        INTERFACE_RE = re.compile(r\"type\\s+([A-Z]\\w*)\\s+interface\", re.MULTILINE)\n",
      "\n",
      "        # Find struct definitions with their fields\n",
      "        STRUCT_RE = re.compile(\n",
      "            r\"type\\s+([A-Z]\\w*)\\s+struct\\s*\\{([^}]*)\\}\", re.MULTILINE | re.DOTALL\n",
      "        )\n",
      "\n",
      "        sigs = []\n",
      "\n",
      "        # Extract functions with parameters and return types\n",
      "        func_matches = FUNC_SIG_RE.findall(content)\n",
      "        for name, params, returns in func_matches:\n",
      "            # Clean up parameters and returns\n",
      "            params = params.strip()\n",
      "            returns = returns.strip()\n",
      "\n",
      "            if returns:\n",
      "                sigs.append(f\"func {name}({params}) {returns}\")\n",
      "            else:\n",
      "                sigs.append(f\"func {name}({params})\")\n",
      "\n",
      "        # Extract receiver methods (including pointers and generics)\n",
      "        receiver_matches = RECEIVER_METHOD_RE.findall(content)\n",
      "        for receiver, method_name, generics, params, returns in receiver_matches:\n",
      "            # Clean up components\n",
      "            receiver = receiver.strip()\n",
      "            params = params.strip()\n",
      "            returns = returns.strip()\n",
      "\n",
      "            # Format signature with generics if present\n",
      "            if generics:\n",
      "                method_sig = f\"func ({receiver}) {method_name}[{generics}]({params})\"\n",
      "            else:\n",
      "                method_sig = f\"func ({receiver}) {method_name}({params})\"\n",
      "\n",
      "            # Add return type if present\n",
      "            if returns:\n",
      "                method_sig = f\"{method_sig} {returns}\"\n",
      "\n",
      "            sigs.append(method_sig)\n",
      "\n",
      "        # Extract types that aren't structs or interfaces\n",
      "        type_matches = TYPE_RE.findall(content)\n",
      "        for name, type_def in type_matches:\n",
      "            if type_def != \"struct\" and type_def != \"interface\":\n",
      "                sigs.append(f\"type {name} {type_def}\")\n",
      "\n",
      "        # Extract interfaces\n",
      "        interface_matches = INTERFACE_RE.findall(content)\n",
      "        for name in interface_matches:\n",
      "            sigs.append(f\"type {name} interface\")\n",
      "\n",
      "        # Extract structs and their fields\n",
      "        struct_matches = STRUCT_RE.findall(content)\n",
      "        for name, fields in struct_matches:\n",
      "            # Clean up fields: remove comments, strip whitespace, join lines\n",
      "            # Replace newlines and extra spaces with a single space\n",
      "            cleaned_fields = re.sub(r\"\\s+\", \" \", fields).strip()\n",
      "            # Remove comments (// and /* ... */)\n",
      "            cleaned_fields = re.sub(r\"//.*\", \"\", cleaned_fields)\n",
      "            cleaned_fields = re.sub(r\"/\\*.*?\\*/\", \"\", cleaned_fields, flags=re.DOTALL)\n",
      "\n",
      "            # Format fields for output, limiting length\n",
      "            if cleaned_fields:\n",
      "                # Truncate if too long\n",
      "                max_length = 120\n",
      "                if len(cleaned_fields) > max_length:\n",
      "                    cleaned_fields = cleaned_fields[: max_length - 3] + \"...\"\n",
      "\n",
      "                sigs.append(f\"struct {name} {{ {cleaned_fields} }}\")\n",
      "            else:\n",
      "                sigs.append(f\"struct {name}\")\n",
      "\n",
      "        return sorted(sigs)\n",
      "    except Exception:\n",
      "        return []\n",
      "\n",
      "\n",
      "def _fence(lang: str, text: str) -> str:\n",
      "    \"\"\"\n",
      "    Add code fences around text with specified language.\n",
      "\n",
      "    Args:\n",
      "        lang: The language for syntax highlighting.\n",
      "        text: The text content to fence.\n",
      "\n",
      "    Returns:\n",
      "        Text wrapped in code fences with language specified.\n",
      "    \"\"\"\n",
      "    return f\"```{lang}\\n{text}\\n```\"\n",
      "\n",
      "\n",
      "async def get_lsp_snapshot(repo_path: Path, file_paths: list[str]) -> tuple[list[str], dict[str, str]]:\n",
      "    \"\"\"\n",
      "    Get an LSP-style snapshot of the codebase, extracting API information.\n",
      "\n",
      "    Extracts function signatures, class signatures, class attributes, struct fields,\n",
      "    and docstrings to create a lightweight representation of the codebase structure.\n",
      "    Respects both .gitignore and .yellhornignore files, just like the full snapshot function.\n",
      "    Supports Python and Go files for API extraction.\n",
      "\n",
      "    Args:\n",
      "        repo_path: Path to the repository\n",
      "\n",
      "    Returns:\n",
      "        Tuple of (file list, file contents dictionary), where contents contain\n",
      "        API signatures, class attributes, and docstrings as plain text (no code fences)\n",
      "    \"\"\"\n",
      "    # Filter for supported files\n",
      "    py_files = [p for p in file_paths if p.endswith(\".py\")]\n",
      "    go_files = [p for p in file_paths if p.endswith(\".go\")]\n",
      "\n",
      "    # Extract signatures from each file\n",
      "    contents = {}\n",
      "\n",
      "    # Process Python files\n",
      "    for file_path in py_files:\n",
      "        full_path = repo_path / file_path\n",
      "        if not full_path.is_file():\n",
      "            continue\n",
      "\n",
      "        sigs = extract_python_api(full_path)\n",
      "        if sigs:\n",
      "            contents[file_path] = \"\\n\".join(sigs)\n",
      "\n",
      "    # Process Go files\n",
      "    for file_path in go_files:\n",
      "        full_path = repo_path / file_path\n",
      "        if not full_path.is_file():\n",
      "            continue\n",
      "\n",
      "        sigs = extract_go_api(full_path)\n",
      "        if sigs:\n",
      "            contents[file_path] = \"\\n\".join(sigs)\n",
      "\n",
      "    return file_paths, contents\n",
      "\n",
      "\n",
      "async def get_lsp_diff(\n",
      "    repo_path: Path, base_ref: str, head_ref: str, changed_files: list[str], git_command_func=None\n",
      ") -> str:\n",
      "    \"\"\"\n",
      "    Create a lightweight LSP-focused diff between two git refs.\n",
      "\n",
      "    This function generates a diff that focuses on API changes (function signatures,\n",
      "    class signatures, attributes, etc.) between the base and head refs for the specified\n",
      "    changed files. It extracts API information from both versions of each file and\n",
      "    compares them to create a concise diff that highlights structural changes.\n",
      "\n",
      "    Args:\n",
      "        repo_path: Path to the repository\n",
      "        base_ref: Base Git ref (commit SHA, branch name, tag) for comparison\n",
      "        head_ref: Head Git ref (commit SHA, branch name, tag) for comparison\n",
      "        changed_files: List of file paths that were changed between refs\n",
      "        git_command_func: Optional Git command function (for mocking)\n",
      "\n",
      "    Returns:\n",
      "        A formatted string containing the LSP-style diff focusing on API changes\n",
      "    \"\"\"\n",
      "\n",
      "    # Initialize result\n",
      "    diff_parts = []\n",
      "    diff_parts.append(f\"# API Changes Between {base_ref} and {head_ref}\")\n",
      "    diff_parts.append(f\"Files changed: {len(changed_files)}\")\n",
      "\n",
      "    # Process each changed file to extract API differences\n",
      "    for file_path in changed_files:\n",
      "        # Skip files we don't support (focus on Python and Go)\n",
      "        if not (file_path.endswith(\".py\") or file_path.endswith(\".go\")):\n",
      "            continue\n",
      "\n",
      "        # Get the file content from both refs\n",
      "        try:\n",
      "            # Get base version content if file existed in base_ref\n",
      "            try:\n",
      "                base_content = await run_git_command(repo_path, [\"show\", f\"{base_ref}:{file_path}\"], git_command_func)\n",
      "            except Exception:\n",
      "                # File didn't exist in base_ref\n",
      "                base_content = \"\"\n",
      "\n",
      "            # Get head version content\n",
      "            try:\n",
      "                head_content = await run_git_command(repo_path, [\"show\", f\"{head_ref}:{file_path}\"], git_command_func)\n",
      "            except Exception:\n",
      "                # File was deleted in head_ref\n",
      "                head_content = \"\"\n",
      "\n",
      "            # If the file was added or deleted, note that in the diff\n",
      "            if not base_content and head_content:\n",
      "                diff_parts.append(f\"\\n## {file_path} (Added)\")\n",
      "            elif base_content and not head_content:\n",
      "                diff_parts.append(f\"\\n## {file_path} (Deleted)\")\n",
      "            else:\n",
      "                diff_parts.append(f\"\\n## {file_path} (Modified)\")\n",
      "\n",
      "            # Extract API information from both versions\n",
      "            base_api = []\n",
      "            head_api = []\n",
      "\n",
      "            # Extract base API\n",
      "            if base_content:\n",
      "                # Create a temporary file for the base content\n",
      "                import tempfile\n",
      "\n",
      "                with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".tmp\", delete=False) as temp:\n",
      "                    temp.write(base_content)\n",
      "                    temp_path = Path(temp.name)\n",
      "\n",
      "                try:\n",
      "                    if file_path.endswith(\".py\"):\n",
      "                        base_api = extract_python_api(temp_path)\n",
      "                    elif file_path.endswith(\".go\"):\n",
      "                        base_api = extract_go_api(temp_path)\n",
      "                finally:\n",
      "                    # Clean up the temporary file\n",
      "                    if temp_path.exists():\n",
      "                        temp_path.unlink()\n",
      "\n",
      "            # Extract head API\n",
      "            if head_content:\n",
      "                # Create a temporary file for the head content\n",
      "                import tempfile\n",
      "\n",
      "                with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".tmp\", delete=False) as temp:\n",
      "                    temp.write(head_content)\n",
      "                    temp_path = Path(temp.name)\n",
      "\n",
      "                try:\n",
      "                    if file_path.endswith(\".py\"):\n",
      "                        head_api = extract_python_api(temp_path)\n",
      "                    elif file_path.endswith(\".go\"):\n",
      "                        head_api = extract_go_api(temp_path)\n",
      "                finally:\n",
      "                    # Clean up the temporary file\n",
      "                    if temp_path.exists():\n",
      "                        temp_path.unlink()\n",
      "\n",
      "            # Compare and show differences\n",
      "            base_api_set = set(base_api)\n",
      "            head_api_set = set(head_api)\n",
      "\n",
      "            # Find additions and deletions\n",
      "            added = head_api_set - base_api_set\n",
      "            removed = base_api_set - head_api_set\n",
      "\n",
      "            # Add to diff if there are changes\n",
      "            if added or removed:\n",
      "                # Add removals first with - prefix\n",
      "                if removed:\n",
      "                    diff_parts.append(\"\\nRemoved:\")\n",
      "                    for item in sorted(removed):\n",
      "                        diff_parts.append(f\"- {item}\")\n",
      "\n",
      "                # Add additions with + prefix\n",
      "                if added:\n",
      "                    diff_parts.append(\"\\nAdded:\")\n",
      "                    for item in sorted(added):\n",
      "                        diff_parts.append(f\"+ {item}\")\n",
      "            else:\n",
      "                # No API changes detected\n",
      "                diff_parts.append(\n",
      "                    \"\\nNo structural API changes detected (implementation details may have changed)\"\n",
      "                )\n",
      "\n",
      "        except Exception as e:\n",
      "            diff_parts.append(f\"\\nError processing {file_path}: {str(e)}\")\n",
      "\n",
      "    # If no supported files were changed, add a note\n",
      "    if len(diff_parts) <= 2:  # Only header and file count\n",
      "        diff_parts.append(\"\\nNo API changes detected in supported files (Python, Go)\")\n",
      "\n",
      "    # Add footer with a note about what's included\n",
      "    diff_parts.append(\"\\n---\")\n",
      "    diff_parts.append(\n",
      "        \"Note: This diff focuses on API changes (functions, classes, methods, signatures) and may not show implementation details.\"\n",
      "    )\n",
      "\n",
      "    return \"\\n\".join(diff_parts)\n",
      "\n",
      "\n",
      "async def update_snapshot_with_full_diff_files(\n",
      "    repo_path: Path,\n",
      "    base_ref: str,\n",
      "    head_ref: str,\n",
      "    file_paths: list[str],\n",
      "    file_contents: dict[str, str],\n",
      "    git_command_func=None,\n",
      ") -> tuple[list[str], dict[str, str]]:\n",
      "    \"\"\"\n",
      "    Update an LSP snapshot with full contents of files included in a diff.\n",
      "\n",
      "    This ensures that any files modified in a diff are included in full in the snapshot,\n",
      "    even when using the 'lsp' mode which normally only includes signatures.\n",
      "\n",
      "    Args:\n",
      "        repo_path: Path to the repository\n",
      "        base_ref: Base Git ref for the diff\n",
      "        head_ref: Head Git ref for the diff\n",
      "        file_paths: List of all file paths in the snapshot\n",
      "        file_contents: Dictionary of file contents from the LSP snapshot\n",
      "        git_command_func: Optional Git command function (for mocking)\n",
      "\n",
      "    Returns:\n",
      "        Updated tuple of (file paths, file contents)\n",
      "    \"\"\"\n",
      "\n",
      "    try:\n",
      "        # Get the diff to identify affected files\n",
      "        diff_output = await run_git_command(repo_path, [\"diff\", f\"{base_ref}..{head_ref}\"], git_command_func)\n",
      "\n",
      "        # Extract file paths from the diff\n",
      "        affected_files = set()\n",
      "        for line in diff_output.splitlines():\n",
      "            if line.startswith(\"+++ b/\") or line.startswith(\"--- a/\"):\n",
      "                # Extract the file path, which is after \"--- a/\" or \"+++ b/\"\n",
      "                file_path = line[6:]\n",
      "                if file_path not in (\"/dev/null\", \"/dev/null\"):\n",
      "                    affected_files.add(file_path)\n",
      "\n",
      "        # Read the full content of affected files and add/replace in the snapshot\n",
      "        for file_path in affected_files:\n",
      "            if file_path not in file_paths:\n",
      "                continue  # Skip if file isn't in our snapshot (e.g., ignored files)\n",
      "\n",
      "            full_path = repo_path / file_path\n",
      "            if not full_path.is_file():\n",
      "                continue\n",
      "\n",
      "            try:\n",
      "                with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
      "                    content = f.read()\n",
      "\n",
      "                # Add or replace with raw content (no fences) - fences will be added by format_codebase_for_prompt\n",
      "                file_contents[file_path] = content\n",
      "            except UnicodeDecodeError:\n",
      "                # Skip binary files\n",
      "                continue\n",
      "            except Exception:\n",
      "                # Skip files we can't read\n",
      "                continue\n",
      "\n",
      "    except Exception:\n",
      "        # In case of any git diff error, just return the original snapshot\n",
      "        pass\n",
      "\n",
      "    return file_paths, file_contents\n",
      "\n",
      "--- File: yellhorn_mcp/utils/search_grounding_utils.py ---\n",
      "\"\"\"\n",
      "Search grounding utilities for Yellhorn MCP.\n",
      "\n",
      "This module provides helpers for configuring Google Search tools for Gemini models\n",
      "and formatting grounding metadata into Markdown citations.\n",
      "\"\"\"\n",
      "\n",
      "from google.genai import types as genai_types\n",
      "from google.genai.types import GroundingMetadata\n",
      "\n",
      "\n",
      "def _get_gemini_search_tools(model_name: str) -> genai_types.ToolListUnion | None:\n",
      "    \"\"\"\n",
      "    Determines and returns the appropriate Google Search tool configuration\n",
      "    based on the Gemini model name/version.\n",
      "\n",
      "    Args:\n",
      "        model_name: The name/version of the Gemini model.\n",
      "\n",
      "    Returns:\n",
      "        List of configured search tools or None if model doesn't support search.\n",
      "    \"\"\"\n",
      "    if not model_name.startswith(\"gemini-\"):\n",
      "        return None\n",
      "\n",
      "    try:\n",
      "        # Gemini 1.5 models use GoogleSearchRetrieval\n",
      "        if \"1.5\" in model_name:\n",
      "            return [genai_types.Tool(google_search_retrieval=genai_types.GoogleSearchRetrieval())]\n",
      "        # Gemini 2.0+ models use GoogleSearch\n",
      "        else:\n",
      "            return [genai_types.Tool(google_search=genai_types.GoogleSearch())]\n",
      "    except Exception:\n",
      "        # If tool creation fails, return None\n",
      "        return None\n",
      "\n",
      "\n",
      "def add_citations(response: genai_types.GenerateContentResponse) -> str:\n",
      "    \"\"\"\n",
      "    Inserts citation links into the response text based on grounding metadata.\n",
      "    Args:\n",
      "        response: The response object from the Gemini API.\n",
      "    Returns:\n",
      "        The response text with citations inserted.\n",
      "    \"\"\"\n",
      "    text = response.text\n",
      "    supports = (\n",
      "        response.candidates[0].grounding_metadata.grounding_supports\n",
      "        if response.candidates\n",
      "        and response.candidates[0].grounding_metadata\n",
      "        and response.candidates[0].grounding_metadata.grounding_supports\n",
      "        else []\n",
      "    )\n",
      "    chunks = (\n",
      "        response.candidates[0].grounding_metadata.grounding_chunks\n",
      "        if response.candidates\n",
      "        and response.candidates[0].grounding_metadata\n",
      "        and response.candidates[0].grounding_metadata.grounding_chunks\n",
      "        else []\n",
      "    )\n",
      "\n",
      "    if not text:\n",
      "        return \"\"\n",
      "\n",
      "    # Sort supports by end_index in descending order to avoid shifting issues when inserting.\n",
      "    sorted_supports: list[genai_types.GroundingSupport] = sorted(\n",
      "        supports,\n",
      "        key=lambda s: s.segment.end_index if s.segment and s.segment.end_index is not None else 0,\n",
      "        reverse=True,\n",
      "    )\n",
      "\n",
      "    for support in sorted_supports:\n",
      "        end_index = (\n",
      "            support.segment.end_index\n",
      "            if support.segment and support.segment.end_index is not None\n",
      "            else 0\n",
      "        )\n",
      "        if support.grounding_chunk_indices:\n",
      "            # Create citation string like [1](link1)[2](link2)\n",
      "            citation_links = []\n",
      "            for i in support.grounding_chunk_indices:\n",
      "                if i < len(chunks):\n",
      "                    chunk = chunks[i]\n",
      "                    uri = chunk.web.uri if chunk.web and chunk.web.uri else None\n",
      "                    citation_links.append(f\"[{i + 1}]({uri})\")\n",
      "\n",
      "            citation_string = \", \".join(citation_links)\n",
      "            text = text[:end_index] + citation_string + text[end_index:]\n",
      "\n",
      "    return text\n",
      "\n",
      "\n",
      "def add_citations_from_metadata(text: str, grounding_metadata: GroundingMetadata) -> str:\n",
      "    \"\"\"\n",
      "    Inserts citation links into text based on grounding metadata.\n",
      "\n",
      "    This is a more direct version of add_citations that works with just the\n",
      "    grounding metadata instead of requiring a full response object.\n",
      "\n",
      "    Args:\n",
      "        text: The text to add citations to\n",
      "        grounding_metadata: The grounding metadata from Gemini API response\n",
      "\n",
      "    Returns:\n",
      "        The text with citations inserted.\n",
      "    \"\"\"\n",
      "    if not text or not grounding_metadata:\n",
      "        return text\n",
      "\n",
      "    # Extract supports and chunks from grounding metadata\n",
      "    # Handle both attribute access and dictionary access for flexibility\n",
      "    supports = []\n",
      "    chunks = []\n",
      "\n",
      "    # Try to get supports\n",
      "    if hasattr(grounding_metadata, \"grounding_supports\") and grounding_metadata.grounding_supports:\n",
      "        supports = grounding_metadata.grounding_supports\n",
      "    elif isinstance(grounding_metadata, dict) and grounding_metadata.get(\"grounding_supports\"):\n",
      "        supports = grounding_metadata[\"grounding_supports\"]\n",
      "\n",
      "    # Try to get chunks\n",
      "    if hasattr(grounding_metadata, \"grounding_chunks\"):\n",
      "        if grounding_metadata.grounding_chunks:\n",
      "            chunks = grounding_metadata.grounding_chunks\n",
      "    elif isinstance(grounding_metadata, dict) and \"grounding_chunks\" in grounding_metadata:\n",
      "        if grounding_metadata[\"grounding_chunks\"]:\n",
      "            chunks = grounding_metadata[\"grounding_chunks\"]\n",
      "\n",
      "    if not supports or not chunks:\n",
      "        return text\n",
      "\n",
      "    # Sort supports by end_index in descending order to avoid shifting issues when inserting.\n",
      "    # Handle both object and dictionary formats for segment and end_index\n",
      "    def get_end_index(support):\n",
      "        if hasattr(support, \"segment\"):\n",
      "            segment = support.segment\n",
      "            if hasattr(segment, \"end_index\") and segment.end_index is not None:\n",
      "                return segment.end_index\n",
      "        elif isinstance(support, dict) and \"segment\" in support:\n",
      "            segment = support[\"segment\"]\n",
      "            if isinstance(segment, dict) and segment.get(\"end_index\") is not None:\n",
      "                return segment[\"end_index\"]\n",
      "        return 0\n",
      "\n",
      "    sorted_supports = sorted(supports, key=get_end_index, reverse=True)\n",
      "\n",
      "    for support in sorted_supports:\n",
      "        # Get end_index from support, handling both object and dict formats\n",
      "        end_index = get_end_index(support)\n",
      "\n",
      "        # Get grounding_chunk_indices, handling both object and dict formats\n",
      "        indices = []\n",
      "        if hasattr(support, \"grounding_chunk_indices\"):\n",
      "            indices = support.grounding_chunk_indices\n",
      "        elif isinstance(support, dict) and \"grounding_chunk_indices\" in support:\n",
      "            indices = support[\"grounding_chunk_indices\"]\n",
      "\n",
      "        if indices:\n",
      "            # Create citation string like [1](link1)[2](link2)\n",
      "            citation_links = []\n",
      "            for i in indices:\n",
      "                if i < len(chunks):\n",
      "                    chunk = chunks[i]\n",
      "                    # Extract URI from chunk, handling both object and dict formats\n",
      "                    uri = None\n",
      "                    if hasattr(chunk, \"web\") and chunk.web:\n",
      "                        web = chunk.web\n",
      "                        if hasattr(web, \"uri\"):\n",
      "                            uri = web.uri\n",
      "                    elif isinstance(chunk, dict) and \"web\" in chunk:\n",
      "                        web = chunk[\"web\"]\n",
      "                        if isinstance(web, dict) and \"uri\" in web:\n",
      "                            uri = web[\"uri\"]\n",
      "\n",
      "                    if uri:\n",
      "                        citation_links.append(f\"[{i + 1}]({uri})\")\n",
      "\n",
      "            citation_string = \", \".join(citation_links)\n",
      "            text = text[:end_index] + citation_string + text[end_index:]\n",
      "\n",
      "    return text\n",
      "</file_contents>\n",
      "\n",
      "# Instructions\n",
      "Revise the \"Original Workplan\" based on the \"Revision Instructions\" and the provided \"Codebase Context\".\n",
      "Your output should be the complete, revised workplan in the same format as the original.\n",
      "\n",
      "The revised workplan should:\n",
      "1. Incorporate all changes requested in the revision instructions\n",
      "2. Maintain the same overall structure and formatting as the original\n",
      "3. Update any implementation details that are affected by the changes\n",
      "4. Ensure all sections remain comprehensive and implementable\n",
      "\n",
      "Respond directly with the complete revised workplan in Markdown format.\n",
      "IMPORTANT: Respond *only* with the Markdown content for the GitHub issue body. Do *not* wrap your entire response in a single Markdown code block (```). Start directly with the '## Summary' heading.\n",
      "\n",
      "```\n",
      "</details>\n",
      "[MOCK GitHub CLI] In directory: /Users/sravanj/project_work/yellhorn-mcp\n",
      "[MOCK GitHub CLI] Added comment to issue 123\n",
      "[INFO] Attempting to enable search grounding for model gemini-2.5-flash\n",
      "[INFO] Search grounding enabled for model gemini-2.5-flash\n",
      "[INFO] Found .yellhornignore with 1 patterns\n",
      "[INFO] Found .yellhorncontext with 17 whitelist, 0 blacklist, and 0 negation patterns\n",
      "[INFO] File categorization results out of 73 files:\n",
      "[INFO]   - 6 always ignored (images, binaries, configs, etc.)\n",
      "[INFO]   - 23 in yellhorncontext whitelist (included)\n",
      "[INFO]   - 0 in yellhorncontext blacklist (excluded)\n",
      "[INFO]   - 0 in yellhornignore whitelist (included)\n",
      "[INFO]   - 0 in yellhornignore blacklist (excluded)\n",
      "[INFO]   - 0 other files (excluded - .yellhorncontext exists)\n",
      "[INFO] Total included: 23 files (excluded 6 always-ignored files)\n",
      "[INFO] Read contents of 23 files\n",
      "[MOCK GitHub CLI] Running: gh issue edit 123 --body # Workplan Revision\n",
      "\n",
      "The user wants me to revise an existing workplan. I need to incorporate detailed testing steps, error handling requirements, and emphasize the creation of test notebooks for each major change.\n",
      "\n",
      "I have the original workplan content, which is a detailed plan for \"Refactor LLM Management with Unified Model Calling Service (LiteLLM)\". I will go through each \"Implementation Step\" and the \"Global Test Strategy\" section to add the requested revisions.\n",
      "\n",
      "Here's the revised workplan:\n",
      "\n",
      "# Revised Workplan\n",
      "\n",
      "Here's the revised workplan, focusing on detailed testing steps, including error handling requirements, and emphasizing the creation of dedicated test notebooks for each major change.\n",
      "\n",
      "## Summary\n",
      "The current `yellhorn-mcp` codebase utilizes direct API calls to OpenAI and Google Gemini models, leading to fragmented LLM interaction logic and increased maintenance overhead for supporting new providers. This work plan proposes to replace and consolidate all LLM model calls through a unified model calling service, LiteLLM. This integration will simplify LLM API interactions, streamline the addition of new models and providers, and centralize advanced features like retry logic and cost tracking, thereby enhancing the system's flexibility, maintainability, and scalability. The primary subsystems affected will be `yellhorn_mcp/llm_manager.py`, `yellhorn_mcp/server.py`, and related utility and configuration files.\n",
      "\n",
      "## Technical Details\n",
      "*   **Languages & Frameworks**: Python 3.10+\n",
      "*   **External Dependencies**:\n",
      "    *   `litellm~=1.33.0`: This new dependency will be added to `pyproject.toml` under `[project] dependencies`. LiteLLM provides a unified interface to over 100 LLM APIs, including OpenAI and Gemini, simplifying API calls and offering built-in features like retry logic and cost tracking.[1](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHkcmjfTVX6YEvrPO5cnweTJlLKXxOPCXurtkQFMRI72cv2IaBDUxW81SJMzBd1HEeBt6ATJBHXMceo0gRMwYCsw8T3_-8GDDM6dH3QokEshQq7feQS8N2iSMJaloYhrkSiZ4ThBLc=)\n",
      "*   **Dependency Management & Pinning Strategy**: `poetry` (as indicated by `pyproject.toml`). The new `litellm` dependency will be pinned using the `~=` operator for compatible releases.\n",
      "*   **Build, Lint, Formatting, Type-checking Commands**:\n",
      "    *   `python -m black yellhorn_mcp tests`\n",
      "    *   `python -m isort yellhorn_mcp tests`\n",
      "    *   `python -m pytest`\n",
      "    *   Type-checking will be performed via `pyright` (configured in `pyrightconfig.json`).\n",
      "*   **Logging & Observability**: Existing `logging` module will be leveraged. Ensure LiteLLM's internal logging integrates seamlessly or is configured to use the existing logging setup.\n",
      "*   **Analytics/KPIs**: Existing cost tracking and usage metadata (`yellhorn_mcp/utils/cost_tracker_utils.py`, `yellhorn_mcp/models/metadata_models.py`) will be adapted to consume LiteLLM's unified usage reports.\n",
      "*   **Testing Frameworks & Helpers**: `pytest`, `pytest-asyncio`, `httpx`, `pytest-cov` will continue to be used. Existing mock testing infrastructure (`examples/mock_context.py`) will be adapted to mock LiteLLM calls.\n",
      "\n",
      "## Architecture\n",
      "*   **Existing Components Leveraged**:\n",
      "    *   `yellhorn_mcp/llm_manager.py`: The core `LLMManager` class will be refactored to use LiteLLM as its underlying LLM interaction layer. Its existing logic for chunking (`ChunkingStrategy`), token counting (`TokenCounter`), and usage metadata aggregation (`UsageMetadata`) will be adapted to work with LiteLLM's outputs.\n",
      "    *   `yellhorn_mcp/token_counter.py`: Will continue to provide model-specific token limits and counting, potentially integrating with LiteLLM's internal token counting utilities if beneficial.\n",
      "    *   `yellhorn_mcp/models/metadata_models.py`: `UsageMetrics` and `CompletionMetadata` will continue to track LLM usage, adapted for LiteLLM's reporting format.\n",
      "    *   `yellhorn_mcp/utils/cost_tracker_utils.py`: `calculate_cost` and `format_metrics_section` will be updated to map LiteLLM's model identifiers to existing pricing.\n",
      "    *   `yellhorn_mcp/utils/search_grounding_utils.py`: The logic for configuring Google Search tools and adding citations will be adapted to pass through LiteLLM's API.\n",
      "    *   `yellhorn_mcp/processors/`: All processors (`context_processor.py`, `judgement_processor.py`, `workplan_processor.py`) will continue to use `LLMManager` as their LLM interface.\n",
      "*   **New Components Introduced**: No new top-level components will be introduced. The change is primarily an internal refactoring of the `LLMManager` to use LiteLLM.\n",
      "*   **Control-flow & Data-flow Diagram (ASCII)**:\n",
      "    ```\n",
      "    +-----------------+       +-------------------+       +-------------------+\n",
      "    | MCP Tools (CLI, |       | yellhorn_mcp/     |       | yellhorn_mcp/     |\n",
      "    | VSCode, Claude) |------>| server.py         |------>| llm_manager.py    |\n",
      "    +-----------------+       | (Tool Dispatcher) |       | (Unified LLM API) |\n",
      "                               +-------------------+       +-------------------+\n",
      "                                         ^                         |\n",
      "                                         |                         |\n",
      "                                         |                         |\n",
      "                                         |                         |\n",
      "                                         |                         v\n",
      "                                         |                   +-----------------+\n",
      "                                         |                   | LiteLLM Library |\n",
      "                                         |                   | (litellm.py)    |\n",
      "                                         |                   +-----------------+\n",
      "                                         |                         |\n",
      "                                         |                         |\n",
      "                                         |                         |\n",
      "                                         |                         v\n",
      "                                         |                   +-----------------+\n",
      "                                         |                   | OpenAI API      |\n",
      "                                         |                   | Gemini API      |\n",
      "                                         |                   | Other LLM APIs  |\n",
      "                                         |                   +-----------------+\n",
      "                                         |\n",
      "                                         +-------------------------------------+\n",
      "                                         (Usage Metadata, Responses, Errors)\n",
      "    ```\n",
      "*   **State-management, Retry/Fallback, and Error-handling Patterns**: LiteLLM offers robust built-in retry and fallback mechanisms. The existing `api_retry` decorator in `llm_manager.py` will be removed, and `LLMManager` will rely on LiteLLM's internal retry logic. Error handling will be adapted to catch LiteLLM-specific exceptions and translate them into `YellhornMCPError` where appropriate.\n",
      "\n",
      "## Completion Criteria & Metrics\n",
      "*   **Engineering Metrics**:\n",
      "    *   All `pytest` unit and integration tests pass with 100% success rate.\n",
      "    *   Test coverage for `yellhorn_mcp` remains at or above 70% line coverage.\n",
      "    *   `black` and `isort` formatting checks pass.\n",
      "    *   `pyright` type-checking runs clean with no errors.\n",
      "    *   `LLMManager` successfully makes calls to both OpenAI and Gemini models via LiteLLM.\n",
      "    *   Cost tracking and usage metadata reported in GitHub comments remain accurate and consistent with previous versions.\n",
      "*   **Business Metrics**:\n",
      "    *   Reduced time-to-integrate new LLM providers by leveraging LiteLLM's unified interface.\n",
      "    *   Improved system reliability due to LiteLLM's robust retry and fallback mechanisms.\n",
      "*   **Code-state Definition of Done**:\n",
      "    *   All CI jobs (linting, formatting, testing) are green.\n",
      "    *   `pyproject.toml` is updated with the new `litellm` dependency.\n",
      "    *   `LLMManagerREADME.md` and `docs/USAGE.md` are updated to reflect the LiteLLM integration.\n",
      "\n",
      "## References\n",
      "*   `yellhorn_mcp/llm_manager.py`\n",
      "*   `yellhorn_mcp/server.py`\n",
      "*   `yellhorn_mcp/cli.py`\n",
      "*   `yellhorn_mcp/token_counter.py`\n",
      "*   `yellhorn_mcp/integrations/gemini_integration.py`\n",
      "*   `yellhorn_mcp/utils/cost_tracker_utils.py`\n",
      "*   `yellhorn_mcp/utils/search_grounding_utils.py`\n",
      "*   `pyproject.toml`\n",
      "*   LiteLLM GitHub Repository: `https://github.com/BerriAI/litellm`\n",
      "*   LiteLLM Documentation: `https://docs.litellm.ai/`\n",
      "*   LiteLLM PyPI: `https://pypi.org/project/litellm/`\n",
      "\n",
      "## Implementation Steps\n",
      "### - [ ] Step 1: Add LiteLLM Dependency and Remove Direct API Clients\n",
      "**Description**: Introduce `litellm` as a core dependency in `pyproject.toml`. Remove direct `google-genai` and `openai` client dependencies, as LiteLLM will abstract these. Remove `tenacity` as LiteLLM provides its own retry logic.\n",
      "**Files**:\n",
      "*   `pyproject.toml`\n",
      "*   `yellhorn_mcp/server.py`\n",
      "**Reference(s)**:\n",
      "*   `pyproject.toml`\n",
      "*   `yellhorn_mcp/server.py`\n",
      "*   LiteLLM PyPI: `https://pypi.org/project/litellm/`\n",
      "**Test(s)**:\n",
      "*   Create a new test notebook `notebooks/test_litellm_dependency.ipynb`.\n",
      "*   **Detailed Testing Steps**:\n",
      "    *   Verify `poetry install` completes successfully without dependency conflicts.\n",
      "    *   Run `python -m yellhorn_mcp.cli --help` and assert that the command executes without `ImportError` or `ModuleNotFoundError` related to `google-genai`, `openai`, or `tenacity`.\n",
      "    *   Attempt to import `google.genai`, `openai`, and `tenacity` directly in a Python script and assert that they are no longer directly importable (or raise appropriate errors if they are now transitive dependencies).\n",
      "*   **Error Handling Requirements**:\n",
      "    *   Ensure `poetry install` provides clear error messages for dependency conflicts.\n",
      "    *   Verify that the CLI exits gracefully with an informative error if `litellm` itself cannot be found or initialized.\n",
      "\n",
      "### - [ ] Step 2: Refactor `LLMManager` Initialization and Core Call Logic\n",
      "**Description**: Modify `LLMManager.__init__` to accept a `litellm` client (or initialize it internally) instead of separate OpenAI and Gemini clients. Update `_single_call` to use `litellm.acompletion()` for all LLM interactions. Remove the `api_retry` decorator from `_call_openai` and `_call_gemini` (or remove these methods entirely if `_single_call` becomes the sole entry point).\n",
      "**Files**:\n",
      "*   `yellhorn_mcp/llm_manager.py`\n",
      "*   `yellhorn_mcp/server.py`\n",
      "**Reference(s)**:\n",
      "*   `yellhorn_mcp/llm_manager.py`\n",
      "*   `yellhorn_mcp/server.py`\n",
      "*   LiteLLM Documentation: `https://docs.litellm.ai/docs/completion`\n",
      "**Test(s)**:\n",
      "*   Create a new test notebook `notebooks/test_llm_manager_litellm.ipynb`.\n",
      "*   **Detailed Testing Steps**:\n",
      "    *   Initialize `LLMManager` with a mocked `litellm` client.\n",
      "    *   Call `llm_manager.call_llm` with various prompts, models (OpenAI and Gemini), temperatures, and `response_format` (text and JSON).\n",
      "    *   Assert that `litellm.acompletion()` is called with the correct parameters (e.g., `model`, `messages`, `temperature`, `response_format`).\n",
      "    *   Verify that the `api_retry` decorator is removed from `_call_openai` and `_call_gemini` (or that these methods are removed entirely).\n",
      "    *   Test both simple and chunked calls to ensure the chunking logic still functions correctly with LiteLLM.\n",
      "    *   Verify that the content returned from `call_llm` matches the mocked LiteLLM response.\n",
      "*   **Error Handling Requirements**:\n",
      "    *   Mock LiteLLM to simulate `RateLimitError` and `APIError` (e.g., 500 errors). Assert that LiteLLM's internal retry mechanism handles these errors as expected (e.g., exponential backoff, eventual success or failure after max retries).[2](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFsYXyG0CGdt2k4UbOYLl4et5Naqk1DonlVHoY4Rtr9FDqjnEpcj7a8FKNEQ2gnXWEI3myNsUz2mQrlBHQ9a-1cJ_8DKGFG8uNaQ1JftVKCZBbUp589e5Ixymc_n6HkKvrFD6kwbYeitKliImcCAdsiWPYpFFjJUPkPW7JPZMc=)\n",
      "    *   Verify that `LLMManager` propagates unrecoverable LiteLLM errors as `YellhornMCPError` or a similar custom exception.\n",
      "    *   Test cases where LiteLLM returns an empty or malformed response.\n",
      "\n",
      "### - [ ] Step 3: Adapt `UsageMetadata` and `TokenCounter` for LiteLLM\n",
      "**Description**: Update `UsageMetadata` to correctly parse LiteLLM's unified usage format. Investigate if `TokenCounter` can leverage LiteLLM's `token_counter` utility for more accurate or consistent token estimation across models.\n",
      "**Files**:\n",
      "*   `yellhorn_mcp/llm_manager.py`\n",
      "*   `yellhorn_mcp/token_counter.py`\n",
      "*   `yellhorn_mcp/models/metadata_models.py`\n",
      "**Reference(s)**:\n",
      "*   `yellhorn_mcp/llm_manager.py`\n",
      "*   `yellhorn_mcp/token_counter.py`\n",
      "*   `yellhorn_mcp/models/metadata_models.py`\n",
      "*   LiteLLM API: `https://docs.litellm.ai/docs/token_counter`\n",
      "**Test(s)**:\n",
      "*   Extend `notebooks/test_llm_manager_litellm.ipynb` or create `notebooks/test_token_usage_litellm.ipynb`.\n",
      "*   **Detailed Testing Steps**:\n",
      "    *   Create mock LiteLLM responses with various `usage` structures (e.g., `prompt_tokens`, `completion_tokens`, `total_tokens`).\n",
      "    *   Initialize `UsageMetadata` with these mock responses and assert that `prompt_tokens`, `completion_tokens`, and `total_tokens` are correctly extracted.\n",
      "    *   Test `TokenCounter`'s `count_tokens` and `get_model_limit` methods with LiteLLM's model naming conventions.\n",
      "    *   If LiteLLM's internal token counter is adopted, verify its accuracy against known token counts for sample texts.\n",
      "    *   Test edge cases for `UsageMetadata` (e.g., missing fields, `None` values).\n",
      "*   **Error Handling Requirements**:\n",
      "    *   Ensure `UsageMetadata` gracefully handles unexpected or incomplete usage data from LiteLLM without crashing.\n",
      "    *   Verify `TokenCounter` provides a sensible fallback (e.g., default limit) for unknown LiteLLM models.\n",
      "\n",
      "### - [ ] Step 4: Integrate Search Grounding and Deep Research Model Logic with LiteLLM\n",
      "**Description**: Adapt the logic for Google Search Grounding (`_get_gemini_search_tools`, `add_citations_from_metadata`) and Deep Research model tool activation (`_is_deep_research_model` in `LLMManager`) to work seamlessly with LiteLLM's unified API. LiteLLM supports passing `tools` parameters.\n",
      "**Files**:\n",
      "*   `yellhorn_mcp/llm_manager.py`\n",
      "*   `yellhorn_mcp/utils/search_grounding_utils.py`\n",
      "*   `yellhorn_mcp/integrations/gemini_integration.py` (will be refactored or removed)\n",
      "**Reference(s)**:\n",
      "*   `yellhorn_mcp/llm_manager.py`\n",
      "*   `yellhorn_mcp/utils/search_grounding_utils.py`\n",
      "*   LiteLLM API: `https://docs.litellm.ai/docs/completion` (for `tools` parameter)\n",
      "*   LiteLLM API: `https://docs.litellm.ai/docs/providers` (for model support)\n",
      "**Test(s)**:\n",
      "*   Extend `notebooks/test_llm_manager_litellm.ipynb`.\n",
      "*   **Detailed Testing Steps**:\n",
      "    *   Mock LiteLLM calls to simulate responses with `tools` parameters for Gemini models (e.g., `web_search_preview`).\n",
      "    *   Call `llm_manager.call_llm_with_citations` with a Gemini model and `tools` enabled. Assert that the `grounding_metadata` is correctly extracted and passed through.\n",
      "    *   Verify that `add_citations_from_metadata` correctly formats citations from LiteLLM's grounding metadata.\n",
      "    *   Test `_is_deep_research_model` in `LLMManager` to ensure it correctly identifies deep research models and that the appropriate `tools` (web search, code interpreter) are included in the LiteLLM call parameters.\n",
      "    *   Test cases where search grounding is disabled (`disable_search_grounding=True`) to ensure no tools are passed.\n",
      "*   **Error Handling Requirements**:\n",
      "    *   Verify that if LiteLLM fails to process `tools` parameters, an appropriate error is logged or raised.\n",
      "    *   Ensure `add_citations_from_metadata` gracefully handles missing or malformed grounding metadata.\n",
      "\n",
      "### - [ ] Step 5: Update `server.py` and `cli.py` for LiteLLM Integration\n",
      "**Description**: Modify `app_lifespan` in `server.py` to initialize LiteLLM instead of separate OpenAI and Gemini clients. Adjust `cli.py` to handle API keys and model selection in a LiteLLM-compatible manner, potentially simplifying environment variable requirements. Remove `yellhorn_mcp/integrations/gemini_integration.py` as its functionality will be absorbed by `llm_manager.py` and LiteLLM.\n",
      "**Files**:\n",
      "*   `yellhorn_mcp/server.py`\n",
      "*   `yellhorn_mcp/cli.py`\n",
      "*   `yellhorn_mcp/integrations/gemini_integration.py` (delete)\n",
      "**Reference(s)**:\n",
      "*   `yellhorn_mcp/server.py`\n",
      "*   `yellhorn_mcp/cli.py`\n",
      "*   LiteLLM Getting Started: `https://docs.litellm.ai/docs/getting_started`\n",
      "**Test(s)**:\n",
      "*   Create a new test notebook `notebooks/test_server_cli_integration.ipynb`.\n",
      "*   **Detailed Testing Steps**:\n",
      "    *   Run the `yellhorn-mcp` CLI with various model arguments (OpenAI, Gemini, deep research) and assert that `LLMManager` is initialized correctly with LiteLLM.\n",
      "    *   Verify that `app_lifespan` in `server.py` no longer initializes `google.genai.Client` or `openai.AsyncOpenAI` directly.\n",
      "    *   Perform end-to-end tests for `create_workplan`, `judge_workplan`, and `curate_context` using the `run_create_workplan`, `run_judge_workplan`, and `run_curate_context` helpers from `examples/mock_context.py`.\n",
      "    *   Assert that these tools successfully interact with the mocked LiteLLM (via `LLMManager`) and produce expected outputs (e.g., GitHub issue updates, context files).\n",
      "    *   Verify that API key validation in `cli.py` is adapted for LiteLLM's requirements (e.g., `LITELLM_API_KEY` or provider-specific keys if LiteLLM still uses them).\n",
      "*   **Error Handling Requirements**:\n",
      "    *   Test CLI startup with missing API keys and assert that it exits with a clear error message.[3](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRmAVLMM6kOUNIenLFDuLUNWgf8y8L6Ru3Rbv_DwAkx0bsy36-xkg1UeaN8FRET-UEm5B2iR2Cdpjd3j7kaEQNdhwSW7-41D98srGwpySfcWlDUe2ym61t8EHIViLphTxsiQTxGs4mVV0OVIKUI2xALTPQsR8-jykYa6fI-YOkn-2THAxF86P9ZBUfJ0cNZZTBHezFxbe9_HC42LX0o5e_y1v3xEnD5Dw=)\n",
      "    *   Simulate LiteLLM API errors during end-to-end tool execution and verify that appropriate error comments are posted to GitHub issues.\n",
      "    *   Ensure `app_lifespan` handles `ValueError` for missing environment variables gracefully.\n",
      "\n",
      "### - [ ] Step 6: Update Cost Tracking and Documentation\n",
      "**Description**: Review and update `yellhorn_mcp/utils/cost_tracker_utils.py` to ensure accurate cost calculation with LiteLLM's model naming conventions and potentially leverage LiteLLM's cost tracking features if they offer more granularity. Update `LLMManagerREADME.md` and `docs/USAGE.md` to reflect the new LiteLLM-based architecture and usage.\n",
      "**Files**:\n",
      "*   `yellhorn_mcp/utils/cost_tracker_utils.py`\n",
      "*   `LLMManagerREADME.md`\n",
      "*   `docs/USAGE.md`\n",
      "**Reference(s)**:\n",
      "*   `yellhorn_mcp/utils/cost_tracker_utils.py`\n",
      "*   `LLMManagerREADME.md`\n",
      "*   `docs/USAGE.md`\n",
      "*   LiteLLM Model Cost Map: `https://docs.litellm.ai/docs/proxy/model_cost_map`\n",
      "**Test(s)**:\n",
      "*   Extend `notebooks/test_llm_manager_litellm.ipynb` or create `notebooks/test_cost_tracking.ipynb`.\n",
      "*   **Detailed Testing Steps**:\n",
      "    *   Update `MODEL_PRICING` in `cost_tracker_utils.py` to reflect LiteLLM's model naming conventions and pricing (if different).\n",
      "    *   Create unit tests for `calculate_cost` with various LiteLLM models and token counts, asserting correct cost calculation.\n",
      "    *   Verify that `format_metrics_section` correctly displays usage and cost information based on LiteLLM's output.\n",
      "    *   Manually review `LLMManagerREADME.md` and `docs/USAGE.md` to ensure all references to direct OpenAI/Gemini clients are replaced with LiteLLM, and new usage instructions are clear.\n",
      "*   **Error Handling Requirements**:\n",
      "    *   Ensure `calculate_cost` gracefully handles unknown LiteLLM models (e.g., returns `None` or a default value).\n",
      "    *   Verify that documentation clearly outlines any new environment variables or configuration required for LiteLLM.\n",
      "\n",
      "## Global Test Strategy\n",
      "*   **Unit Tests**: All modified functions and classes will have comprehensive unit tests using `pytest` and `pytest-asyncio`. Mocking will be used for external API calls (LiteLLM). Focus on isolated component behavior and edge cases.\n",
      "*   **Test Notebooks**: Dedicated Jupyter notebooks (e.g., `notebooks/test_litellm_dependency.ipynb`, `notebooks/test_llm_manager_litellm.ipynb`, `notebooks/test_server_cli_integration.ipynb`, `notebooks/test_cost_tracking.ipynb`) will be created for each major change. These notebooks will serve as executable integration tests and living documentation for testing the new LiteLLM integration.[4](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGpUwdoufWEksdj8zArZfyd3H-QZlMhUTxQ6eszwM4hrDWO4FVP6rOfhhEs13tU6FXma8sKRkIGzNUqy6OalT1SBm1eTSy3KlpWDSPyjIFbqo1jZqPMegn_f5kmgiF_mfmuGKZDdAvglg==),[5](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGTmQHV2z-WlM-MLaSEbO-NKwU8OmwIJApIZYhufjX2pyWDom-rPNLIGfoebSOUOGvoz4CTbGLwlnZwo1P8GUx2_ZlL08_vCL1hbuu260IpUPvDpGO0dm-v1Nb05I_8STn0DSBIzqc=),[6](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQER0fwQSoGtHLUyhNZEY7KnQBgXy9GAM1gDlD_BMvUNUL1G6FKlH2rsT8KtGUqm6gUXiw8jOEuneHrFi8NvJS_plprjENdl8cAbmUF1NjcooCs9AC6uSA5VTDJQd0uuU8T9XSTrwmI2cgavMdY6LeGQ8p48BJH0cWaq6MtnJVjyiBel2nB1wNq3GVrRrW6miDlFLnyeqe6kDkDfpA==),[7](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFXAJHZtIEnlT7VErBrYsi3bxgRwPIK3GMnPcs0haRJIthIRKkvFOj2G8H7t-ZATrnkpQT-zn6SLf8mqr4kfAYsUv_latZZjtC4iGyR4og9NZFYNLYwM5oFHJsoCzFSadgjZ_5ebxU=),[8](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFMWJT3Uc8UVfULgZ6btx_WlvyqUj4H-3L_mHFHHk3v114eBXokBdpVOyu9VMgEKSGQJnwfcFagqGgPwrF5Tb-cqvkU3URfKINwETAXSli0enuOc7HkMDKS85Ioe7m3qpzQX_5f_77WaNWrxH6OXIRPjHhzRky_zrECwtvVwEhje8bZbzrN2VtYfO1WdD_bY3gyGd14xxehmCisuHGt)\n",
      "*   **Integration Tests**: The `examples/mock_context.py` will be updated and used within the test notebooks to run integration tests for `create_workplan`, `judge_workplan`, and `curate_context` tools. These tests will simulate GitHub interactions and verify LLM calls through the LiteLLM integration, including chunking, search grounding, and deep research model activation.[9](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGaKemsTt6bOPjjHXax9oZh72h0n5y3k_h_vrRPgOfDIZ4h9b3ALynHuJ3NSWFMI4arNnR89oQRp251yaA6LqPFTq9WaGZqXCiLNou3l09erAQ3hWXXZKZ8UcpHGW4zl7d5o9lCGE716es7bTsiNZjizUuk),[10](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFW_rj_6m6mFtPMWs5T5TmU-qNlA4Ll08160ignhPxw15FkwZAPFdCorGJJL9X6r7cZCADHF6UHD8NXCRemWCyq-pZR4x1dxAjUKYYdrwfoTa_W3oediy6TzljzGbL17MrEZFO--CcIE3vnkFSm48UqpSi9kFmsXwcZod0rXxbG),[11](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEuLoZsQ_0hR_lk6PoRdQZ-IjLxZlyAeOkfW2a1J0ZizOLxdGWtfU523XQf1XkzvhDa-Wizld-2Z31Gb_hHAbts4-K1AXMMU5W38wzdLSrHVx3WuHyEJV6Mvuoio99iGlJKdsnFDjoBQP4utUg=),[12](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGhNabIsOsM18kJcs5RzwFRDckoMyT-q4WdeDe8JL2M28zTXKHzjR0Fou8l5yixFWxnizqB3d3oGL2MoIi9TLtq6cachyo-Vk3ObwY3bL4aXBshJm9N33zBu3_ZJfYyNJn8TCE5L6aQ_jmrw43S296GFuZ7VLM59cV7sN0TtCQeHQ==),[13](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGnGJHPdSVC0os9h7zG91EpTJt_8kzXjkttaQqGie9ORFSA4Ld26Ac5O4_1TX2zs37kknWr1A-d-UvM9kS1pa5GL5uYVNaveew-uIFVQO4MF-E4n97hAJpRK8tm1w5Bk-tkRxXmhxiZZpZt9prpDfwyls9o7lUm)\n",
      "*   **Error Handling Tests**: Comprehensive tests will be added to verify robust error handling and retry logic. This includes:[2](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFsYXyG0CGdt2k4UbOYLl4et5Naqk1DonlVHoY4Rtr9FDqjnEpcj7a8FKNEQ2gnXWEI3myNsUz2mQrlBHQ9a-1cJ_8DKGFG8uNaQ1JftVKCZBbUp589e5Ixymc_n6HkKvrFD6kwbYeitKliImcCAdsiWPYpFFjJUPkPW7JPZMc=),[14](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH77RMfhF8XlxdHmMsPdRVa0hScRdj-q87FVjLjtQoaLpkbVEPsr3zsdKM3JJQGUuL-hQzGeJcSnDp9YopfDztWDhhPRKTun3Uv9DbdYdlx6mbqmVM__HKgOn-UHHhuFar2irsDmBTP47egRpSg3Ud1Q6l-R5xt1g==),[15](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHhS004XVwih6JICJJOtI-JSFr7MxhFcu1vTNW30IgF56UEWNvKd8Yo3jax9p-gu8EzOGexD3fIaRT9DULXzUVInJ3v80Q1h-4GX2UDhAQmfbX6NcGrvCyI75I8Pb5WduGxsM2ODkF9PyrBvl8RsNmiupxVzFfAr9AhOAbVp8nRVRZtXdskKuGtKB98Bk6GdRi6GjKI_7Rs30tc282qcwLh5PXJDexr3sncEeKMPvFYofXiW1j-2IUGSA==),[3](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRmAVLMM6kOUNIenLFDuLUNWgf8y8L6Ru3Rbv_DwAkx0bsy36-xkg1UeaN8FRET-UEm5B2iR2Cdpjd3j7kaEQNdhwSW7-41D98srGwpySfcWlDUe2ym61t8EHIViLphTxsiQTxGs4mVV0OVIKUI2xALTPQsR8-jykYa6fI-YOkn-2THAxF86P9ZBUfJ0cNZZTBHezFxbe9_HC42LX0o5e_y1v3xEnD5Dw=),[16](https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHdDpfEYC5MyETK6lIvUWFuLWmp0iQ3UXCbYR3aLHQor55b6mHGtqkQPjyZD1E2x77icQfB03qzcuMxGQz-Kjmuf-0toiDxGV5S2CJX_4o-nUrEX0mDSHHoB9lJ5WOIL2uCde2ZjsAntqEqykHEAUeFH20-MdWW-IOG1pGHC9CyC2-QHQ==)\n",
      "    *   Simulating API rate limits and transient network errors to ensure LiteLLM's built-in retry mechanism functions correctly.\n",
      "    *   Testing cases where LiteLLM returns malformed or empty responses.\n",
      "    *   Verifying that `YellhornMCPError` or other appropriate custom exceptions are raised for unrecoverable errors, and that informative error messages are logged and posted to GitHub issues.\n",
      "    *   Testing CLI startup with missing or invalid API keys.\n",
      "*   **End-to-End Tests**: Manual verification of the `yellhorn-mcp` CLI and its interaction with GitHub issues will be performed to confirm full functionality in a real environment.\n",
      "*   **Test Coverage**: Maintain minimum 70% test coverage for all new and modified code, enforced by `pytest-cov`.\n",
      "*   **Local Execution**: Tests can be run locally using `poetry run pytest` and by executing the new test notebooks.\n",
      "*   **Environment Variables/Secrets**: API keys for LiteLLM (and underlying providers if needed) will be managed via environment variables (`LITELLM_API_KEY` or provider-specific keys like `OPENAI_API_KEY`, `GEMINI_API_KEY` as LiteLLM supports them).\n",
      "*   **Async Helpers/Fixtures**: Existing `pytest-asyncio` fixtures will be leveraged.\n",
      "\n",
      "## Files to Modify / New Files to Create\n",
      "*   **Files to Modify**:\n",
      "    *   `pyproject.toml`: Add `litellm` dependency, remove `google-genai`, `openai`, `tenacity`, `google-api-core`.\n",
      "    *   `yellhorn_mcp/llm_manager.py`: Refactor `LLMManager` to use LiteLLM, update `UsageMetadata`, remove `api_retry` decorator.\n",
      "    *   `yellhorn_mcp/server.py`: Update `app_lifespan` to initialize LiteLLM, remove direct client initializations.\n",
      "    *   `yellhorn_mcp/cli.py`: Adjust API key validation and model selection logic.\n",
      "    *   `yellhorn_mcp/token_counter.py`: Potentially integrate LiteLLM's token counting.\n",
      "    *   `yellhorn_mcp/utils/cost_tracker_utils.py`: Update `MODEL_PRICING` and `calculate_cost` for LiteLLM models.\n",
      "    *   `yellhorn_mcp/utils/search_grounding_utils.py`: Ensure compatibility with LiteLLM's `tools` parameter.\n",
      "    *   `LLMManagerREADME.md`: Update LLM Manager usage and architecture.\n",
      "    *   `docs/USAGE.md`: Update installation, configuration, and tool usage sections.\n",
      "*   **New Files to Create**:\n",
      "    *   `notebooks/test_litellm_dependency.ipynb`: Test notebook for dependency changes and CLI startup.\n",
      "    *   `notebooks/test_llm_manager_litellm.ipynb`: Test notebook for `LLMManager` core logic, chunking, search grounding, deep research models, and usage/token counting.\n",
      "    *   `notebooks/test_server_cli_integration.ipynb`: Test notebook for end-to-end server and CLI tool integration.\n",
      "    *   `notebooks/test_cost_tracking.ipynb`: (Optional, can be merged with `test_llm_manager_litellm.ipynb`) Test notebook for cost calculation accuracy.\n",
      "    *   `yellhorn_mcp/integrations/gemini_integration.py` (delete) - This file will be removed as its functionality is absorbed.\n",
      "[MOCK GitHub CLI] In directory: /Users/sravanj/project_work/yellhorn-mcp\n",
      "[MOCK GitHub CLI] Updated issue 123\n",
      "[INFO] Successfully updated GitHub issue #123 with generated workplan and metrics\n",
      "[MOCK GitHub CLI] Running: gh issue comment 123 --body ## ✅ Workplan generated successfully\n",
      "\n",
      "### Generation Details\n",
      "**Time**: 54.5 seconds  \n",
      "**Completed**: 2025-08-10 23:19:59 UTC  \n",
      "**Model Used**: `gemini-2.5-flash`  \n",
      "\n",
      "### Token Usage\n",
      "**Input Tokens**: 135,490  \n",
      "**Output Tokens**: 6,104  \n",
      "**Total Tokens**: 153,148  \n",
      "**Estimated Cost**: $0.0559  \n",
      "\n",
      "**Search Results Used**: 1  \n",
      "**Context Size**: 465,785 characters  \n",
      "[MOCK GitHub CLI] In directory: /Users/sravanj/project_work/yellhorn-mcp\n",
      "[MOCK GitHub CLI] Added comment to issue 123\n",
      "[INFO] Waiting for 0 background tasks...\n",
      "[INFO] All background tasks completed\n",
      "Workplan Revised:\n",
      "--------------------------------------------------\n",
      "Issue URL: https://github.com/mock/repo/issues/123\n",
      "Issue Number: 123\n"
     ]
    }
   ],
   "source": [
    "# Call process_revision_async with our mock context\n",
    "revision_result = await run_revise_workplan(\n",
    "    issue_number=\"123\", \n",
    "    original_workplan=\"Original workplan content here...\", \n",
    "    revision_instructions=\"Focus on building test notebooks for each of the major changes. Add more detailed testing steps and include error handling requirements\",\n",
    "    repo_path=REPO_PATH,\n",
    "    llm_manager=llm_manager,\n",
    "    model=MODEL,\n",
    "    codebase_reasoning=\"full\",\n",
    "    debug=True,\n",
    "    disable_search_grounding=False,\n",
    "    github_command_func=mock_github_command,\n",
    "    git_command_func=run_git_command_with_set_cwd(REPO_PATH),\n",
    "    log_callback=log_callback,\n",
    "    wait_for_background_tasks=True,\n",
    "    background_task_timeout=180\n",
    ")\n",
    "\n",
    "print(\"Workplan Revised:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Issue URL: {revision_result['issue_url']}\")\n",
    "print(f\"Issue Number: {revision_result['issue_number']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using Judge Workplan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.mock_context import run_judge_workplan, mock_github_command\n",
    "from yellhorn_mcp.server import process_judgement_async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example workplan content (you would typically get this from a GitHub issue)\n",
    "WORKPLAN_CONTENT = \"\"\"\n",
    "# Example Workplan: Add Token Counter Feature\n",
    "\n",
    "## Summary\n",
    "Implement a token counting system using tiktoken to prevent token overflow in LLM calls.\n",
    "\n",
    "## Implementation Steps\n",
    "1. Create TokenCounter class with tiktoken integration\n",
    "2. Add token counting to LLMManager\n",
    "3. Implement automatic prompt chunking when limits are exceeded\n",
    "4. Update all LLM call sites to use the new system\n",
    "\n",
    "## Files to Modify\n",
    "- `yellhorn_mcp/llm_manager.py`: Add token counting integration\n",
    "- `yellhorn_mcp/server.py`: Update LLM call sites\n",
    "\n",
    "## New Files to Create\n",
    "- `yellhorn_mcp/token_counter.py`: Core token counting functionality\n",
    "\"\"\"\n",
    "\n",
    "# Example diff content (you would typically get this from git diff)\n",
    "DIFF_CONTENT = \"\"\"\n",
    "diff --git a/yellhorn_mcp/token_counter.py b/yellhorn_mcp/token_counter.py\n",
    "new file mode 100644\n",
    "index 0000000..1234567\n",
    "--- /dev/null\n",
    "+++ b/yellhorn_mcp/token_counter.py\n",
    "@@ -0,0 +1,50 @@\n",
    "+# Token counting utilities using tiktoken.\n",
    "+\n",
    "+import tiktoken\n",
    "+from typing import Optional\n",
    "+\n",
    "+class TokenCounter:\n",
    "+    \\\"\\\"\\\"Token counter using tiktoken for accurate token counting.\\\"\\\"\\\"\n",
    "+    \n",
    "+    def __init__(self, model: str):\n",
    "+        self.model = model\n",
    "+        self.encoding = tiktoken.encoding_for_model(model)\n",
    "+    \n",
    "+    def count_tokens(self, text: str) -> int:\n",
    "+        \\\"\\\"\\\"Count tokens in the given text.\\\"\\\"\\\"\n",
    "+        return len(self.encoding.encode(text))\n",
    "\n",
    "diff --git a/yellhorn_mcp/llm_manager.py b/yellhorn_mcp/llm_manager.py\n",
    "index abcd123..efgh456 100644\n",
    "--- a/yellhorn_mcp/llm_manager.py\n",
    "+++ b/yellhorn_mcp/llm_manager.py\n",
    "@@ -10,6 +10,7 @@ from typing import Dict, List, Optional, Any, Union\n",
    " from openai import AsyncOpenAI\n",
    " import google.genai as genai\n",
    " from .usage_metadata import UsageMetadata\n",
    "+from .token_counter import TokenCounter\n",
    " \n",
    " class LLMManager:\n",
    "     \\\"\\\"\\\"Unified LLM manager with token counting and chunking support.\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "# Judge workplan parameters\n",
    "base_ref = \"main\"\n",
    "head_ref = \"feature/token-counter\"\n",
    "subissue_to_update = \"456\"  # GitHub issue number for the sub-issue to update\n",
    "parent_workplan_issue_number = \"123\"  # Original workplan issue number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 16:28:21,836 INFO AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Attempting to enable search grounding for model gemini-2.5-flash\n",
      "[INFO] Search grounding enabled for model gemini-2.5-flash\n",
      "[INFO] LLM call initiated - Model: gemini-2.5-flash, Input tokens: 805, Model limit: 1048576, Temperature: 0.0\n",
      "[INFO] LLM call completed - Model: gemini-2.5-flash, Completion tokens: 1010, Total tokens: 3198, Time: 11.34s\n",
      "[MOCK GitHub CLI] Running: gh issue edit 456 --title Judgement for #123: feature/token-counter vs main --body-file /var/folders/3h/5k3yk1s13yn9cql9x6fl9g9h0000gn/T/tmpcpva3948.md\n",
      "[MOCK GitHub CLI] In directory: /Users/sravanj/project_work/yellhorn-mcp\n",
      "[MOCK GitHub CLI] File contents: Parent workplan: #123\n",
      "\n",
      "## Comparison Metadata\n",
      "- **Workplan Issue**: `#123`\n",
      "- **Base Ref**: `main` (Commit: `abc123`)\n",
      "- **Head Ref**: `feature/token-counter` (Commit: `def456`)\n",
      "- **Codebase Reasoning Mode**: `full`\n",
      "- **AI Model**: `gemini-2.5-flash`\n",
      "\n",
      "## Judgement Summary\n",
      "INCOMPLETE. The provided code diff only partially implements the first two steps of the workplan and completely omits the crucial aspects of automatic prompt chunking and updating LLM call sites.\n",
      "\n",
      "## Implementation Analysis\n",
      "The diff successfully implements the creation of the `TokenCounter` class in a new file `yellhorn_mcp/token_counter.py`. This class correctly utilizes `tiktoken` for encoding and provides a `count_tokens` method as outlined in step 1 of the workplan (\"Create TokenCounter class with tiktoken integration\").\n",
      "\n",
      "Additionally, the diff includes the necessary import of `TokenCounter` into `yellhorn_mcp/llm_manager.py` and updates the `LLMManager` class docstring to reflect future token counting and chunking support. This is a foundational piece for \"Add token counting to LLMManager\" (step 2), but the actual integration and usage within the `LLMManager` class are missing from the provided diff.\n",
      "\n",
      "## Missing or Incomplete Items\n",
      "The following critical items from the workplan have not been addressed in the provided diff:\n",
      "\n",
      "*   **Step 2 (Add token counting to LLMManager) - Incomplete:** While `TokenCounter` is imported, the `LLMManager` class itself does not instantiate `TokenCounter`, nor does it use its `count_tokens` method to perform token counting on LLM inputs or outputs.\n",
      "*   **Step 3 (Implement automatic prompt chunking when limits are exceeded) - Missing:** There is no code in the diff that implements any logic for automatic prompt chunking when token limits are surpassed. This is a core feature described in the workplan's summary and implementation steps.\n",
      "*   **Step 4 (Update all LLM call sites to use the new system) - Missing:** The `yellhorn_mcp/server.py` file, which was explicitly listed in \"Files to Modify\" for updating LLM call sites, is not touched at all in this diff. This indicates that no LLM call sites have been updated to leverage the new token counting and potential chunking system.\n",
      "*   **`yellhorn_mcp/llm_manager.py` modifications - Incomplete:** Beyond the import statement and a docstring change, the functional modifications required in `llm_manager.py` to integrate token counting and implement chunking are entirely absent.\n",
      "\n",
      "## Code Quality Assessment\n",
      "*   **Code style and consistency:** The `token_counter.py` file adheres to good Python style with clear class and method definitions, and appropriate docstrings. The variable naming is clear.\n",
      "*   **Error handling:** Basic error handling is not explicitly shown, such as handling cases where a non-existent model string is passed to `tiktoken.encoding_for_model`. However, for the scope of a `TokenCounter`, this might be handled at a higher level.\n",
      "*   **Test coverage:** No unit tests are included in the diff for the new `TokenCounter` class or for the (missing) token counting and chunking logic in `LLMManager`.\n",
      "*   **Documentation:** The new `TokenCounter` class and its method are well-documented with clear docstrings. The `LLMManager` docstring was updated to reflect future capabilities, which is good.\n",
      "*   **Potential issues:** The `TokenCounter` is initialized with a `model` in its constructor. If `LLMManager` needs to handle calls for different models, it would need to manage multiple `TokenCounter` instances or a more dynamic way of getting the correct encoding. This is a design consideration that isn't problematic for the current (limited) implementation but might become relevant later.\n",
      "\n",
      "## Recommendations\n",
      "1.  **Complete LLMManager Integration:** Implement the instantiation of `TokenCounter` within `LLMManager` and integrate the `count_tokens` method for all LLM calls.\n",
      "2.  **Implement Prompt Chunking:** Add the logic within `LLMManager` to automatically chunk prompts when they exceed defined token limits. This is a critical feature of the workplan.\n",
      "3.  **Update Call Sites:** Modify `yellhorn_mcp/server.py` and any other relevant files to utilize the new token counting and chunking capabilities provided by `LLMManager`.\n",
      "4.  **Add Unit Tests:** Develop comprehensive unit tests for `TokenCounter` to ensure its accuracy, and for `LLMManager` to verify the correct functioning of token counting and prompt chunking.\n",
      "5.  **Consider Model Flexibility:** If `LLMManager` is intended to interact with multiple LLM models with different tokenization schemes, refine `TokenCounter` or its usage to gracefully handle dynamic model changes or multiple encodings.\n",
      "\n",
      "## References\n",
      "None provided in the workplan or code diff....\n",
      "[MOCK GitHub CLI] Updated issue 456\n",
      "[INFO] Successfully created judgement sub-issue: git@github-personal:SVJayanthi/yellhorn-mcp/issues/456\n",
      "[MOCK GitHub CLI] Running: gh issue comment 456 --body <details>\n",
      "<summary>Debug: Full prompt used for generation</summary>\n",
      "\n",
      "```\n",
      "You are an expert software reviewer tasked with judging whether a code diff successfully implements a given workplan.\n",
      "\n",
      "# Original Workplan\n",
      "\n",
      "# Example Workplan: Add Token Counter Feature\n",
      "\n",
      "## Summary\n",
      "Implement a token counting system using tiktoken to prevent token overflow in LLM calls.\n",
      "\n",
      "## Implementation Steps\n",
      "1. Create TokenCounter class with tiktoken integration\n",
      "2. Add token counting to LLMManager\n",
      "3. Implement automatic prompt chunking when limits are exceeded\n",
      "4. Update all LLM call sites to use the new system\n",
      "\n",
      "## Files to Modify\n",
      "- `yellhorn_mcp/llm_manager.py`: Add token counting integration\n",
      "- `yellhorn_mcp/server.py`: Update LLM call sites\n",
      "\n",
      "## New Files to Create\n",
      "- `yellhorn_mcp/token_counter.py`: Core token counting functionality\n",
      "\n",
      "\n",
      "# Code Diff\n",
      "\n",
      "diff --git a/yellhorn_mcp/token_counter.py b/yellhorn_mcp/token_counter.py\n",
      "new file mode 100644\n",
      "index 0000000..1234567\n",
      "--- /dev/null\n",
      "+++ b/yellhorn_mcp/token_counter.py\n",
      "@@ -0,0 +1,50 @@\n",
      "+# Token counting utilities using tiktoken.\n",
      "+\n",
      "+import tiktoken\n",
      "+from typing import Optional\n",
      "+\n",
      "+class TokenCounter:\n",
      "+    \"\"\"Token counter using tiktoken for accurate token counting.\"\"\"\n",
      "+    \n",
      "+    def __init__(self, model: str):\n",
      "+        self.model = model\n",
      "+        self.encoding = tiktoken.encoding_for_model(model)\n",
      "+    \n",
      "+    def count_tokens(self, text: str) -> int:\n",
      "+        \"\"\"Count tokens in the given text.\"\"\"\n",
      "+        return len(self.encoding.encode(text))\n",
      "\n",
      "diff --git a/yellhorn_mcp/llm_manager.py b/yellhorn_mcp/llm_manager.py\n",
      "index abcd123..efgh456 100644\n",
      "--- a/yellhorn_mcp/llm_manager.py\n",
      "+++ b/yellhorn_mcp/llm_manager.py\n",
      "@@ -10,6 +10,7 @@ from typing import Dict, List, Optional, Any, Union\n",
      " from openai import AsyncOpenAI\n",
      " import google.genai as genai\n",
      " from .usage_metadata import UsageMetadata\n",
      "+from .token_counter import TokenCounter\n",
      "\n",
      " class LLMManager:\n",
      "     \"\"\"Unified LLM manager with token counting and chunking support.\"\"\"\n",
      "\n",
      "\n",
      "# Task\n",
      "Review the code diff against the original workplan and provide a detailed judgement. Consider:\n",
      "\n",
      "1. **Completeness**: Does the diff implement all the steps and requirements outlined in the workplan?\n",
      "2. **Correctness**: Is the implementation technically correct and does it follow best practices?\n",
      "3. **Missing Elements**: What parts of the workplan, if any, were not addressed?\n",
      "4. **Additional Changes**: Were there any changes made that weren't part of the original workplan?\n",
      "5. **Quality**: Comment on code quality, testing, documentation, and any potential issues.\n",
      "\n",
      "The diff represents changes between 'main' and 'feature/token-counter'.\n",
      "\n",
      "Structure your response with these clear sections:\n",
      "\n",
      "## Judgement Summary\n",
      "Provide a clear verdict: APPROVED, NEEDS_WORK, or INCOMPLETE, followed by a brief explanation.\n",
      "\n",
      "## Implementation Analysis\n",
      "Detail what was successfully implemented from the workplan.\n",
      "\n",
      "## Missing or Incomplete Items\n",
      "List specific items from the workplan that were not addressed or were only partially implemented.\n",
      "\n",
      "## Code Quality Assessment\n",
      "Evaluate the quality of the implementation including:\n",
      "- Code style and consistency\n",
      "- Error handling\n",
      "- Test coverage\n",
      "- Documentation\n",
      "\n",
      "## Recommendations\n",
      "Provide specific, actionable recommendations for improvement.\n",
      "\n",
      "## References\n",
      "Extract any URLs mentioned in the workplan or that would be helpful for understanding the implementation and list them here. This ensures important links are preserved.\n",
      "\n",
      "IMPORTANT: Respond *only* with the Markdown content for the judgement. Do *not* wrap your entire response in a single Markdown code block (```). Start directly with the '## Judgement Summary' heading.\n",
      "\n",
      "```\n",
      "</details>\n",
      "[MOCK GitHub CLI] In directory: /Users/sravanj/project_work/yellhorn-mcp\n",
      "[MOCK GitHub CLI] Added comment to issue 456\n",
      "[MOCK GitHub CLI] Running: gh issue comment 456 --body ## ✅ Judgement generated successfully\n",
      "\n",
      "### Generation Details\n",
      "**Time**: 11.3 seconds  \n",
      "**Completed**: 2025-08-10 23:28:33 UTC  \n",
      "**Model Used**: `gemini-2.5-flash`  \n",
      "\n",
      "### Token Usage\n",
      "**Input Tokens**: 918  \n",
      "**Output Tokens**: 1,010  \n",
      "**Total Tokens**: 3,198  \n",
      "**Estimated Cost**: $0.0028  \n",
      "\n",
      "**Search Results Used**: 0  \n",
      "**Context Size**: 3,551 characters  \n",
      "[MOCK GitHub CLI] In directory: /Users/sravanj/project_work/yellhorn-mcp\n",
      "[MOCK GitHub CLI] Added comment to issue 456\n",
      "[INFO] Waiting for 0 background tasks...\n",
      "[INFO] All background tasks completed\n",
      "\n",
      "Judgement completed successfully!\n",
      "Check GitHub sub-issue #456 for the detailed judgement results.\n"
     ]
    }
   ],
   "source": [
    "# Call judge_workplan with our mock context\n",
    "await run_judge_workplan(\n",
    "    workplan_content=WORKPLAN_CONTENT,\n",
    "    diff_content=DIFF_CONTENT,\n",
    "    base_ref=base_ref,\n",
    "    head_ref=head_ref,\n",
    "    subissue_to_update=subissue_to_update,\n",
    "    parent_workplan_issue_number=parent_workplan_issue_number,\n",
    "    repo_path=REPO_PATH,\n",
    "    gemini_client=gemini_client,\n",
    "    openai_client=openai_client,\n",
    "    llm_manager=llm_manager,\n",
    "    model=MODEL,\n",
    "    base_commit_hash=\"abc123\",  # Optional: actual commit hash\n",
    "    head_commit_hash=\"def456\",  # Optional: actual commit hash\n",
    "    debug=True,  # Set to True to see the full prompt used\n",
    "    codebase_reasoning=\"full\",  # Options: \"full\", \"lsp\", \"file_structure\", \"none\"\n",
    "    disable_search_grounding=False,\n",
    "    github_command_func=mock_github_command,\n",
    "    log_callback=log_callback,\n",
    "    wait_for_background_tasks=True,\n",
    "    background_task_timeout=180\n",
    ")\n",
    "\n",
    "print(\"\\nJudgement completed successfully!\")\n",
    "print(f\"Check GitHub sub-issue #{subissue_to_update} for the detailed judgement results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sravan-yellhorn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
